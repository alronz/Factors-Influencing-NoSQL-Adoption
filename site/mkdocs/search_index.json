{
    "docs": [
        {
            "location": "/", 
            "text": "NoSQL selection factors\n\n\nAn attempt to analysis different NoSQL databases from different NoSQL categories in order to study and test their data modelling capabilities and query expressiveness. Furthermore, many features of each database will be investigated and documented in an easy tutorial-like approach.\n\n\n\n\nMotivation\n\n\ncontent comes here.\n\n\nInvestigated databases\n\n\ncontent comes here.\n\n\nExamples\n\n\ncontent comes here.\n\n\nContent\n\n\nRedis\n\n\n\n\nBasics\n\n\nOverview\n\n\nInstallation\n\n\nUnderline Structure\n\n\nPossible Use Cases\n\n\nQuery Language\n\n\nTransaction Support\n\n\nData Import and Export\n\n\n\n\n\n\nData Model\n\n\nData Layout\n\n\nRelational Data Support\n\n\nNormalization/Denormalization\n\n\nNested Data Structures\n\n\nReferential Integrity\n\n\n\n\n\n\nQuery Model\n\n\nQuery Options\n\n\nFull Text Search Support\n\n\nRegular Expressions Support\n\n\nAggregation and Filtering\n\n\nIndexing\n\n\nSorting\n\n\n\n\n\n\nAdministration and Maintenance  \n\n\nConfiguration\n\n\nScalability\n\n\nPersistency\n\n\nBackup\n\n\nUpgrading\n\n\nSecurity\n\n\nAvailability\n\n\n\n\n\n\nSpecial Features\n\n\nScripting Support\n \n\n\nPub/Sub Support\n\n\nExpire Option\n\n\nRedis as a Cache\n\n\n\n\n\n\nExamples\n\n\nCart Management Service\n \n\n\nSession Management Service\n\n\nCache Service\n\n\nAnalytics Service\n\n\nJob Queue System\n\n\nHandling Relational Data\n\n\nBulk Transactions Support\n\n\nTransaction Support\n\n\n\n\n\n\nResults\n\n\nStrengths and Weaknesses\n\n\nSummary\n\n\n\n\n\n\n\n\nMongoDB\n\n\n\n\nBasics\n\n\nOverview\n\n\nInstallation\n\n\nUnderline Structure\n\n\nPossible Use Cases\n\n\nQuery Language\n\n\nTransaction Support\n\n\nData Import and Export\n\n\n\n\n\n\nData Model\n\n\nData Layout\n\n\nRelational Data Support\n\n\nNormalization/Denormalization\n\n\nNested Data Structures\n\n\nReferential Integrity\n\n\n\n\n\n\nQuery Model\n\n\nQuery Options\n\n\nFull Text Search Support\n\n\nRegular Expressions Support\n\n\nAggregation and Filtering\n\n\nIndexing\n\n\nSorting\n\n\n\n\n\n\nAdministration and Maintenance  \n\n\nConfiguration\n\n\nScalability\n\n\nPersistency\n\n\nBackup\n\n\nUpgrading\n\n\nSecurity\n\n\nAvailability\n  \n\n\n\n\n\n\nSpecial Features\n\n\nGridFS\n\n\nDocument Validation\n\n\n\n\n\n\nExamples\n\n\nTPC-H Queries\n\n\n\n\n\n\nResults\n\n\nStrengths and Weaknesses\n\n\nSummary\n\n\n\n\n\n\n\n\nNeo4j\n\n\n\n\nBasics\n\n\nOverview\n\n\nInstallation\n\n\nUnderline Structure\n\n\nPossible Use Cases\n\n\nQuery Language\n\n\nTransaction Support\n\n\nData Import and Export\n\n\n\n\n\n\nData Model\n\n\nData Layout\n\n\nRelational Data Support\n\n\nNormalization/Denormalization\n\n\nNested Data Structures\n\n\nReferential Integrity\n\n\n\n\n\n\nQuery Model\n\n\nQuery Options\n\n\nFull Text Search Support\n\n\nRegular Expressions Support\n\n\nAggregation\n\n\nIndexing\n\n\nSorting\n\n\nFiltering\n\n\n\n\n\n\nAdministration and Maintenance  \n\n\nConfiguration\n\n\nScalability\n\n\nPersistency\n\n\nBackup\n\n\nUpgrading\n\n\nSecurity\n\n\nAvailability\n\n\n\n\n\n\nSpecial Features\n\n\nExamples\n\n\nResults\n\n\nStrengths and Weaknesses\n\n\nSummary\n\n\n\n\n\n\n\n\nCassandra\n\n\n\n\nBasics\n\n\nOverview\n\n\nInstallation\n\n\nUnderline Structure\n\n\nPossible Use Cases\n\n\nQuery Language\n\n\nTransaction Support\n\n\nData Import and Export\n\n\n\n\n\n\nData Model\n\n\nData Layout\n\n\nRelational Data Support\n\n\nNormalization/Denormalization\n\n\nNested Data Structures\n\n\nReferential Integrity\n\n\n\n\n\n\nQuery Model\n\n\nQuery Options\n\n\nFull Text Search Support\n\n\nRegular Expressions Support\n\n\nAggregation\n\n\nIndexing\n\n\nSorting\n\n\nFiltering\n\n\n\n\n\n\nAdministration and Maintenance  \n\n\nConfiguration\n\n\nScalability\n\n\nPersistency\n\n\nBackup\n\n\nUpgrading\n\n\nSecurity\n\n\nAvailability\n\n\n\n\n\n\nSpecial Features\n\n\nExamples\n\n\nResults\n\n\nStrengths and Weaknesses\n\n\nSummary\n\n\n\n\n\n\n\n\nReferences\n\n\ncontent comes here.\n\n\nSummary\n\n\ncontent comes here.", 
            "title": "Home"
        }, 
        {
            "location": "/#nosql-selection-factors", 
            "text": "An attempt to analysis different NoSQL databases from different NoSQL categories in order to study and test their data modelling capabilities and query expressiveness. Furthermore, many features of each database will be investigated and documented in an easy tutorial-like approach.", 
            "title": "NoSQL selection factors"
        }, 
        {
            "location": "/#motivation", 
            "text": "content comes here.", 
            "title": "Motivation"
        }, 
        {
            "location": "/#investigated-databases", 
            "text": "content comes here.", 
            "title": "Investigated databases"
        }, 
        {
            "location": "/#examples", 
            "text": "content comes here.", 
            "title": "Examples"
        }, 
        {
            "location": "/#content", 
            "text": "", 
            "title": "Content"
        }, 
        {
            "location": "/#redis", 
            "text": "Basics  Overview  Installation  Underline Structure  Possible Use Cases  Query Language  Transaction Support  Data Import and Export    Data Model  Data Layout  Relational Data Support  Normalization/Denormalization  Nested Data Structures  Referential Integrity    Query Model  Query Options  Full Text Search Support  Regular Expressions Support  Aggregation and Filtering  Indexing  Sorting    Administration and Maintenance    Configuration  Scalability  Persistency  Backup  Upgrading  Security  Availability    Special Features  Scripting Support    Pub/Sub Support  Expire Option  Redis as a Cache    Examples  Cart Management Service    Session Management Service  Cache Service  Analytics Service  Job Queue System  Handling Relational Data  Bulk Transactions Support  Transaction Support    Results  Strengths and Weaknesses  Summary", 
            "title": "Redis"
        }, 
        {
            "location": "/#mongodb", 
            "text": "Basics  Overview  Installation  Underline Structure  Possible Use Cases  Query Language  Transaction Support  Data Import and Export    Data Model  Data Layout  Relational Data Support  Normalization/Denormalization  Nested Data Structures  Referential Integrity    Query Model  Query Options  Full Text Search Support  Regular Expressions Support  Aggregation and Filtering  Indexing  Sorting    Administration and Maintenance    Configuration  Scalability  Persistency  Backup  Upgrading  Security  Availability       Special Features  GridFS  Document Validation    Examples  TPC-H Queries    Results  Strengths and Weaknesses  Summary", 
            "title": "MongoDB"
        }, 
        {
            "location": "/#neo4j", 
            "text": "Basics  Overview  Installation  Underline Structure  Possible Use Cases  Query Language  Transaction Support  Data Import and Export    Data Model  Data Layout  Relational Data Support  Normalization/Denormalization  Nested Data Structures  Referential Integrity    Query Model  Query Options  Full Text Search Support  Regular Expressions Support  Aggregation  Indexing  Sorting  Filtering    Administration and Maintenance    Configuration  Scalability  Persistency  Backup  Upgrading  Security  Availability    Special Features  Examples  Results  Strengths and Weaknesses  Summary", 
            "title": "Neo4j"
        }, 
        {
            "location": "/#cassandra", 
            "text": "Basics  Overview  Installation  Underline Structure  Possible Use Cases  Query Language  Transaction Support  Data Import and Export    Data Model  Data Layout  Relational Data Support  Normalization/Denormalization  Nested Data Structures  Referential Integrity    Query Model  Query Options  Full Text Search Support  Regular Expressions Support  Aggregation  Indexing  Sorting  Filtering    Administration and Maintenance    Configuration  Scalability  Persistency  Backup  Upgrading  Security  Availability    Special Features  Examples  Results  Strengths and Weaknesses  Summary", 
            "title": "Cassandra"
        }, 
        {
            "location": "/#references", 
            "text": "content comes here.", 
            "title": "References"
        }, 
        {
            "location": "/#summary", 
            "text": "content comes here.", 
            "title": "Summary"
        }, 
        {
            "location": "/Cassandra/Cassandra_Main/", 
            "text": "back\n\n\nGetting Started\n\n\nBasic Features\n\n\nData Modeling\n\n\nSearching Data\n\n\nAdministration and Maintenance\n\n\nExample\n\n\nResults", 
            "title": "Cassandra Main"
        }, 
        {
            "location": "/Cassandra/Cassandra_Main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Cassandra_Main/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/Cassandra/Cassandra_Main/#basic-features", 
            "text": "", 
            "title": "Basic Features"
        }, 
        {
            "location": "/Cassandra/Cassandra_Main/#data-modeling", 
            "text": "", 
            "title": "Data Modeling"
        }, 
        {
            "location": "/Cassandra/Cassandra_Main/#searching-data", 
            "text": "", 
            "title": "Searching Data"
        }, 
        {
            "location": "/Cassandra/Cassandra_Main/#administration-and-maintenance", 
            "text": "", 
            "title": "Administration and Maintenance"
        }, 
        {
            "location": "/Cassandra/Cassandra_Main/#example", 
            "text": "", 
            "title": "Example"
        }, 
        {
            "location": "/Cassandra/Cassandra_Main/#results", 
            "text": "", 
            "title": "Results"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/", 
            "text": "back\n\n\nConfiguration\n\n\nScalability\n\n\nPersistency\n\n\nBackup\n\n\nSecurity\n\n\nUpgrading\n\n\nAvailability\n\n\nRefernces", 
            "title": "Admin main"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/#scalability", 
            "text": "", 
            "title": "Scalability"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/#persistency", 
            "text": "", 
            "title": "Persistency"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/#backup", 
            "text": "", 
            "title": "Backup"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/#security", 
            "text": "", 
            "title": "Security"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/#upgrading", 
            "text": "", 
            "title": "Upgrading"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/#availability", 
            "text": "", 
            "title": "Availability"
        }, 
        {
            "location": "/Cassandra/Adminstration/admin_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Adminstration/availability/", 
            "text": "back\n\n\nCassandra is a highly available distributed system. This is because You can easily replicate your data across multiple nodes or even data centres that spans multiple locations. Replicating the data prevents downtime in case the node or the data centre experiences a catastrophic event or an unexpected failure since the data can be still served from the replica nodes. \n\n\nCassandra is a decentralised system that doesn't have a single point of failure since it is a masterless system and all nodes are equally important. When we create the Keyspace, we need to decide about the replication factor and the replication strategy. The replication factor defines how many copies or replicas of our data that we want to keep. And the replication strategy defines where we are planning to replciate the data. Cassandra nodes uses a gossip protocol to know the state of all other nodes. Based on the state of the nodes, each node will decide where to forward the received read/write requests to ensure availability.  Replicating the data in more than one node will ensure reliability and fault tolerance. Once a node is down, then all read and write requests will be rerouted to the replica nodes.\n\n\n\n\nThe replication strategy used in each node to determine the physical location of the nodes that will be used as a replica. Replication strategy uses the Snitch which is a protocol used to determine which nodes are performing well and which aren't. Based on the snitch information, the replication strategy will replicate the data to the best performing nodes. There are different types of replication strategies that can be configured in Cassandra. For example, the SimpleStrategy is used if we have single data centre or only an individual node. This is the default strategy used and it will simply choose the replica based on the partitioner. In the SimpleStrategy, the first replica will be the first node selected by the partitioner and then the second node is the next node that was added clock-wisely to the ring and so forth.  However, it is encouraged to use the NetworkTopologyStrategy replication strategy instead of the simple strategy especially if your deployment span multiple data centres. This strategy will define how many replicas you require to have in each data centre. Then the NetworkTopologyStrategy will choose the replicas by walking the ring clock-wisely until reaching the initial node in another rack. Additionally, the NetworkTopologyStrategy will make sure to choose replicas in different racks due to the fact that nodes in the same rack tend to fail together because of power or network issues.", 
            "title": "Availability"
        }, 
        {
            "location": "/Cassandra/Adminstration/availability/#back", 
            "text": "Cassandra is a highly available distributed system. This is because You can easily replicate your data across multiple nodes or even data centres that spans multiple locations. Replicating the data prevents downtime in case the node or the data centre experiences a catastrophic event or an unexpected failure since the data can be still served from the replica nodes.   Cassandra is a decentralised system that doesn't have a single point of failure since it is a masterless system and all nodes are equally important. When we create the Keyspace, we need to decide about the replication factor and the replication strategy. The replication factor defines how many copies or replicas of our data that we want to keep. And the replication strategy defines where we are planning to replciate the data. Cassandra nodes uses a gossip protocol to know the state of all other nodes. Based on the state of the nodes, each node will decide where to forward the received read/write requests to ensure availability.  Replicating the data in more than one node will ensure reliability and fault tolerance. Once a node is down, then all read and write requests will be rerouted to the replica nodes.   The replication strategy used in each node to determine the physical location of the nodes that will be used as a replica. Replication strategy uses the Snitch which is a protocol used to determine which nodes are performing well and which aren't. Based on the snitch information, the replication strategy will replicate the data to the best performing nodes. There are different types of replication strategies that can be configured in Cassandra. For example, the SimpleStrategy is used if we have single data centre or only an individual node. This is the default strategy used and it will simply choose the replica based on the partitioner. In the SimpleStrategy, the first replica will be the first node selected by the partitioner and then the second node is the next node that was added clock-wisely to the ring and so forth.  However, it is encouraged to use the NetworkTopologyStrategy replication strategy instead of the simple strategy especially if your deployment span multiple data centres. This strategy will define how many replicas you require to have in each data centre. Then the NetworkTopologyStrategy will choose the replicas by walking the ring clock-wisely until reaching the initial node in another rack. Additionally, the NetworkTopologyStrategy will make sure to choose replicas in different racks due to the fact that nodes in the same rack tend to fail together because of power or network issues.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Adminstration/backup/", 
            "text": "back\n\n\nCassandra supports taking backups through snapshots and by enabling the incremental backup feature. The snapshot is taken for the SSTable files in a node either for a single keyspace or for all keyspaces or even for the whole cluster. Generally, a system-wide snapshot is taken then the incremental backup feature is enabled in each node to backup only the delta data that has been changed since the last snapshot. \n\n\nTo take a snapshot on a node, we use the nodetool snapshot command as shown below:\n\n\nnodetool -h localhost -p 7199 snapshot mykeyspace\n\n\n\n\nThen the snapshot will be created under the snapshot folder in your main data directory. \n\n\nCassandra doesn't delete the old snapshot files automatically, so you need to delete them manually if you no longer need them using the following command:\n\n\nnodetool -h localhost -p 7199 clearsnapshot\n\n\n\n\nThe above command will delete all the existing snapshots, so generally you need to run this command before you start taking new snapshot to save space by deleting old snapshot files. \n\n\nTo enable the incremental backup, you need to set the incremental_backups parameter in the configuration file to true.", 
            "title": "Backup"
        }, 
        {
            "location": "/Cassandra/Adminstration/backup/#back", 
            "text": "Cassandra supports taking backups through snapshots and by enabling the incremental backup feature. The snapshot is taken for the SSTable files in a node either for a single keyspace or for all keyspaces or even for the whole cluster. Generally, a system-wide snapshot is taken then the incremental backup feature is enabled in each node to backup only the delta data that has been changed since the last snapshot.   To take a snapshot on a node, we use the nodetool snapshot command as shown below:  nodetool -h localhost -p 7199 snapshot mykeyspace  Then the snapshot will be created under the snapshot folder in your main data directory.   Cassandra doesn't delete the old snapshot files automatically, so you need to delete them manually if you no longer need them using the following command:  nodetool -h localhost -p 7199 clearsnapshot  The above command will delete all the existing snapshots, so generally you need to run this command before you start taking new snapshot to save space by deleting old snapshot files.   To enable the incremental backup, you need to set the incremental_backups parameter in the configuration file to true.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Adminstration/configurations/", 
            "text": "back\n\n\nCassandra uses a yaml configuration file called cassandra.yaml to store all configuration related parameters. The configuration file can be found under the install_location/conf folder. Cassandra doesn't support changing the configurations on the fly and you will need to restart the node for the new configurations to take effect. \n\n\nCassandra have many configuration parameters that can be used to tune the different options and capabilities of Cassandra. Below I will show few common parameters as an example:\n\n\ncluster_name\nThis configuration parameter is used to allow the node to join a specific cluster.  The default cluster name is \nTest Cluster\n\n\n\ncommitlog_directory\nThis is where you set the path where the commit log file is stored.\n\ndata_file_directories\nThis is where you set the path where the SSTables files are stored.\n\n\n\n\nFor more information about all other configuration options, please have a look at \nCassandra documentations\n.", 
            "title": "Configurations"
        }, 
        {
            "location": "/Cassandra/Adminstration/configurations/#back", 
            "text": "Cassandra uses a yaml configuration file called cassandra.yaml to store all configuration related parameters. The configuration file can be found under the install_location/conf folder. Cassandra doesn't support changing the configurations on the fly and you will need to restart the node for the new configurations to take effect.   Cassandra have many configuration parameters that can be used to tune the different options and capabilities of Cassandra. Below I will show few common parameters as an example:  cluster_name\nThis configuration parameter is used to allow the node to join a specific cluster.  The default cluster name is  Test Cluster \n\n\ncommitlog_directory\nThis is where you set the path where the commit log file is stored.\n\ndata_file_directories\nThis is where you set the path where the SSTables files are stored.  For more information about all other configuration options, please have a look at  Cassandra documentations .", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Adminstration/data migration/", 
            "text": "back\n\n\nPlease have a", 
            "title": "Data migration"
        }, 
        {
            "location": "/Cassandra/Adminstration/data migration/#back", 
            "text": "Please have a", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Adminstration/persistance/", 
            "text": "back\n\n\nCassandra is fully durable data store since the data will be written directly to a commit log once received. The commit log is then persisted to disk and later it can be used as a crash recovery mechanism whenever the node accidentally failed. The write operation will be considered a successful only once it has been written to the commit log. After the write operation is written to the commit log, it will be written to the MemTable. The written data will be kept in the MemTable until it reaches a pre-configured size threshold. Once this threshold is reached, data will be flushed to disk or what is called the SSTable files and be persisted permanently.", 
            "title": "Persistance"
        }, 
        {
            "location": "/Cassandra/Adminstration/persistance/#back", 
            "text": "Cassandra is fully durable data store since the data will be written directly to a commit log once received. The commit log is then persisted to disk and later it can be used as a crash recovery mechanism whenever the node accidentally failed. The write operation will be considered a successful only once it has been written to the commit log. After the write operation is written to the commit log, it will be written to the MemTable. The written data will be kept in the MemTable until it reaches a pre-configured size threshold. Once this threshold is reached, data will be flushed to disk or what is called the SSTable files and be persisted permanently.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Adminstration/refernces/", 
            "text": "back\n\n\n1- Cassandra: The Definitive Guide ,Eben Hewitt\n\n\n2- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry\n\n\n3- https://docs.datastax.com/\n\n\n4- http://www.rapidvaluesolutions.com/tech_blog/cassandra-the-right-data-store-for-scalability-performance-availability-and-maintainability/", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Adminstration/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Adminstration/refernces/#1-cassandra-the-definitive-guide-eben-hewitt", 
            "text": "", 
            "title": "1- Cassandra: The Definitive Guide ,Eben Hewitt"
        }, 
        {
            "location": "/Cassandra/Adminstration/refernces/#2-practical-cassandra-a-developers-approach-by-eric-lubow-and-russell-bradberry", 
            "text": "", 
            "title": "2- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry"
        }, 
        {
            "location": "/Cassandra/Adminstration/refernces/#3-httpsdocsdatastaxcom", 
            "text": "", 
            "title": "3- https://docs.datastax.com/"
        }, 
        {
            "location": "/Cassandra/Adminstration/refernces/#4-httpwwwrapidvaluesolutionscomtech_blogcassandra-the-right-data-store-for-scalability-performance-availability-and-maintainability", 
            "text": "", 
            "title": "4- http://www.rapidvaluesolutions.com/tech_blog/cassandra-the-right-data-store-for-scalability-performance-availability-and-maintainability/"
        }, 
        {
            "location": "/Cassandra/Adminstration/scalability/", 
            "text": "back\n\n\nCassandra uses a peer-to-peer distribution model, such that all nodes in a cluster have an equal importance and there is no concept for a master or a salve. This makes Cassandra design very scalable and highly available. This peer-to-peer design Improves the overall scalability since all the nodes in a cluster can serve read or write requests and there is no single point of failure.  Because all nodes can serve read and write requests, scaling the system is as simple as adding new node to the cluster. This makes Cassandra elastically scalable since you can easily scale up or down by just adding or removing nodes from the system.  Adding nodes to the cluster is also largely automated and has minimal configuration.  \n\n\nThe data in Cassandra is partitioned across the different system nodes by using the Partitioner that uses a hashing function to create a token based on the partition key to decide where to put the data. Then when a node receives a read or write request from an application client, it will check with the Partitioner to find in which node the requested data is stored. Then it will act as a coordinator to forward the request to the right node. Once the read or write request is a successful based on the configured consistency level, it will forward back the results to the client. So read and write requests are performed in a similar way and can be fulfilled by any node connected to the system. Horizontal scaling the system is then as simple as adding or removing nodes, data centres or even clusters.\n\n\n\n\n\n\nAfter the virtual nodes has been introduced in Cassandra, adding and removing nodes is easily done using the nodetool commands. The node will join the existing cluster and will automatically will be responsible for an even portion of the data from the other nodes in the cluster. Removing the nodes is also easily done using the nodetool command (nodetool decommission) which will remove the node from the cluster and assign its data evenly to the other nodes in the cluster.", 
            "title": "Scalability"
        }, 
        {
            "location": "/Cassandra/Adminstration/scalability/#back", 
            "text": "Cassandra uses a peer-to-peer distribution model, such that all nodes in a cluster have an equal importance and there is no concept for a master or a salve. This makes Cassandra design very scalable and highly available. This peer-to-peer design Improves the overall scalability since all the nodes in a cluster can serve read or write requests and there is no single point of failure.  Because all nodes can serve read and write requests, scaling the system is as simple as adding new node to the cluster. This makes Cassandra elastically scalable since you can easily scale up or down by just adding or removing nodes from the system.  Adding nodes to the cluster is also largely automated and has minimal configuration.    The data in Cassandra is partitioned across the different system nodes by using the Partitioner that uses a hashing function to create a token based on the partition key to decide where to put the data. Then when a node receives a read or write request from an application client, it will check with the Partitioner to find in which node the requested data is stored. Then it will act as a coordinator to forward the request to the right node. Once the read or write request is a successful based on the configured consistency level, it will forward back the results to the client. So read and write requests are performed in a similar way and can be fulfilled by any node connected to the system. Horizontal scaling the system is then as simple as adding or removing nodes, data centres or even clusters.    After the virtual nodes has been introduced in Cassandra, adding and removing nodes is easily done using the nodetool commands. The node will join the existing cluster and will automatically will be responsible for an even portion of the data from the other nodes in the cluster. Removing the nodes is also easily done using the nodetool command (nodetool decommission) which will remove the node from the cluster and assign its data evenly to the other nodes in the cluster.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Adminstration/security/", 
            "text": "back\n\n\nIn the community version of Cassandra, you can provide security for your data by the following features:\n\n\nSSL encryption\n\n\nAll the communications between the client and any of the database nodes are secured by SSL encryption. This ensures that the data is transferred in a secure way and won't be compromised. \n\n\nAuthentication based control\n\n\nCassandra supports the creation of users and roles. Then any one who wants to connect to the database needs to be authenticated first. CQL supports the creation of the users and roles using the CREATE USER and the CREATE ROLE statements. The users and the roles can be dropped or altered later if needed. This provides security for the data since the users are authenticated first using passwords and can access the data based on pre-defined roles.\n\n\nObject permission\n\n\nCassandra supports also object permissions that can be granted to the intended users only. The permission can be granted or revoked using CQL statements GRANT or REVOKE.", 
            "title": "Security"
        }, 
        {
            "location": "/Cassandra/Adminstration/security/#back", 
            "text": "In the community version of Cassandra, you can provide security for your data by the following features:", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Adminstration/security/#ssl-encryption", 
            "text": "All the communications between the client and any of the database nodes are secured by SSL encryption. This ensures that the data is transferred in a secure way and won't be compromised.", 
            "title": "SSL encryption"
        }, 
        {
            "location": "/Cassandra/Adminstration/security/#authentication-based-control", 
            "text": "Cassandra supports the creation of users and roles. Then any one who wants to connect to the database needs to be authenticated first. CQL supports the creation of the users and roles using the CREATE USER and the CREATE ROLE statements. The users and the roles can be dropped or altered later if needed. This provides security for the data since the users are authenticated first using passwords and can access the data based on pre-defined roles.", 
            "title": "Authentication based control"
        }, 
        {
            "location": "/Cassandra/Adminstration/security/#object-permission", 
            "text": "Cassandra supports also object permissions that can be granted to the intended users only. The permission can be granted or revoked using CQL statements GRANT or REVOKE.", 
            "title": "Object permission"
        }, 
        {
            "location": "/Cassandra/Adminstration/upgrade/", 
            "text": "back\n\n\nI will explain in this section briefly the general steps that you need to follow to upgrade your Cassandra to a new version. The steps below needs to be executed in each of the nodes.\n\n\n1-First of all, you need to read and familiarize your self with the new changes and features of the new release. \n\n\n2- Run the below command to rewrite the SSTables to make sure that all of them are running the current version of Cassandra:\n\n\nnodetool upgradesstables\n\n\n\n\n3- Run the below command to flush all the data that are currently in your MemTables to be written to the SSTables:\n\n\nnodetool drain \n\n\n\n\n4- Stop your node as shown below:\n\n\nsudo service cassandra stop\n\n\n\n\n5- Take a copy of your current configuration file to be used later as a backup because this file might be overwritten with default values of the new Cassandra version.\n\n\n6- Install the new version. For more details, check the \ninstallation section\n.\n\n\n7-  Configure the new version based on your old configurations and configure any new configurations for any new features.\n\n\n8- Start the node again using the below command:\n\n\nsudo service cassandra start \n\n\n\n\n9- Upgrade the SSTable as done in step 2 so that the tables will be rewritten to work with the new version :\n\n\nnodetool upgradesstables\n\n\n\n\n10-  Finally, check the logs for any errors, warning or exceptions.", 
            "title": "Upgrade"
        }, 
        {
            "location": "/Cassandra/Adminstration/upgrade/#back", 
            "text": "I will explain in this section briefly the general steps that you need to follow to upgrade your Cassandra to a new version. The steps below needs to be executed in each of the nodes.  1-First of all, you need to read and familiarize your self with the new changes and features of the new release.   2- Run the below command to rewrite the SSTables to make sure that all of them are running the current version of Cassandra:  nodetool upgradesstables  3- Run the below command to flush all the data that are currently in your MemTables to be written to the SSTables:  nodetool drain   4- Stop your node as shown below:  sudo service cassandra stop  5- Take a copy of your current configuration file to be used later as a backup because this file might be overwritten with default values of the new Cassandra version.  6- Install the new version. For more details, check the  installation section .  7-  Configure the new version based on your old configurations and configure any new configurations for any new features.  8- Start the node again using the below command:  sudo service cassandra start   9- Upgrade the SSTable as done in step 2 so that the tables will be rewritten to work with the new version :  nodetool upgradesstables  10-  Finally, check the logs for any errors, warning or exceptions.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Basic Features/Pipline support/", 
            "text": "back\n\n\nTo import or export data from or into Cassandra, CQL provides a simple statement called \"COPY\" that can be used to either import or export data. The COPY statement can either export data from a table into a CSV file or import data from a CSV file and into a table. The \"COPY\" command can be used with the below syntax:\n\n\nTo import data: \n\n\nCOPY table_name ( column, ...)\nFROM ( 'file_name' | STDIN )\nWITH option = 'value' AND ...\n\n\n\n\nTo Export data:\n\n\nCOPY table_name ( column , ... )\nTO ( 'file_name' | STDOUT )\nWITH option = 'value' AND ...\n\n\n\n\nThere are many options that can be specified with the above commands such as CHUNKSIZE, MAXBATCHSIZE, MAXROWS, or SKIPROWS.  For more details about all the options, please have a look to the \ndocumentations\n.\n\n\nAn example is given below to show how to import and export data from the product table:\n\n\nTo import data into the product table:\n\n\nCOPY ecommerce.product\n(name, price, size, weight)\n FROM 'product.csv'\n WITH DELIMITER='|'\n\n\n\n\nOr to export data from the product and into the product.csv file:\n\n\nCOPY ecommerce.product\n(name, price, size, weight)\n TO 'product.csv'\n WITH DELIMITER='|'\n\n\n\n\nIn addition, Cassandra supports a shell command called SOURCE which allows you to execute multiple CQL statement from a file. An example is show below:\n\n\nSOURCE ./file_containing_cql_statements", 
            "title": "Pipline support"
        }, 
        {
            "location": "/Cassandra/Basic Features/Pipline support/#back", 
            "text": "To import or export data from or into Cassandra, CQL provides a simple statement called \"COPY\" that can be used to either import or export data. The COPY statement can either export data from a table into a CSV file or import data from a CSV file and into a table. The \"COPY\" command can be used with the below syntax:  To import data:   COPY table_name ( column, ...)\nFROM ( 'file_name' | STDIN )\nWITH option = 'value' AND ...  To Export data:  COPY table_name ( column , ... )\nTO ( 'file_name' | STDOUT )\nWITH option = 'value' AND ...  There are many options that can be specified with the above commands such as CHUNKSIZE, MAXBATCHSIZE, MAXROWS, or SKIPROWS.  For more details about all the options, please have a look to the  documentations .  An example is given below to show how to import and export data from the product table:  To import data into the product table:  COPY ecommerce.product\n(name, price, size, weight)\n FROM 'product.csv'\n WITH DELIMITER='|'  Or to export data from the product and into the product.csv file:  COPY ecommerce.product\n(name, price, size, weight)\n TO 'product.csv'\n WITH DELIMITER='|'  In addition, Cassandra supports a shell command called SOURCE which allows you to execute multiple CQL statement from a file. An example is show below:  SOURCE ./file_containing_cql_statements", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Basic Features/basic_features_main/", 
            "text": "back\n\n\nQuery Language\n\n\nTransaction Support\n\n\nData Import and Export\n\n\nRefernces", 
            "title": "Basic features main"
        }, 
        {
            "location": "/Cassandra/Basic Features/basic_features_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Basic Features/basic_features_main/#query-language", 
            "text": "", 
            "title": "Query Language"
        }, 
        {
            "location": "/Cassandra/Basic Features/basic_features_main/#transaction-support", 
            "text": "", 
            "title": "Transaction Support"
        }, 
        {
            "location": "/Cassandra/Basic Features/basic_features_main/#data-import-and-export", 
            "text": "", 
            "title": "Data Import and Export"
        }, 
        {
            "location": "/Cassandra/Basic Features/basic_features_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Basic Features/commands/", 
            "text": "back\n\n\nCassandra supports a SQL-simlar query language called CQL (Cassandra Query Language) that can be used to interact with the stored data. CQL can be accessed using a shell script called cqlsh which is found inside the installation folder in the below path :\n\n\nbin/sh cqlsh\n\n\n\n\nThen you will be connected as shown below:\n\n\n\n\nWhen you run cqlsh command, you will be connected by default to your local node. If you want to connect to another node, you can run the command as shown below:\n\n\nbin/sh cqlsh 1.2.3.4 9042\n\n\n\n\nThe \"1.2.3.4 9042\" in the above example is the IP address and port number of the node that you want to connect to. \n\n\nIn the following sections, I will explain briefly how you can read and write data in Cassandra using CQL.\n\n\nReading data\n\n\nQuerying data in CQL is done using the \"SELECT\" statement in a similar way like we do in SQL.\nThe mean difference in the SELECT statement between SQL and CQL is that in CQL you can only query the data using the columns of the primary key. Otherwise Cassandra will reject the query. For example, if you want to query the product table using the id column, then you need to define the id column to be part of the primary key as shown below:\n\n\nCREATE TABLE ecommerce.product ( id UUID, name text,\n added_date timestamp, price decimal, weight text, size text, PRIMARY KEY (id) )\n\n\n\n\nNow since the id column is part of the primary key, then you can run queries against it as shown below:\n\n\nSELECT * FROM ecommerce.product WHERE id =2\n\n\n\n\nYou can also run queries against columns that aren't part of the primary key of the table if you create a secondary index on them. More about primary key and indexes will be explained in the \ndata layout\n section and the \nindexes section\n. \n\n\nQuerying the data will be explained in more details in the \nsearching section\n.\n\n\nWriting data\n\n\nFor each application, we would need at first to create a Keyspace that will contain our tables which acts like a schema in a relational database. For creating a Keyspace, you can use the \"CREATE KEYSPACE\" statement as shown below:\n\n\nCREATE KEYSPACE ecommerce WITH REPLICATION =\n { 'class' : 'NetworkTopologyStrategy'[ 'datacenter1' : 3 ] };\n ````\n\nFor each Keyspace, you can define some attributes as shown above. We have created a keyspace called ecommerce using the \nNetworkTopologyStrategy\n replication strategy.\n\nLater if you want to change the keyspace attributes, you can use the \nALTER KEYSPACE\n clause as shown below:\n\n\n\n\n\nALTER KEYSPACE \"ecommerce\" WITH REPLICATION =\n  { 'class' : 'SimpleStrategy', 'replication_factor' : 2 };\n\n\n\n\nTo drop the keyspace, you can use the \nDROP\n clause as shown below:\n\n\n\n\n\nDROP KEYSPACE ecommerce;\n\n\n\n\nTo create a CQL table inside the \necommerce\n keyspace, we can use the \nCREATE TABLE\n statement using the \nWITH\n clause to assign table properties as seen below:\n\n\n\n\n\n\nCREATE TABLE ecommerce.product ( id UUID PRIMARY KEY, name text,\n added_date timestamp, price decimal, weight text, size text ) WITH compaction =\n    { 'class' : 'SizeTieredCompactionStrategy'}; \n\n\n\nAbove we have created a table having the id as the primary key. Enforcing a schema as seen above is optional but it is also recommended to know what are the data stored in each table. Unlike the relational databases, if you don't want to store a value in a certain column, then nothing will be stored instead of storing null. This means you can have rows having only name and price, while others having also weight and size. \n\n\nLater if you wish to change the table structure such as changing the data type of a particular column, then you can use the \nALTER TABLE\n statement as shown below:\n\n\n\n\n\nALTER TABLE ecommerce.product ADD color text;\n\n\n\nYou can also create a materialised views in Cassandra using the \nCREATE MATERIALIZED VIEW\n statement as seen below:\n\n\n\n\n\n\nCREATE MATERIALIZED VIEW ecommerce.product_MV\nAS SELECT * FROM ecommerce.product\nWHERE price \n 20 AND size \n 50\nPRIMARY KEY (id);\n\n\n\n\n\nTo drop a table or a materialised view, you can use the \nDROP TABLE\n or \nDROP MATERIALIZED VIEW\n clauses as shown below:\n\n\n\n\n\nDROP TABLE ecommerce.product;\n\n\n\n\n\nDROP TABLE ecommerce.product_MV;\n\n\n\n\nTo insert data into the table, you can use the \nINSERT INTO\n statement as shown below:\n\n\n\n\n\nINSERT INTO ecommerce.product ( name, price,size ) VALUES ('Dell Laptop', 736.3, 15);\n\n\n\nSimilarly you can run the \nUPDATE\n statement to update the data as seen below:\n\n\n\n\n\nUPDATE ecommerce.product SET weight = 20 WHERE id = \"2\";\n\n\n\nAnd to delete the data, the \nDELETE\n statement can be used. Unlike SQL, you can delete columns or even values from a columns with a collection data type, example is shown below:\n\n\n\n\n\nDELETE size FROM ecommerce.product WHERE id IN (1,2,3) \n\n\n\nIt is also possible to delete entire rows using just \nDELETE\n without specifying any columns as shown below:\n\n\n\n\n\nDELETE FROM ecommerce.product WHERE price = 0;\n````", 
            "title": "Commands"
        }, 
        {
            "location": "/Cassandra/Basic Features/commands/#back", 
            "text": "Cassandra supports a SQL-simlar query language called CQL (Cassandra Query Language) that can be used to interact with the stored data. CQL can be accessed using a shell script called cqlsh which is found inside the installation folder in the below path :  bin/sh cqlsh  Then you will be connected as shown below:   When you run cqlsh command, you will be connected by default to your local node. If you want to connect to another node, you can run the command as shown below:  bin/sh cqlsh 1.2.3.4 9042  The \"1.2.3.4 9042\" in the above example is the IP address and port number of the node that you want to connect to.   In the following sections, I will explain briefly how you can read and write data in Cassandra using CQL.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Basic Features/commands/#reading-data", 
            "text": "Querying data in CQL is done using the \"SELECT\" statement in a similar way like we do in SQL.\nThe mean difference in the SELECT statement between SQL and CQL is that in CQL you can only query the data using the columns of the primary key. Otherwise Cassandra will reject the query. For example, if you want to query the product table using the id column, then you need to define the id column to be part of the primary key as shown below:  CREATE TABLE ecommerce.product ( id UUID, name text,\n added_date timestamp, price decimal, weight text, size text, PRIMARY KEY (id) )  Now since the id column is part of the primary key, then you can run queries against it as shown below:  SELECT * FROM ecommerce.product WHERE id =2  You can also run queries against columns that aren't part of the primary key of the table if you create a secondary index on them. More about primary key and indexes will be explained in the  data layout  section and the  indexes section .   Querying the data will be explained in more details in the  searching section .", 
            "title": "Reading data"
        }, 
        {
            "location": "/Cassandra/Basic Features/commands/#writing-data", 
            "text": "For each application, we would need at first to create a Keyspace that will contain our tables which acts like a schema in a relational database. For creating a Keyspace, you can use the \"CREATE KEYSPACE\" statement as shown below:  CREATE KEYSPACE ecommerce WITH REPLICATION =\n { 'class' : 'NetworkTopologyStrategy'[ 'datacenter1' : 3 ] };\n ````\n\nFor each Keyspace, you can define some attributes as shown above. We have created a keyspace called ecommerce using the  NetworkTopologyStrategy  replication strategy.\n\nLater if you want to change the keyspace attributes, you can use the  ALTER KEYSPACE  clause as shown below:  ALTER KEYSPACE \"ecommerce\" WITH REPLICATION =\n  { 'class' : 'SimpleStrategy', 'replication_factor' : 2 };  \n\nTo drop the keyspace, you can use the  DROP  clause as shown below:  DROP KEYSPACE ecommerce;  \n\nTo create a CQL table inside the  ecommerce  keyspace, we can use the  CREATE TABLE  statement using the  WITH  clause to assign table properties as seen below:  CREATE TABLE ecommerce.product ( id UUID PRIMARY KEY, name text,\n added_date timestamp, price decimal, weight text, size text ) WITH compaction =\n    { 'class' : 'SizeTieredCompactionStrategy'};   \nAbove we have created a table having the id as the primary key. Enforcing a schema as seen above is optional but it is also recommended to know what are the data stored in each table. Unlike the relational databases, if you don't want to store a value in a certain column, then nothing will be stored instead of storing null. This means you can have rows having only name and price, while others having also weight and size. \n\n\nLater if you wish to change the table structure such as changing the data type of a particular column, then you can use the  ALTER TABLE  statement as shown below:  ALTER TABLE ecommerce.product ADD color text;  \nYou can also create a materialised views in Cassandra using the  CREATE MATERIALIZED VIEW  statement as seen below:  CREATE MATERIALIZED VIEW ecommerce.product_MV\nAS SELECT * FROM ecommerce.product\nWHERE price   20 AND size   50\nPRIMARY KEY (id);  \n\n\nTo drop a table or a materialised view, you can use the  DROP TABLE  or  DROP MATERIALIZED VIEW  clauses as shown below:  DROP TABLE ecommerce.product;   DROP TABLE ecommerce.product_MV;  \n\nTo insert data into the table, you can use the  INSERT INTO  statement as shown below:  INSERT INTO ecommerce.product ( name, price,size ) VALUES ('Dell Laptop', 736.3, 15);  \nSimilarly you can run the  UPDATE  statement to update the data as seen below:  UPDATE ecommerce.product SET weight = 20 WHERE id = \"2\";  \nAnd to delete the data, the  DELETE  statement can be used. Unlike SQL, you can delete columns or even values from a columns with a collection data type, example is shown below:  DELETE size FROM ecommerce.product WHERE id IN (1,2,3)   \nIt is also possible to delete entire rows using just  DELETE  without specifying any columns as shown below:  DELETE FROM ecommerce.product WHERE price = 0;\n````", 
            "title": "Writing data"
        }, 
        {
            "location": "/Cassandra/Basic Features/refernces/", 
            "text": "back\n\n\n1- http://docs.datastax.com/en/\n\n\n2- Cassandra: The Definitive Guide ,Eben Hewitt\n\n\n3- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry\n\n\n4- http://www.datastax.com/dev/blog/scalable-inventory-example", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Basic Features/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Basic Features/refernces/#1-httpdocsdatastaxcomen", 
            "text": "", 
            "title": "1- http://docs.datastax.com/en/"
        }, 
        {
            "location": "/Cassandra/Basic Features/refernces/#2-cassandra-the-definitive-guide-eben-hewitt", 
            "text": "", 
            "title": "2- Cassandra: The Definitive Guide ,Eben Hewitt"
        }, 
        {
            "location": "/Cassandra/Basic Features/refernces/#3-practical-cassandra-a-developers-approach-by-eric-lubow-and-russell-bradberry", 
            "text": "", 
            "title": "3- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry"
        }, 
        {
            "location": "/Cassandra/Basic Features/refernces/#4-httpwwwdatastaxcomdevblogscalable-inventory-example", 
            "text": "", 
            "title": "4- http://www.datastax.com/dev/blog/scalable-inventory-example"
        }, 
        {
            "location": "/Cassandra/Basic Features/transaction_support/", 
            "text": "back\n\n\nBefore start discussing how transactions are supported in Cassandra, I will first explain briefly how Cassandra supports the ACID (Atomicity, Consistency, Isolation, Durability) properties.\n\n\nConsistency\n\n\nData Consistency in Cassandra means how the data is kept up-to-date between all the replica nodes. Cassandra supports a configurable consistency levels or what is called a tunable consistency. Additionally, Cassandra supports a linearisable consistency that can be used in transactions as will be explained in the following sections.   \n\n\nTunable consistency\n\n\nIn Cassandra, you can configure the read and write consistency levels to provide strong or eventual consistency depending on your application requirements. Depending on the configured consistency level, Cassandra can be a CP system (consistent and partition tolerant) or AP system (highly available and partition tolerant) in terms of the CAP theorem. The consistency level can be configured to determine how many replicas need to acknowledge the read or the write operation before considering the operation as a successful or as a failure.  A common practice is to configure the consistency level to be of a QUORUM which means the majority number of replica should acknowledge the request before it will be considered as successful. In general the consistency level should be less than the replication factor. To measure how strong is the consistency level, the below two equations can be used:\n\n\nTo get a strong consistency, the below inequality should be satisfied:\n\n\nR+W \n N\n\n\nWhere R is the consistency level of a read request and W is the consistency level of a write request and the N is the number of replicas. In the same way, we have eventual consistency when the below inequality is satisfied:\n\n\nR + W =\n N\n\n\nLinearizable consistency\n\n\nThis consistency level is used when we want to achieve a serial isolation level. We need this  serial isolation level in applications where we need to perform a sequence of isolated operations where each operation must not be interrupted by others. Cassandra uses this consistency level when it needs to execute Lightweight transactions as will be explained later.\n\n\nAtomicity\n\n\nAt the row level or the partition-level, the insert, update, and delete operations are all executed atomically depending on the consistency level configured. This means if you configured the write consistency level to 2 and the replication factor is 3, then any write operation will be considered a successful only when we get acknowledgements from at least 2 replicas, otherwise it will be considered a failure. \n\n\nIsolation\n\n\nCassandra provides full isolation at the low-level which means that any writes from a client application will not be visible to other until it is completed. \n\n\nDurability\n\n\nCassandra is fully durable and all writes will be persisted on disk since nodes are using a persisted commit log and the write is considered a successful only after it is written to the commit log. If the node crashes, the commit log will be played back once the node restarts and it will recover the data written before the crash.\n\n\nTransactions\n\n\nCassandra is a distributed system with no single point of failure which means that we don't have a single master that handles all writes. This means we can't simply write some locks in the master node to implement transactions. Therefore, Cassandra uses a transaction mechanism called lightweight transactions which are implemented using the Paxos protocol. Paxos is a well-known consensus protocol that is used in distributed systems to make all nodes agree on proposals based on the majority voting. Paxos ensures that there exists only one agreed values between all the nodes which is what we need to implement transactions. Using this approach, Cassandra provides the ability to implement transactions. However this comes with a high cost since there will be many round trips between nodes in a cluster and should be used only whenever it is really necessary.  Using lightweight transactions will allow us to have a lineaizable consistency level suitable for transactions. \n\n\nAn example of how to use the lightweight transactions is if we want to create an order in a B2C application which will decrease the inventory by one. Then we need to do it in a transaction to prevent any concurrent write operations that tries to decrease the inventory at the same time. We can do that using the lightweight transaction \"IF\" statement as shown below:\n\n\nFirst we check what is the current quantity of the particular product:\n\n\nSELECT quantity FROM products WHERE id = 1;\n\n\n\n\nThen to decrease this quantity during the order, we do it as below:\n\n\nUPDATE products SET quantity = existing_quantity -1\nWHERE id = 1\nIF quantity = existing_quantity;\n\n\n\n\nThen if we want to do a complete transaction, we will create an order only after the decreasing of the quantity is a successful as shown below:\n\n\nUPDATE products SET quantity = 9\nWHERE id = 1\nIF quantity = 10;\n\nif(aboveQuerySuccessful())\n  insertNewOrder();\nelse\n retry();\n\n\n\n\nAnother example if we want to create a customer account and make sure that the user doesn't already exist, then we do it as shown below:\n\n\nINSERT INTO CUSTOMERS (user, email, name)\nVALUES ('John', 'user@email.com', 'John Robert', 1)\nIF NOT EXISTS;\n\n\n\n\nAs seen above, unlike in relational database transactions, Cassandra lightweight transactions uses optimistic locking instead of pessimistic locking and doesn't have the common BEGIN/COMMIT/ROLLBACK statements. In Cassandra lightweight transactions, we can only ensure that a given sequence of operations are executed as a single unit.", 
            "title": "Transaction support"
        }, 
        {
            "location": "/Cassandra/Basic Features/transaction_support/#back", 
            "text": "Before start discussing how transactions are supported in Cassandra, I will first explain briefly how Cassandra supports the ACID (Atomicity, Consistency, Isolation, Durability) properties.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Basic Features/transaction_support/#consistency", 
            "text": "Data Consistency in Cassandra means how the data is kept up-to-date between all the replica nodes. Cassandra supports a configurable consistency levels or what is called a tunable consistency. Additionally, Cassandra supports a linearisable consistency that can be used in transactions as will be explained in the following sections.", 
            "title": "Consistency"
        }, 
        {
            "location": "/Cassandra/Basic Features/transaction_support/#tunable-consistency", 
            "text": "In Cassandra, you can configure the read and write consistency levels to provide strong or eventual consistency depending on your application requirements. Depending on the configured consistency level, Cassandra can be a CP system (consistent and partition tolerant) or AP system (highly available and partition tolerant) in terms of the CAP theorem. The consistency level can be configured to determine how many replicas need to acknowledge the read or the write operation before considering the operation as a successful or as a failure.  A common practice is to configure the consistency level to be of a QUORUM which means the majority number of replica should acknowledge the request before it will be considered as successful. In general the consistency level should be less than the replication factor. To measure how strong is the consistency level, the below two equations can be used:  To get a strong consistency, the below inequality should be satisfied:  R+W   N  Where R is the consistency level of a read request and W is the consistency level of a write request and the N is the number of replicas. In the same way, we have eventual consistency when the below inequality is satisfied:  R + W =  N", 
            "title": "Tunable consistency"
        }, 
        {
            "location": "/Cassandra/Basic Features/transaction_support/#linearizable-consistency", 
            "text": "This consistency level is used when we want to achieve a serial isolation level. We need this  serial isolation level in applications where we need to perform a sequence of isolated operations where each operation must not be interrupted by others. Cassandra uses this consistency level when it needs to execute Lightweight transactions as will be explained later.", 
            "title": "Linearizable consistency"
        }, 
        {
            "location": "/Cassandra/Basic Features/transaction_support/#atomicity", 
            "text": "At the row level or the partition-level, the insert, update, and delete operations are all executed atomically depending on the consistency level configured. This means if you configured the write consistency level to 2 and the replication factor is 3, then any write operation will be considered a successful only when we get acknowledgements from at least 2 replicas, otherwise it will be considered a failure.", 
            "title": "Atomicity"
        }, 
        {
            "location": "/Cassandra/Basic Features/transaction_support/#isolation", 
            "text": "Cassandra provides full isolation at the low-level which means that any writes from a client application will not be visible to other until it is completed.", 
            "title": "Isolation"
        }, 
        {
            "location": "/Cassandra/Basic Features/transaction_support/#durability", 
            "text": "Cassandra is fully durable and all writes will be persisted on disk since nodes are using a persisted commit log and the write is considered a successful only after it is written to the commit log. If the node crashes, the commit log will be played back once the node restarts and it will recover the data written before the crash.", 
            "title": "Durability"
        }, 
        {
            "location": "/Cassandra/Basic Features/transaction_support/#transactions", 
            "text": "Cassandra is a distributed system with no single point of failure which means that we don't have a single master that handles all writes. This means we can't simply write some locks in the master node to implement transactions. Therefore, Cassandra uses a transaction mechanism called lightweight transactions which are implemented using the Paxos protocol. Paxos is a well-known consensus protocol that is used in distributed systems to make all nodes agree on proposals based on the majority voting. Paxos ensures that there exists only one agreed values between all the nodes which is what we need to implement transactions. Using this approach, Cassandra provides the ability to implement transactions. However this comes with a high cost since there will be many round trips between nodes in a cluster and should be used only whenever it is really necessary.  Using lightweight transactions will allow us to have a lineaizable consistency level suitable for transactions.   An example of how to use the lightweight transactions is if we want to create an order in a B2C application which will decrease the inventory by one. Then we need to do it in a transaction to prevent any concurrent write operations that tries to decrease the inventory at the same time. We can do that using the lightweight transaction \"IF\" statement as shown below:  First we check what is the current quantity of the particular product:  SELECT quantity FROM products WHERE id = 1;  Then to decrease this quantity during the order, we do it as below:  UPDATE products SET quantity = existing_quantity -1\nWHERE id = 1\nIF quantity = existing_quantity;  Then if we want to do a complete transaction, we will create an order only after the decreasing of the quantity is a successful as shown below:  UPDATE products SET quantity = 9\nWHERE id = 1\nIF quantity = 10;\n\nif(aboveQuerySuccessful())\n  insertNewOrder();\nelse\n retry();  Another example if we want to create a customer account and make sure that the user doesn't already exist, then we do it as shown below:  INSERT INTO CUSTOMERS (user, email, name)\nVALUES ('John', 'user@email.com', 'John Robert', 1)\nIF NOT EXISTS;  As seen above, unlike in relational database transactions, Cassandra lightweight transactions uses optimistic locking instead of pessimistic locking and doesn't have the common BEGIN/COMMIT/ROLLBACK statements. In Cassandra lightweight transactions, we can only ensure that a given sequence of operations are executed as a single unit.", 
            "title": "Transactions"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_layout/", 
            "text": "back\n\n\nModelling your data in Cassandra is probably one of the most difficult tasks especially if you come from a relational database background. Before talking about how you can model your data in Cassandra and what are the best principles that we need to follow to come up with an efficient data model, I will talk about very important Cassandra concepts that impact your data model design.\n\n\nPartition Key\n\n\nIn Cassandra the data is divided into different partitions and then distributed across the nodes for scalability. Each partition will have a portion of the data and will be stored as a map having different columns keys and values and this map is accessed by a row key called a partition key. Later if any node receives a read request from a client application, it will easily reroute the request to the node having this partition by simply checking which node holds this partition key. It is important that we understand how Cassandra query the data using partition key since this will impact our data model design. We define which column key will be used to be the partition key during the creation of the CQL table. You can do that by using the \"PRIMARY KEY\" statement. You can either use one column to create a single partition key or more than one column to create a composite partition key. An example is shown below to create a single or composite partition key:\n\n\ncreate table product_by_id (\n      id text,\n      name text,\n      price decimal,\n      size text,\n      PRIMARY KEY(id)      \n  );\n\n\n\n\nOr to create a composite key using id and name columns:\n\n\ncreate table product_by_id_name (\n      id text,\n      name text,\n      price decimal,\n      size text,\n      PRIMARY KEY((id,name))      \n  );\n\n\n\n\nIn the above example, you should query your product table by using both partition keys columns (id,name). For instance, if you run a query against the product id only, the query will fail since you need to provide the product name as well. So the below querY will fail:\n\n\nSELECT * from product_by_id_name where id = 1\n\n\n\n\nBut the below query will succeed:\n\n\nSELECT * from product_by_id_name where id =1 and name = \nproduct_name\n \n\n\n\n\nAs seen above, choosing the right partition key is so important to have a data model that will cover your query pattern. However, it is always possible to create secondary indexes on the fields that we want to query. More about creating indexes will be explained on a \nlater section\n.\n\n\nThe partition key should be chosen so that it can be used to distribute the data equally across the nodes. If you choose the partition key to be a boolean value where it can be either true or false, then you will have only two large partitions that can impact scalability. Additionally, Cassandra has a limit on the data that can be written to a particular partition which is currently around 2 billion rows. In the same way, if you choose the partition key to be a so unique such as a UUID column, then each partition will contain only one row and the ordering will have no value.  Therefore, it is so important to choose the partition key so carefully to be not too high or too low. Besides if you choose a wrong partition key, it can results on an unbalanced partitions which leads to hot spot issues. For example if the partition key is the product category, then we might have partitions having many products while other partitions having only few products. Hence the partitions that contain many products will result on a hot spot nodes that receives most of the read and write request which impacts scalability.\n\n\nClustering columns\n\n\nIn Cassandra, you can also define clustering columns which can be used if you want to sort your data. To define a clustering columns, you need to add them to the PRIMARY KEY definition after the partition key definition, an example is shown below:\n\n\ncreate table product_by_release_Year_and_size (\n      id text,\n      name text,\n      release_year text,\n      price decimal,\n      size text,\n      PRIMARY KEY((release_year,size), price )      \n  );\n\n\n\n\nAs seen above, we have defined our partition key to be a composite key containing the release_year and size columns. Then we have used the price column to be the clustering column that we will be used for sorting the product by their price inside each partition. This mean that products will be stored as partitions for each release_year and size values. Then inside each partition, the products will be sorted by the price.  The above primary key is called a compound primary key which contains a composite partition key and a clustering column. \n\n\nNow you can run queries to get the products released in a particular year and with a particular size sorted by their prices as seen below:\n\n\nSELECT * from product_by_release_Year_and_size where release_year =2015 and size = large ORDER BY price DESC LIMIT 50;\n\n\n\n\nYou can also run queries without the clustering columns like below:\n\n\nSELECT * from product_by_release_Year_and_size where release_year =2015 and size = large\n\n\n\n\nAdditionally, you can run parameterized or range queries against the clustering column as shown below:\n\n\nSELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and price = 1000\n\nSELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and price \n 1000\n\n\n\n\nChoosing the right clustering columns is so important in your data model design and needs to be done with care. For example the order of the clustering columns is so important since they are used for grouping the data. For instance, if you want to use two clustering columns for the product table like the price and color. Then the order of the clustering columns are (price,color), then your products will be grouped and sorted first by the price and then for each price, it will be grouped and sorted by color.  \n\n\nIf you have used more than one column for clustering, then you can run range queries only on the last clustering column after you specify the values of the preceding clustering columns. For example, if we have created the below table that has (price,color) clustering columns:\n\n\n````\ncreate table product_by_release_Year_and_size (\n      id text,\n      name text,\n      release_year text,\n      price decimal,\n      size text,\n      PRIMARY KEY((release_year,size), price, color )    \n\n  );\n\n\n\nThen you can run the below query:\n\n\n\n\n\nSELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and price = 1000 and color \n 20\n\n\n\nBut the below two queries will fail:\n\n\n\n\n\n\nSELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and price \n 1000 and color \n 20\n\n\nSELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and  color \n 20\n\n\n\n\n\n##### User-Defined and collection data types\n\nCassandra gives you the ability to create your own custom data type which can be used to embed objects in your data model. For example, if you have a customer table you might be interested to store the address as a separate custom data type field as shown below:\n\n\n\n\n\n\nCREATE TYPE address (\n  street text,\n  city text,\n  zip_code int,\n  phones set\n\n)\n\n\n\nThen you can create the customer table as shown below:\n\n\n\n\n\n\nCREATE TABLE customer (\n  name text,\n  address address,\n)\n\n\n\n\nCassandra also supports the use of collection data types such as lists, sets or maps that can contain either a normal or a custom data types. \n\nUsing Cassandra's user-defined and collection data types can impact your data model design by embedding related data from other tables since there is no support for joins in Cassandra.\n\n\n\n##### Data modelling principles\n\nData modelling design in Cassandra is very different from the relational databases. The first rule for good data modelling design in Cassandra is to forget everything we know about relational database data modelling techniques. Although in Cassandra we use similar terminologies such as tables, columns and rows. However, the closest thing to Cassandra data models are maps and not tables. The data in Cassandra is stored as map of maps where each row is like a map that we access by using the partition key.  Below I will talk about few principles that we need to follow to come up with an efficient data model in Cassandra. \n\n\n###### Data model design goals\n\nOne of the most important goals that we should keep in mind when designing our data model is how to distribute the data evenly across the different nodes. The distribution of the data across the different nodes is done by our partition key choice. For example, If you choose the partition key for the product table to be the product release year.  Then you might end up with partitions having different sizes since maybe in some years more products have been produced than in other years. This results in an unbalanced partitions which leads to more load on certain nodes in the cluster. This mean we should pick up a very good partition key. \n\nAnother important design goal is how to minimise the number of reads. This is also decided based on the partition key. If you have each partition resides in a different node, then we will have to do many reads to get our data which impacts the performance. This mean we should also choose a partition key that will results in partitions containing relatively large amount of related data to minimise the reads between different nodes.\n\n\n###### Data model design rules\n\nAfter keeping the above two design goals in mind, we can start designing our data model. Below I WILL explain the two most important design rules that we should keep in mind while designing our data model.\n\nFirst rule is that in Cassandra we design our data model around the query patterns and not around the entities, objects or the relationships that we have. This is relatively difficult if you don't know in details all the queries that you need in a new application that you are creating. However, it is always possible to think of the possible query patterns based on the functions or use cases that your application is planning to have. Also you might be able to guess the queries from your experience in the application domain. For example, in an B2C ecommerce application, you can guess that we will have products, orders, customer, supplier and categories and you can guess that an important queries would be to get all products under a specific category, get most sold products based on the completed order , etc ..   \n\n\nThe second rule is that it is absolutely to use denormalisation heavily if needed.  Writes in Cassandra are cheap and since joins aren't supported, we would need to do a lot of denormalisation to retrieve our data efficiently and improve performance.\n\n\nBased on the above two design rules, I will show below a simple example:\n\n\nAssuming we want to build a data model for a B2C application that serves the below two queries in a product and categories tables:\n\nQ1) get a product per id\nQ2) get a category per cat_id\nQ3) get all products under a particular category\nQ4) get the categories of a particular product\n\n\nFor the first query, we can create a table like below that can be used to serve only this query:\n\n\n\n\n\n\n\ncreate table product_by_id (\n      id text,\n      name text,\n      price decimal,\n      size text,\n      PRIMARY KEY(id)    \n\n  );\n\n\n\nThen you can easily run queries as below:\n\n\n\n\n\n\nSELECT * from product_by_id where id =1\n\n\n\nFor the second query, we can create another table that will serve this query as seen below:\n\n\n\n\n\n\ncreate table category_by_id (\n      id text,\n      name text,\n      PRIMARY KEY(id)    \n\n  );\n\n\n\nThen you can easily run queries as below:\n\n\n\n\n\n\nSELECT * from category_by_id where id =1\n\n\n\nFor the third query and since joins aren't supported, we should create a table like below:\n\n\n\n\n\n\ncreate table all_products_by_cat_id (\n      cat_id text,\n      prod_id text,\n      name text,\n      price decimal,\n      size text,\n      PRIMARY KEY(cat_id,prod_id )    \n\n  );\n\n\n\nThen you can easily run queries as below:\n\n\n\n\n\n\nSELECT * from all_products_by_cat_id where cat_id =1\n\n\n\n\nAnd for the last query, we redesign the product_by_id table so that it will store the category names as a set as shown below:\n\n\n\n\n\n\n\ncreate table product_by_id (\n      id text,\n      name text,\n      cat set\n\n      price decimal,\n      size text,\n      PRIMARY KEY(id)    \n\n  );\n\n\n\nThen you can easily run queries as below:\n\n\n\n\n\n\nSELECT cat from product_by_id where id =1\n````\n\n\nAs seen above we create a table for each query that we are planning to run later in our application even if that means so much data redundancy. We need also to think if this query is going to be run frequently or not and then think if it worths creating a separate table for it to improve the performance. \n\n\nIt is difficult to give strict rules when designing data model in Cassandra since you can come up with many different data models. So you need to think which design will be suitable for your application requirements.", 
            "title": "Data layout"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_layout/#back", 
            "text": "Modelling your data in Cassandra is probably one of the most difficult tasks especially if you come from a relational database background. Before talking about how you can model your data in Cassandra and what are the best principles that we need to follow to come up with an efficient data model, I will talk about very important Cassandra concepts that impact your data model design.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_layout/#partition-key", 
            "text": "In Cassandra the data is divided into different partitions and then distributed across the nodes for scalability. Each partition will have a portion of the data and will be stored as a map having different columns keys and values and this map is accessed by a row key called a partition key. Later if any node receives a read request from a client application, it will easily reroute the request to the node having this partition by simply checking which node holds this partition key. It is important that we understand how Cassandra query the data using partition key since this will impact our data model design. We define which column key will be used to be the partition key during the creation of the CQL table. You can do that by using the \"PRIMARY KEY\" statement. You can either use one column to create a single partition key or more than one column to create a composite partition key. An example is shown below to create a single or composite partition key:  create table product_by_id (\n      id text,\n      name text,\n      price decimal,\n      size text,\n      PRIMARY KEY(id)      \n  );  Or to create a composite key using id and name columns:  create table product_by_id_name (\n      id text,\n      name text,\n      price decimal,\n      size text,\n      PRIMARY KEY((id,name))      \n  );  In the above example, you should query your product table by using both partition keys columns (id,name). For instance, if you run a query against the product id only, the query will fail since you need to provide the product name as well. So the below querY will fail:  SELECT * from product_by_id_name where id = 1  But the below query will succeed:  SELECT * from product_by_id_name where id =1 and name =  product_name    As seen above, choosing the right partition key is so important to have a data model that will cover your query pattern. However, it is always possible to create secondary indexes on the fields that we want to query. More about creating indexes will be explained on a  later section .  The partition key should be chosen so that it can be used to distribute the data equally across the nodes. If you choose the partition key to be a boolean value where it can be either true or false, then you will have only two large partitions that can impact scalability. Additionally, Cassandra has a limit on the data that can be written to a particular partition which is currently around 2 billion rows. In the same way, if you choose the partition key to be a so unique such as a UUID column, then each partition will contain only one row and the ordering will have no value.  Therefore, it is so important to choose the partition key so carefully to be not too high or too low. Besides if you choose a wrong partition key, it can results on an unbalanced partitions which leads to hot spot issues. For example if the partition key is the product category, then we might have partitions having many products while other partitions having only few products. Hence the partitions that contain many products will result on a hot spot nodes that receives most of the read and write request which impacts scalability.", 
            "title": "Partition Key"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_layout/#clustering-columns", 
            "text": "In Cassandra, you can also define clustering columns which can be used if you want to sort your data. To define a clustering columns, you need to add them to the PRIMARY KEY definition after the partition key definition, an example is shown below:  create table product_by_release_Year_and_size (\n      id text,\n      name text,\n      release_year text,\n      price decimal,\n      size text,\n      PRIMARY KEY((release_year,size), price )      \n  );  As seen above, we have defined our partition key to be a composite key containing the release_year and size columns. Then we have used the price column to be the clustering column that we will be used for sorting the product by their price inside each partition. This mean that products will be stored as partitions for each release_year and size values. Then inside each partition, the products will be sorted by the price.  The above primary key is called a compound primary key which contains a composite partition key and a clustering column.   Now you can run queries to get the products released in a particular year and with a particular size sorted by their prices as seen below:  SELECT * from product_by_release_Year_and_size where release_year =2015 and size = large ORDER BY price DESC LIMIT 50;  You can also run queries without the clustering columns like below:  SELECT * from product_by_release_Year_and_size where release_year =2015 and size = large  Additionally, you can run parameterized or range queries against the clustering column as shown below:  SELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and price = 1000\n\nSELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and price   1000  Choosing the right clustering columns is so important in your data model design and needs to be done with care. For example the order of the clustering columns is so important since they are used for grouping the data. For instance, if you want to use two clustering columns for the product table like the price and color. Then the order of the clustering columns are (price,color), then your products will be grouped and sorted first by the price and then for each price, it will be grouped and sorted by color.    If you have used more than one column for clustering, then you can run range queries only on the last clustering column after you specify the values of the preceding clustering columns. For example, if we have created the below table that has (price,color) clustering columns:  ````\ncreate table product_by_release_Year_and_size (\n      id text,\n      name text,\n      release_year text,\n      price decimal,\n      size text,\n      PRIMARY KEY((release_year,size), price, color )     \n  );  \nThen you can run the below query:  SELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and price = 1000 and color   20  \nBut the below two queries will fail:  SELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and price   1000 and color   20  SELECT * from product_by_release_Year_and_size where release_year =2015 and size = large and  color   20  \n\n\n##### User-Defined and collection data types\n\nCassandra gives you the ability to create your own custom data type which can be used to embed objects in your data model. For example, if you have a customer table you might be interested to store the address as a separate custom data type field as shown below:  CREATE TYPE address (\n  street text,\n  city text,\n  zip_code int,\n  phones set \n)  \nThen you can create the customer table as shown below:  CREATE TABLE customer (\n  name text,\n  address address,\n)  \n\nCassandra also supports the use of collection data types such as lists, sets or maps that can contain either a normal or a custom data types. \n\nUsing Cassandra's user-defined and collection data types can impact your data model design by embedding related data from other tables since there is no support for joins in Cassandra.\n\n\n\n##### Data modelling principles\n\nData modelling design in Cassandra is very different from the relational databases. The first rule for good data modelling design in Cassandra is to forget everything we know about relational database data modelling techniques. Although in Cassandra we use similar terminologies such as tables, columns and rows. However, the closest thing to Cassandra data models are maps and not tables. The data in Cassandra is stored as map of maps where each row is like a map that we access by using the partition key.  Below I will talk about few principles that we need to follow to come up with an efficient data model in Cassandra. \n\n\n###### Data model design goals\n\nOne of the most important goals that we should keep in mind when designing our data model is how to distribute the data evenly across the different nodes. The distribution of the data across the different nodes is done by our partition key choice. For example, If you choose the partition key for the product table to be the product release year.  Then you might end up with partitions having different sizes since maybe in some years more products have been produced than in other years. This results in an unbalanced partitions which leads to more load on certain nodes in the cluster. This mean we should pick up a very good partition key. \n\nAnother important design goal is how to minimise the number of reads. This is also decided based on the partition key. If you have each partition resides in a different node, then we will have to do many reads to get our data which impacts the performance. This mean we should also choose a partition key that will results in partitions containing relatively large amount of related data to minimise the reads between different nodes.\n\n\n###### Data model design rules\n\nAfter keeping the above two design goals in mind, we can start designing our data model. Below I WILL explain the two most important design rules that we should keep in mind while designing our data model.\n\nFirst rule is that in Cassandra we design our data model around the query patterns and not around the entities, objects or the relationships that we have. This is relatively difficult if you don't know in details all the queries that you need in a new application that you are creating. However, it is always possible to think of the possible query patterns based on the functions or use cases that your application is planning to have. Also you might be able to guess the queries from your experience in the application domain. For example, in an B2C ecommerce application, you can guess that we will have products, orders, customer, supplier and categories and you can guess that an important queries would be to get all products under a specific category, get most sold products based on the completed order , etc ..   \n\n\nThe second rule is that it is absolutely to use denormalisation heavily if needed.  Writes in Cassandra are cheap and since joins aren't supported, we would need to do a lot of denormalisation to retrieve our data efficiently and improve performance.\n\n\nBased on the above two design rules, I will show below a simple example:\n\n\nAssuming we want to build a data model for a B2C application that serves the below two queries in a product and categories tables:\n\nQ1) get a product per id\nQ2) get a category per cat_id\nQ3) get all products under a particular category\nQ4) get the categories of a particular product\n\n\nFor the first query, we can create a table like below that can be used to serve only this query:  create table product_by_id (\n      id text,\n      name text,\n      price decimal,\n      size text,\n      PRIMARY KEY(id)     \n  );  \nThen you can easily run queries as below:  SELECT * from product_by_id where id =1  \nFor the second query, we can create another table that will serve this query as seen below:  create table category_by_id (\n      id text,\n      name text,\n      PRIMARY KEY(id)     \n  );  \nThen you can easily run queries as below:  SELECT * from category_by_id where id =1  \nFor the third query and since joins aren't supported, we should create a table like below:  create table all_products_by_cat_id (\n      cat_id text,\n      prod_id text,\n      name text,\n      price decimal,\n      size text,\n      PRIMARY KEY(cat_id,prod_id )     \n  );  \nThen you can easily run queries as below:  SELECT * from all_products_by_cat_id where cat_id =1  \n\nAnd for the last query, we redesign the product_by_id table so that it will store the category names as a set as shown below:  create table product_by_id (\n      id text,\n      name text,\n      cat set \n      price decimal,\n      size text,\n      PRIMARY KEY(id)     \n  );  \nThen you can easily run queries as below:  SELECT cat from product_by_id where id =1\n````  As seen above we create a table for each query that we are planning to run later in our application even if that means so much data redundancy. We need also to think if this query is going to be run frequently or not and then think if it worths creating a separate table for it to improve the performance.   It is difficult to give strict rules when designing data model in Cassandra since you can come up with many different data models. So you need to think which design will be suitable for your application requirements.", 
            "title": "Clustering columns"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_modeling_main/", 
            "text": "back\n\n\nData Layout\n\n\nRelational Data\n\n\nReferential Integrity \n\n\nNormalization\n\n\nNested Data Structures\n\n\nRefernces", 
            "title": "Data modeling main"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_modeling_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_modeling_main/#data-layout", 
            "text": "", 
            "title": "Data Layout"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_modeling_main/#relational-data", 
            "text": "", 
            "title": "Relational Data"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_modeling_main/#referential-integrity", 
            "text": "", 
            "title": "Referential Integrity"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_modeling_main/#normalization", 
            "text": "", 
            "title": "Normalization"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_modeling_main/#nested-data-structures", 
            "text": "", 
            "title": "Nested Data Structures"
        }, 
        {
            "location": "/Cassandra/Data Modeling/data_modeling_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Data Modeling/nested_structures/", 
            "text": "back\n\n\nAlthough in Cassandra you can use the collection data type such as lists, maps, and sets. However it is not possible to nest collections inside each other. Therefore, nesting data in Cassandra isn't supported.", 
            "title": "Nested structures"
        }, 
        {
            "location": "/Cassandra/Data Modeling/nested_structures/#back", 
            "text": "Although in Cassandra you can use the collection data type such as lists, maps, and sets. However it is not possible to nest collections inside each other. Therefore, nesting data in Cassandra isn't supported.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Data Modeling/normalisation/", 
            "text": "back\n\n\nAs explained in the previous sections, we usually use denormalization to design the data model in Cassandra.", 
            "title": "Normalisation"
        }, 
        {
            "location": "/Cassandra/Data Modeling/normalisation/#back", 
            "text": "As explained in the previous sections, we usually use denormalization to design the data model in Cassandra.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Data Modeling/referential_integerity/", 
            "text": "back\n\n\nThere is no such concept as referential integrity, joins, or foreign keys in Cassandra.", 
            "title": "Referential integerity"
        }, 
        {
            "location": "/Cassandra/Data Modeling/referential_integerity/#back", 
            "text": "There is no such concept as referential integrity, joins, or foreign keys in Cassandra.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Data Modeling/refernces/", 
            "text": "back\n\n\n1- http://docs.datastax.com/\n\n\n2- https://academy.datastax.com/courses/ds220-data-modeling\n\n\n3- http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/\n\n\n4- http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\n\n\n5- Cassandra: The Definitive Guide ,Eben Hewitt\n\n\n6- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Data Modeling/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Data Modeling/refernces/#1-httpdocsdatastaxcom", 
            "text": "", 
            "title": "1- http://docs.datastax.com/"
        }, 
        {
            "location": "/Cassandra/Data Modeling/refernces/#2-httpsacademydatastaxcomcoursesds220-data-modeling", 
            "text": "", 
            "title": "2- https://academy.datastax.com/courses/ds220-data-modeling"
        }, 
        {
            "location": "/Cassandra/Data Modeling/refernces/#3-httpwwwebaytechblogcom20120716cassandra-data-modeling-best-practices-part-1", 
            "text": "", 
            "title": "3- http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/"
        }, 
        {
            "location": "/Cassandra/Data Modeling/refernces/#4-httpwwwdatastaxcomdevblogbasic-rules-of-cassandra-data-modeling", 
            "text": "", 
            "title": "4- http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"
        }, 
        {
            "location": "/Cassandra/Data Modeling/refernces/#5-cassandra-the-definitive-guide-eben-hewitt", 
            "text": "", 
            "title": "5- Cassandra: The Definitive Guide ,Eben Hewitt"
        }, 
        {
            "location": "/Cassandra/Data Modeling/refernces/#6-practical-cassandra-a-developers-approach-by-eric-lubow-and-russell-bradberry", 
            "text": "", 
            "title": "6- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry"
        }, 
        {
            "location": "/Cassandra/Data Modeling/relational_data/", 
            "text": "back\n\n\nCassandra doesn't support joins which are used in relational databases to model relationships. Therefore, as explained in the previous section, it is absolutely ok to denormalise your data by creating more than one table for your queries and duplicate the data in each one of them. In this section, I will explain how to model relationships in Cassandra by walking through one simple example from a B2C application domain. \n\n\nAssuming you are having a B2C application where you want to run queries that needs to be run across different entities and through a many to many relationship. For example, we have two entities, a customer and a product and there is a many to many relationship between them. The customer can buy one or more products and the product can be bought by one or more customers. Assuming we want to run the below join queries :\n\n\nQ1) get all the products that were bought by a certain customer\n\n\nNow depending on how frequent this query will be running and which data to be returned from the query, we can a table to serve this query. Assuming we just want to get the product names and this query will be executed so frequently, maybe thousands of times. Then we will create a separate table having duplicate information from the customer and the product tables as shown below:\n\n\ncreate table bought_products_by_customer (\n      customer_id text,\n      product_id text,\n      product_name text,\n      PRIMARY KEY(customer_id,product_id)      \n  );\n\n\n\n\nThen you can easily run queries as below:\n\n\nSELECT product_name from bought_products_by_customer where customer_id =1\n\n\n\n\nIf you want to get the product name, size and color , then you can duplicate this data as well in the bought_products_by_customer table as shown below:\n\n\ncreate table bought_products_by_customer (\n      customer_id text,\n      product_id text,\n      size text,\n      color text,\n      product_name text,\n      PRIMARY KEY(customer_id,product_id )      \n  );\n\n\n\n\nThen you can easily run queries as below:\n\n\nSELECT size,color,product_name from bought_products_by_customer where customer_id =1\n\n\n\n\nQ2) get all customers that bought a certain product\n\n\nThis is the other way around for the first query and can be done in a very similar way as shown below:\n\n\ncreate table customers_bought_product (\n      product_id text,\n      customer_id text,\n      name text,\n      address text,\n      age text,\n      PRIMARY KEY(product_id,customer_id)      \n  );\n\n\n\n\nThen you can easily run queries as below:\n\n\nSELECT name,address,age from customers_bought_product where product_id =1", 
            "title": "Relational data"
        }, 
        {
            "location": "/Cassandra/Data Modeling/relational_data/#back", 
            "text": "Cassandra doesn't support joins which are used in relational databases to model relationships. Therefore, as explained in the previous section, it is absolutely ok to denormalise your data by creating more than one table for your queries and duplicate the data in each one of them. In this section, I will explain how to model relationships in Cassandra by walking through one simple example from a B2C application domain.   Assuming you are having a B2C application where you want to run queries that needs to be run across different entities and through a many to many relationship. For example, we have two entities, a customer and a product and there is a many to many relationship between them. The customer can buy one or more products and the product can be bought by one or more customers. Assuming we want to run the below join queries :  Q1) get all the products that were bought by a certain customer  Now depending on how frequent this query will be running and which data to be returned from the query, we can a table to serve this query. Assuming we just want to get the product names and this query will be executed so frequently, maybe thousands of times. Then we will create a separate table having duplicate information from the customer and the product tables as shown below:  create table bought_products_by_customer (\n      customer_id text,\n      product_id text,\n      product_name text,\n      PRIMARY KEY(customer_id,product_id)      \n  );  Then you can easily run queries as below:  SELECT product_name from bought_products_by_customer where customer_id =1  If you want to get the product name, size and color , then you can duplicate this data as well in the bought_products_by_customer table as shown below:  create table bought_products_by_customer (\n      customer_id text,\n      product_id text,\n      size text,\n      color text,\n      product_name text,\n      PRIMARY KEY(customer_id,product_id )      \n  );  Then you can easily run queries as below:  SELECT size,color,product_name from bought_products_by_customer where customer_id =1  Q2) get all customers that bought a certain product  This is the other way around for the first query and can be done in a very similar way as shown below:  create table customers_bought_product (\n      product_id text,\n      customer_id text,\n      name text,\n      address text,\n      age text,\n      PRIMARY KEY(product_id,customer_id)      \n  );  Then you can easily run queries as below:  SELECT name,address,age from customers_bought_product where product_id =1", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Examples/example_main/", 
            "text": "back\n\n\nTPC-H Queries\n\n\nReferences", 
            "title": "Example main"
        }, 
        {
            "location": "/Cassandra/Examples/example_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Examples/example_main/#tpc-h-queries", 
            "text": "", 
            "title": "TPC-H Queries"
        }, 
        {
            "location": "/Cassandra/Examples/example_main/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/Cassandra/Examples/references/", 
            "text": "1- http://www.tpc.org/tpch", 
            "title": "References"
        }, 
        {
            "location": "/Cassandra/Examples/references/#1-httpwwwtpcorgtpch", 
            "text": "", 
            "title": "1- http://www.tpc.org/tpch"
        }, 
        {
            "location": "/Cassandra/Examples/tpch/", 
            "text": "In this example, I will show how we can model the data of a complete B2C application and how to write complex queries on this data model. I am going to use the data model used by the \nTPC-H benchmark\n which is shown below.\n\n\n\n\nThe idea behind this example is to show how to use Cassandra to design a real B2C application data model and how to write advance SQL queries with joins, grouping and sorting using Cassandra query language (CQL). I have chosen three queries from the TPCH benchmark that I think will cover most of the common query capabilities of SQL. The chosen queries are Q1, Q3 and Q4 as will be explained in the following sections.\n\n\nThe data used as input for this example is a small set from the data generated using TPCH DBGEN tool. The data is stored in CSV files that correspond to each object in the TPCH benchmark schema shown above (customer, supplier, part, lineitem, order , etc ...). For more details about how to generate the data, please have a look at \nthis\n blog post. \n\n\nThe complete code of this example can be found in Github along with the steps on how to run it. You can also change the used input data by changing the content of the \ninput files\n stored in the \"data\" folder.\n\n\nPricing Summary Report Query (Q1)\n\n\nThis query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:\n\n\nselect\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate \n= date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;\n\n\n\n\nAs explained previously in the \ndata layout section\n, writes are cheap in Cassandra and it is absolutely ok to duplicate your data. We also model our data around the query patterns that we are planning to have in our application and not around the entities or relationships of the data model. This means that if we are going to frequently run the above query in our application, then it make sense to create a separate table for it to get better performance and to allow the data to scale efficiently. \n\n\nOfcourse first we will create the Keyspace that will hold the tables as seen below:\n\n\nCREATE KEYSPACE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};\n\n\n\n\nThen we will create a Cassandra table that will be used to serve the above query using as shown below:\n\n\nCREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 \n(\norderkey text,\nlinestatus text,\nreturnflag text,\nquantity double,\nextendedprice double,\ndiscount double,\ntax double,\nshipdate timestamp,\nPRIMARY KEY ((returnflag,linestatus),shipdate,orderkey,linenumber)\n);\n\n\n\n\nThe most important thing that you need to consider very carefully before creating CQL tables is the primary key. The primary key can be a single key, a composite key or a compound key and based on the primary key the data will be written to the partitions. If you use a primary key that is not unique, the data will be overwritten. This means you should always ensure that the primary key is unique to avoid overwriting your data. The primary key contains also the partition key which could be a single key or a composite key. The partition key defines how the data will be distributed across the different nodes, which means we should choose partition keys that allows for equal data distribution across the nodes and not creating what is called a hot spot in one of the partitions. A hot spot is created when the data is distributed unequally among the partitions where one of the partitions contains most of the data and hence receives most of the read and write requests. Additionally, Cassandra support only two billion columns per partition and after this limit it will reject the write requests. If you also choose a partition key that is too high (too unique),  then each partition will have very little data which will negate the benefits of the wide row data model and you will have too little data in each partition that worth ordering. For all these reasons, it is quite important that we choose a unique primary key to prevent data from overwriting and a good partition key that is not too high or too low so that it will allow for a good data distribution across the cluster. \n\n\nFrom the above SQL query (TPCH Q1), we have to perform aggregate functions such as sum, avg and count on the data inside the lineitem table grouped by the returnflag, and the linestatus. In Cassandra aggregates are always performed in the partition level, this means that the data will always be grouped by the partition key. Hence we will have to make sure that the linestatus and returnflag are part of the composite key that will make the partition key. However if we use only returnflag and linestatus as the primary key, then the data will be overwritten in each partition since they are not unique. For example, we could have multiple rows in the lineitem table having the same linestatus and returnflag. So we add the orderkey and the linenumber as clustering columns to the primary key to form a compound key which will make the primary key unique and will prevent overwriting the data. So the primary key is ((returnflag,linestatus),orderkey,linenumber). And since we want to filter results using the shipdate, then it would be a good idea to include the shipdate column in the compound key as clustering column. So now our compound primary key is  ((returnflag,linestatus),shipdate,orderkey,linenumber). The shipdate should be the first clustering column since we want to run range queries against it. If you put the shipdate as the second clustering column, then if you want to run range queries on it you would need first to specify the value of the first clustering column using either '=' or 'IN'.  The partition key (returnflag,linestatus) is good for data distribution since each partition will contain the data of a specific returnflag and linestatus.\n\n\nNow after creating the table we need to load it with data. This is done using the below function:\n\n\nprivate void loadTpchQ1() {\n\n        File file = new File(\nsrc/main/java/org/cassandra/tpcH/data/lineitem.txt\n);\n\n        try {\n            BufferedReader br = new BufferedReader(new FileReader(file));\n            String line;\n\n            while ((line = br.readLine()) != null) {\n                String[] lineFields = line.split(Pattern.quote(\n|\n));\n\n                LOGGER.info(line.toString());\n\n                String insertStatement = \nINSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 (orderkey,linenumber,linestatus,returnflag,quantity,\n +\n                        \nextendedprice,discount,tax,shipdate) VALUES\n +\n                        \n ('\n + lineFields[0] +\n                        \n','\n + lineFields[3] +\n                        \n','\n + lineFields[9] +\n                        \n','\n + lineFields[8] +\n                        \n',\n + Double.valueOf(lineFields[4]) +\n                        \n,\n + Double.valueOf(lineFields[5]) +\n                        \n,\n + Double.valueOf(lineFields[6]) +\n                        \n,\n + Double.valueOf(lineFields[7]) +\n                        \n,'\n + lineFields[10] + \n') IF NOT EXISTS;\n;\n\n                LOGGER.info(insertStatement);\n\n                session.execute(insertStatement);\n            }\n\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n}\n\n\n\n\nAfter loading the data, now if we run a \"select * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1;\" statement we will see the below:\n\n\n\n\nAs mentioned in a \nprevious sections\n, Cassandra supports some built in aggregate functions such as sum, avg, min, max and count. However if you want to perform some mathematical operations within the SELECT statement, you will have to create a separate user defined function for that. As seen in the SQL query, we need to perform the below mathematical operations in the SELECT statement:\n\n\nsum(l_extendedprice*(1-l_discount)) as sum_disc_price\n// And\nsum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge\n\n\n\n\nFor that we need to create two user defined functions as shown below:\n\n\nCREATE OR REPLACE FUNCTION  CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice (l_extendedprice double,l_discount double) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS 'return (Double.valueOf( l_extendedprice.doubleValue() *  (1.0 - l_discount.doubleValue() ) ));\n\n\n\n\nCREATE OR REPLACE FUNCTION CASSANDRA_EXAMPLE_KEYSPACE.fSumChargePrice (l_extendedprice double,l_discount double,l_tax double) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS 'return (Double.valueOf( l_extendedprice.doubleValue() *  (1.0 - l_discount.doubleValue() ) * (1.0 + l_tax.doubleValue()) ));';\n\n\n\n\nAfter creating the Keyspace, the CQL table, loaded the table with data ,  and created the two user defined functions fSumDiscPrice and fSumChargePrice, we can run a CQL SELECT query as shown below:\n\n\nSELECT \n returnflag,\n linestatus, \n sum(quantity) as sum_qty,\n sum(extendedprice) as sum_base_price,\n sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(extendedprice,discount)) as sum_disc_price,\n sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumChargePrice(extendedprice,discount,tax)) as sum_charge, \n avg(quantity) as avg_qty, avg(extendedprice) as avg_price, \n avg(discount) as avg_disc, \n count(*) as count_order \nFROM \n CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 \nWHERE \n shipdate \n '2000-01-01 22:00:00-0700'  \n and returnflag='N' \n and linestatus = 'O' ;\n\n\n\n\nNotice that we have specified the partition key values (returnflag and linestatus) which not what we want on the original query. We had to specify the partition key values since if you don't, Cassandra will aggregate the results across all the partitions in the cluster and gives wrong results beside it will be very slow. If you don't specify the partition key values, you will get a warning from Cassandra as shown below:\n\n\n\n\nBut this is not useful since we want to aggregate across the whole data in all the partition to get some kind of overall analytic report and not for each partition separately. To overcome this challenge, you can get the partition key values across all partitions using the below query :\n\n\nselect distinct  returnflag, linestatus from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 ;\n\n\n\n\nFor our example data, you will get only one row as shown below, but for different data you might get more than one row:\n\n\n\n\nThen you can iterate through the results and run the CQL aggregate query shown above for each partition keys as shown below in the java code sinppest:\n\n\n\nString getAllPartitionKeyValues = \nselect distinct  returnflag, linestatus from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 ;\n;\n\nResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n// get aggregate function per partition\nfor (Row row : rsAllPartitionKey.all()) {\n\nString q1Statement = \nSELECT returnflag, linestatus, sum(quantity) as sum_qty,sum(extendedprice) as sum_base_price, sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(extendedprice,discount)) as sum_disc_price\\n\n +                       \n, sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumChargePrice(extendedprice,discount,tax)) as sum_charge, avg(quantity) as avg_qty,\\n\n + \n\navg(extendedprice) as avg_price, avg(discount) as avg_disc, count(*) as count_order \\n\n +\n\nFROM CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 WHERE shipdate \n '2000-01-01 22:00:00-0700'  and returnflag = '\n + row.getString(\nreturnflag\n) + \n' and linestatus = '\n + row.getString(\nlinestatus\n) + \n' ALLOW FILTERING ; \n;\n\nResultSet rsQ1 = this.model.executeStatement(q1Statement);\n\nfor (Row innerRow : rsQ1.all())\n   rowsResult.add(innerRow);\n}\n\n\n\n\n\nYou might be wondering about the \"order by\" clause in the SQL query. Unfortunately, you can't group and order by the same columns in Cassandra. Grouping in Cassandra is done using the partition key while ordering happens inside each partition using what is called the clustering columns. To put it simple, you can't group by a column (use it as a partition key) and then order by the same column using the ORDER BY clause since you can order by only a clustering column. Although there is a way to order cluster by the partition keys using the Byte Ordered Partitioner (BOP), however it is discouraged because it can lead to hot spots and load balancing problems.\n\n\nThe complete java code for the api that will return the results of TPCH Q1 is shown below:\n\n\n@GET\n    @Path(\n/q1\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q1 using this model\n, notes = \nReturns String\n, response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public String getQ1Results() {\n\n\n        try {\n\n            this.model.connect();\n\n            ArrayList\nRow\n rowsResult = new ArrayList\nRow\n();\n\n            String createIndexOnCommitDate = \nCREATE INDEX IF NOT EXISTS shipdate_index ON CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 (shipdate)\n;\n\n            this.model.executeStatement(createIndexOnCommitDate);\n\n\n            String getAllPartitionKeyValues = \nselect distinct  returnflag, linestatus from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 ;\n;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n\n                String q1Statement = \nSELECT returnflag, linestatus, sum(quantity) as sum_qty,sum(extendedprice) as sum_base_price, sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(extendedprice,discount)) as sum_disc_price\\n\n +\n                        \n, sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumChargePrice(extendedprice,discount,tax)) as sum_charge, avg(quantity) as avg_qty,\\n\n +\n                        \navg(extendedprice) as avg_price, avg(discount) as avg_disc, count(*) as count_order \\n\n +\n                        \nFROM CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 WHERE shipdate \n '2000-01-01 22:00:00-0700'  and returnflag = '\n + row.getString(\nreturnflag\n) + \n' and linestatus = '\n + row.getString(\nlinestatus\n) + \n' ALLOW FILTERING ; \n;\n                ResultSet rsQ1 = this.model.executeStatement(q1Statement);\n\n                for (Row innerRow : rsQ1.all())\n                    rowsResult.add(innerRow);\n            }\n\n\n            this.model.close();\n\n            return rowsResult.toString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\nShipping Priority Query (Q3)\n\n\nThis query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:\n\n\nselect\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate \n date '[DATE]'\n and l_shipdate \n date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;\n\n\n\n\nSince joins aren't supported in Cassandra, we create a CQL table that has all the needed values from the different tables since duplicating your data is ok in Cassandra as mentioned in the \ndata layout section\n. We create the table as shown below:\n\n\n CREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\n(\norderkey text,\nlinenumber text,\no_orderdate timestamp,\no_shippriority text,\nc_mktsegment text,\nl_extendedprice double,\nl_discount double,\nl_shipdate timestamp,\nPRIMARY KEY ((orderkey,o_orderdate,o_shippriority),c_mktsegment,l_shipdate,linenumber)\n);\n\n\n\n\nSimilar to what we have done in Q1, we have chosen the partition key to be the columns that we are going to group with as shown in the SQL query (l_orderkey,o_orderdate,o_shippriority). This will let us run aggregate functions on them since the aggregate functions are run in partition level in Cassandra. Then we have added the linenumber to the compound primary key so that the primary key is unique and to avoid overwriting. I have added the c_mktsegment and the l_shipdate as clustering columns to the compound primary key since we need to query against them. If you don't want to use it as a clustering column, you can also create a separate secondary indexes for the columns you want to filter with. \n\n\nNow we need to load the table with data as shown below:\n\n\n\nprivate void loadTpchQ3() {\n\n        File file = new File(\nsrc/main/java/org/cassandra/tpcH/data/order.txt\n);\n\n\n        try {\n            BufferedReader br = new BufferedReader(new FileReader(file));\n            String line;\n\n            while ((line = br.readLine()) != null) {\n                String[] lineFields = line.split(Pattern.quote(\n|\n));\n\n                // get c_mktsegment based on the custkey\n                String c_mktsegment = null;\n\n                File customerFile = new File(\nsrc/main/java/org/cassandra/tpcH/data/customer.txt\n);\n                String customerLine;\n                BufferedReader customerBr = new BufferedReader(new FileReader(customerFile));\n\n                while ((customerLine = customerBr.readLine()) != null) {\n                    String[] customerLineFields = customerLine.split(Pattern.quote(\n|\n));\n\n                    if (lineFields[1].equals(customerLineFields[0])) {\n                        c_mktsegment = customerLineFields[6];\n                    }\n                }\n\n\n                File lineItems = new File(\nsrc/main/java/org/cassandra/tpcH/data/lineitem.txt\n);\n                String lineItemLine;\n                BufferedReader lineItemBr = new BufferedReader(new FileReader(lineItems));\n\n                while ((lineItemLine = lineItemBr.readLine()) != null) {\n                    String[] lineItemLineFields = lineItemLine.split(Pattern.quote(\n|\n));\n\n                    if (lineItemLineFields[0].equals(lineFields[0])) {\n\n                        String insertStatement = \nINSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 (orderkey,linenumber,o_orderdate,o_shippriority,\n +\n                                \nc_mktsegment,l_extendedprice,l_discount,l_shipdate) VALUES\n +\n                                \n ('\n + lineFields[0] +\n                                \n','\n + lineItemLineFields[3] +\n                                \n','\n + lineFields[4] +\n                                \n','\n + lineFields[5] +\n                                \n','\n + c_mktsegment +\n                                \n',\n + Double.valueOf(lineItemLineFields[5]) +\n                                \n,\n + Double.valueOf(lineItemLineFields[6]) +\n                                \n,'\n + lineItemLineFields[10] + \n') IF NOT EXISTS\n;\n\n                        ResultSet rs = session.execute(insertStatement);\n                        LOGGER.info(rs.toString());\n                    }\n                }\n            }\n\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n    }\n\n\n\n\n\n\nAfter the data is loaded in the table. If you run a \"select * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3;\", you will get the below:\n\n\n\n\nNow after we have created the table and loaded it with date, we can run aggregate query like the SQL version as shown below:\n\n\nSELECT orderkey,sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(l_extendedprice,l_discount)) as revenue, o_orderdate,l_shipdate, o_shippriority,linenumber from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\n where orderkey= 'somekey'  and o_orderdate = 'somedate' and o_shippriority='somepriority'  and c_mktsegment='Segement' and l_shipdate \n '1990-01-01';\n\n\n\n\nAs you can see, we have specified the partition key values since it is necessary to get correct aggregated results. However we can run it across all partitions by retrieving all the partition values and then iterate through them to run the above query in each partition as shown below:\n\n\n\nString getAllPartitionKeyValues = \nselect distinct  orderkey, o_orderdate,o_shippriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 ;\n;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n                String q3TempStatement = \nSELECT orderkey,sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(l_extendedprice,l_discount)) as revenue, o_orderdate,l_shipdate, o_shippriority,linenumber from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\\n\n +\n                        \n where orderkey= '\n + row.getString(\norderkey\n) + \n'  and o_orderdate = '\n + formatter.format(row.getTimestamp(\no_orderdate\n)) + \n' and o_shippriority='\n + row.getString(\no_shippriority\n) + \n'  and c_mktsegment='AUTOMOBILE' and l_shipdate \n '1990-01-01' ;\n;\n\n\n                ResultSet rsQ3Temp = this.model.executeStatement(q3TempStatement);\n\n\n                if (rsQ3Temp != null) {\n\n                    for (Row innerRow : rsQ3Temp.all()) {\n\n                        if (innerRow != null \n !innerRow.isNull(\norderkey\n)) {\n\n                        for (Row innerRow : rsQ1.all())\n                              rowsResult.add(innerRow);\n\n                        }\n                    }\n                }\n            }\n\n\n\n\n\nAs you can see from the SQL query, the results are sorted by the revenue which is a field that was created using an aggregate function during the SELECT statement. Unfortunately this is not easily done in Cassandra. The sorting columns (clustering columns) are defined during the table creation and not during the SELECT statement. One idea to solve this, is to create a Materialised View that will be created using a SELECT statement and then use the newly created column (revenue) to be the clustering column of the created Materialised View. However unfortunately Cassandra doesn't support user defined functions on the Materialised View that would be needed to create the revenue field ( sum(l_extendedprice*(1-l_discount)) as revenue). Another idea I used to overcome this challenge is by creating a temporary table to store the intermediate aggregate results, then query this table and finally delete it once finished. The temporary table has the below structure:\n\n\nCREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP\norderkey text,\nrevenue double,\no_orderdate timestamp,\no_shippriority text,\nPRIMARY KEY (orderkey,revenue,o_orderdate) \n)WITH CLUSTERING ORDER BY (revenue DESC, o_orderdate ASC);\n\n\n\n\nAs you can see, it has a simple partition key (orderkey) as it would contain only the results of the aggregate query. The revenue and o_orderdate are the clustering columns used for sorting. I have also specified the sorting order using the WITH CLUSTERING ORDER BY clause. \n\n\nNow you can load it with results of the aggregate query of each partition as show below:\n\n\n\nString getAllPartitionKeyValues = \nselect distinct  orderkey, o_orderdate,o_shippriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 ;\n;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n                String q3TempStatement = \nSELECT orderkey,sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(l_extendedprice,l_discount)) as revenue, o_orderdate,l_shipdate, o_shippriority,linenumber from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\\n\n +\n                        \n where orderkey= '\n + row.getString(\norderkey\n) + \n'  and o_orderdate = '\n + formatter.format(row.getTimestamp(\no_orderdate\n)) + \n' and o_shippriority='\n + row.getString(\no_shippriority\n) + \n'  and c_mktsegment='AUTOMOBILE' and l_shipdate \n '1990-01-01' ;\n;\n\n\n                ResultSet rsQ3Temp = this.model.executeStatement(q3TempStatement);\n\n\n                if (rsQ3Temp != null) {\n\n                    for (Row innerRow : rsQ3Temp.all()) {\n\n                        if (innerRow != null \n !innerRow.isNull(\norderkey\n)) {\n\n                            String insertStatement = \nINSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP (orderkey,revenue,o_orderdate,\n +\n                                    \no_shippriority) VALUES\n +\n                                    \n ('\n + innerRow.getString(\norderkey\n) +\n                                    \n',\n + Double.valueOf(innerRow.getDouble(\nrevenue\n)) +\n                                    \n,'\n + formatter.format(innerRow.getTimestamp(\no_orderdate\n)) +\n                                    \n','\n + innerRow.getString(\no_shippriority\n) + \n') IF NOT EXISTS;\n;\n\n                            this.model.executeStatement(insertStatement);\n\n                            LOGGER.info(\ninsert executed\n);\n                        }\n                    }\n                }\n            }\n\n\n\n\n\nThen you can run queries against this temporary table as below:\n\n\nSELECT * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP ;\n\n\n\n\nNotice that we don't use any ORDER BY clause since it will be sorted by the order specified when we created the table.  We get the results as shown below:\n\n\n\n\nThe complete api function used to retrieve Q3 is shown below:\n\n\n    @GET\n    @Path(\n/q3\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q3 using this model\n, notes = \nReturns String\n, response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public String getQ2Results() {\n\n\n        try {\n\n            String tpchQ3TempTable = \n CREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP\\n\n +\n                    \n (\\n\n +\n                    \n orderkey text,\\n\n +\n                    \n revenue double,\\n\n +\n                    \n o_orderdate timestamp,\\n\n +\n                    \n o_shippriority text,\\n\n +\n                    \n PRIMARY KEY (orderkey,revenue,o_orderdate) \\n\n +\n                    \n )WITH CLUSTERING ORDER BY (revenue DESC, o_orderdate ASC);\n;\n\n\n            this.model.connect();\n            this.model.executeStatement(tpchQ3TempTable);\n\n            String getAllPartitionKeyValues = \nselect distinct  orderkey, o_orderdate,o_shippriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 ;\n;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n                String q3TempStatement = \nSELECT orderkey,sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(l_extendedprice,l_discount)) as revenue, o_orderdate,l_shipdate, o_shippriority,linenumber from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\\n\n +\n                        \n where orderkey= '\n + row.getString(\norderkey\n) + \n'  and o_orderdate = '\n + formatter.format(row.getTimestamp(\no_orderdate\n)) + \n' and o_shippriority='\n + row.getString(\no_shippriority\n) + \n'  and c_mktsegment='AUTOMOBILE' and l_shipdate \n '1990-01-01' ;\n;\n\n\n                ResultSet rsQ3Temp = this.model.executeStatement(q3TempStatement);\n\n\n                if (rsQ3Temp != null) {\n\n                    for (Row innerRow : rsQ3Temp.all()) {\n\n                        if (innerRow != null \n !innerRow.isNull(\norderkey\n)) {\n\n                            String insertStatement = \nINSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP (orderkey,revenue,o_orderdate,\n +\n                                    \no_shippriority) VALUES\n +\n                                    \n ('\n + innerRow.getString(\norderkey\n) +\n                                    \n',\n + Double.valueOf(innerRow.getDouble(\nrevenue\n)) +\n                                    \n,'\n + formatter.format(innerRow.getTimestamp(\no_orderdate\n)) +\n                                    \n','\n + innerRow.getString(\no_shippriority\n) + \n') IF NOT EXISTS;\n;\n\n                            this.model.executeStatement(insertStatement);\n\n                            LOGGER.info(\ninsert executed\n);\n                        }\n                    }\n                }\n            }\n\n\n            String q3FinalStatement = \nSELECT * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP ; \n;\n\n\n            ResultSet rsQ3 = this.model.executeStatement(q3FinalStatement);\n\n            String dropQ3TempTable = \nDROP TABLE CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP;\n;\n\n            ResultSet rsQ3DropMV = this.model.executeStatement(dropQ3TempTable);\n            LOGGER.info(rsQ3DropMV.toString());\n\n            this.model.close();\n\n            return rsQ3.all().toString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\nOrder Priority Checking Query (Q4)\n\n\nThis query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:\n\n\nselect\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate \n= date '[DATE]'\n and o_orderdate \n date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate \n l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;\n\n\n\n\nIn a similar way like we have done in Q1 and Q3 , we create a new CQL table that will be used to serve the above query. The table is shown below:\n\n\nCREATE TABLE CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4\n(\norderkey text,\nlinenumber text,\no_orderpriority text,\no_orderdate timestamp,\nl_receiptdate timestamp,\nl_commitdate timestamp,\nPRIMARY KEY (o_orderpriority,o_orderdate,orderkey,linenumber)\n)\n\n\n\n\nWe have used the primary key to be a combination of o_orderpriority, o_orderdate, orderkey, and linenumber to ensure the uniqueness of the inserted data in each partition. The o_orderpriority is the partition key since it is the group by column in the original SQL query. And as explained before, aggregation in Cassandra is done in the partition level so we have to use the o_orderpriority as the partition key. Note that we have used o_orderdate as the first clustering column so that we can run range queries on it. However we will have to create a secondary index for the l_commitdate since we can't have two clustering columns and still be able to run range queries on both of them. Hence I have created a secondary index on the l_commitdate as shown below:\n\n\nCREATE INDEX IF NOT EXISTS l_commitdate_index_on_TPCH_Q4 ON CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 (l_commitdate);\n\n\n\n\nThen we load the table with data using the below method:\n\n\nprivate void loadTpchQ4() {\n\n        File file = new File(\nsrc/main/java/org/cassandra/tpcH/data/order.txt\n);\n\n\n        try {\n            BufferedReader br = new BufferedReader(new FileReader(file));\n            String line;\n\n            while ((line = br.readLine()) != null) {\n                String[] lineFields = line.split(Pattern.quote(\n|\n));\n\n                File lineItems = new File(\nsrc/main/java/org/cassandra/tpcH/data/lineitem.txt\n);\n                String lineItemLine;\n                BufferedReader lineItemBr = new BufferedReader(new FileReader(lineItems));\n\n                while ((lineItemLine = lineItemBr.readLine()) != null) {\n                    String[] lineItemLineFields = lineItemLine.split(Pattern.quote(\n|\n));\n\n                    if (lineItemLineFields[0].equals(lineFields[0])) {\n\n                        String insertStatement = \nINSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 (linenumber,orderkey,o_orderpriority,o_orderdate,l_receiptdate,\n +\n                                \nl_commitdate) VALUES\n +\n                                \n ('\n + lineItemLineFields[3] +\n                                \n','\n + lineFields[0] +\n                                \n','\n + lineFields[5] +\n                                \n','\n + lineFields[4] +\n                                \n','\n + lineItemLineFields[12] +\n                                \n','\n + lineItemLineFields[11] + \n') IF NOT EXISTS\n;\n\n                        ResultSet rs = session.execute(insertStatement);\n                        LOGGER.info(rs.toString());\n                    }\n                }\n            }\n\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n    }\n\n\n\n\n\n\nAfter loading the data, if you run a \"SELECT * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4;\" you will get the below results:\n\n\n\n\nNow we can run an aggregate query against our table but first we need to get all the partition keys. Then we can run the query against each partition as we have done in Q1 and Q3 as shown below:\n\n\n\nString getAllPartitionKeyValues = \nselect distinct  o_orderpriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 ;\n;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n\n                String q4Statement = \nSELECT o_orderpriority, count(*) as order_count from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 \\n\n +\n                        \nwhere  o_orderpriority ='\n + row.getString(\no_orderpriority\n) + \n' and o_orderdate \n=  '1990-01-01' and o_orderdate \n '2000-01-01' and l_commitdate \n '2000-01-01' ALLOW FILTERING \n;\n\n\n                ResultSet rsQ4 = this.model.executeStatement(q4Statement);\n\n                for (Row innerRow : rsQ4.all())\n                    rowsResult.add(innerRow);\n            }\n\n\n\n\n\nYou can notice that I couldn't run the SQL condition ( and l_commitdate \n l_receiptdate) in Cassandra since it isn't supported and you will get the below error:\n\n\n[Syntax error in CQL query] message=\nline 1:253 no viable alternative at input ';' (...\n '2000-01-01' and l_commitdate \n [l_receiptdate] ;)\u201d\n\\\n\n\n\n\nCassandra doesn't support having a condition of a column compared to another column within the same table. So I had to use a normal static value (and l_commitdate \n '2000-01-01') which not what the original query require. \n\n\nFinally, similar to the first query Q1 you can't order by the same column that you are grouping by. So I couldn't order the results by the o_orderpriority as in the original query since this column is the partition key and can't be used as a culturing column. \n\n\nBased on our data, the result of the query is as shown below:\n\n\n[Row[1-URGENT, 1], Row[5-LOW, 4]]\n\n\n\n\nThe complete api function that will get the results of Q4 is shown below:\n\n\n\n    @GET\n    @Path(\n/q4\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q4 using this model\n, notes = \nReturns String\n, response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public String getQ3Results() {\n\n        try {\n\n            String createIndexOnCommitDate = \nCREATE INDEX IF NOT EXISTS l_commitdate_index_on_TPCH_Q4 ON CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 (l_commitdate);\n;\n\n            this.model.connect();\n\n            ArrayList\nRow\n rowsResult = new ArrayList\nRow\n();\n\n\n            String getAllPartitionKeyValues = \nselect distinct  o_orderpriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 ;\n;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n\n                String q4Statement = \nSELECT o_orderpriority, count(*) as order_count from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 \\n\n +\n                        \nwhere  o_orderpriority ='\n + row.getString(\no_orderpriority\n) + \n' and o_orderdate \n=  '1990-01-01' and o_orderdate \n '2000-01-01' and l_commitdate \n '2000-01-01' ALLOW FILTERING \n;\n\n\n                ResultSet rsQ4 = this.model.executeStatement(q4Statement);\n\n                for (Row innerRow : rsQ4.all())\n                    rowsResult.add(innerRow);\n            }\n\n\n            this.model.executeStatement(createIndexOnCommitDate);\n\n            this.model.close();\n\n            return rowsResult.toString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }", 
            "title": "Tpch"
        }, 
        {
            "location": "/Cassandra/Examples/tpch/#pricing-summary-report-query-q1", 
            "text": "This query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:  select\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate  = date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;  As explained previously in the  data layout section , writes are cheap in Cassandra and it is absolutely ok to duplicate your data. We also model our data around the query patterns that we are planning to have in our application and not around the entities or relationships of the data model. This means that if we are going to frequently run the above query in our application, then it make sense to create a separate table for it to get better performance and to allow the data to scale efficiently.   Ofcourse first we will create the Keyspace that will hold the tables as seen below:  CREATE KEYSPACE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};  Then we will create a Cassandra table that will be used to serve the above query using as shown below:  CREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 \n(\norderkey text,\nlinestatus text,\nreturnflag text,\nquantity double,\nextendedprice double,\ndiscount double,\ntax double,\nshipdate timestamp,\nPRIMARY KEY ((returnflag,linestatus),shipdate,orderkey,linenumber)\n);  The most important thing that you need to consider very carefully before creating CQL tables is the primary key. The primary key can be a single key, a composite key or a compound key and based on the primary key the data will be written to the partitions. If you use a primary key that is not unique, the data will be overwritten. This means you should always ensure that the primary key is unique to avoid overwriting your data. The primary key contains also the partition key which could be a single key or a composite key. The partition key defines how the data will be distributed across the different nodes, which means we should choose partition keys that allows for equal data distribution across the nodes and not creating what is called a hot spot in one of the partitions. A hot spot is created when the data is distributed unequally among the partitions where one of the partitions contains most of the data and hence receives most of the read and write requests. Additionally, Cassandra support only two billion columns per partition and after this limit it will reject the write requests. If you also choose a partition key that is too high (too unique),  then each partition will have very little data which will negate the benefits of the wide row data model and you will have too little data in each partition that worth ordering. For all these reasons, it is quite important that we choose a unique primary key to prevent data from overwriting and a good partition key that is not too high or too low so that it will allow for a good data distribution across the cluster.   From the above SQL query (TPCH Q1), we have to perform aggregate functions such as sum, avg and count on the data inside the lineitem table grouped by the returnflag, and the linestatus. In Cassandra aggregates are always performed in the partition level, this means that the data will always be grouped by the partition key. Hence we will have to make sure that the linestatus and returnflag are part of the composite key that will make the partition key. However if we use only returnflag and linestatus as the primary key, then the data will be overwritten in each partition since they are not unique. For example, we could have multiple rows in the lineitem table having the same linestatus and returnflag. So we add the orderkey and the linenumber as clustering columns to the primary key to form a compound key which will make the primary key unique and will prevent overwriting the data. So the primary key is ((returnflag,linestatus),orderkey,linenumber). And since we want to filter results using the shipdate, then it would be a good idea to include the shipdate column in the compound key as clustering column. So now our compound primary key is  ((returnflag,linestatus),shipdate,orderkey,linenumber). The shipdate should be the first clustering column since we want to run range queries against it. If you put the shipdate as the second clustering column, then if you want to run range queries on it you would need first to specify the value of the first clustering column using either '=' or 'IN'.  The partition key (returnflag,linestatus) is good for data distribution since each partition will contain the data of a specific returnflag and linestatus.  Now after creating the table we need to load it with data. This is done using the below function:  private void loadTpchQ1() {\n\n        File file = new File( src/main/java/org/cassandra/tpcH/data/lineitem.txt );\n\n        try {\n            BufferedReader br = new BufferedReader(new FileReader(file));\n            String line;\n\n            while ((line = br.readLine()) != null) {\n                String[] lineFields = line.split(Pattern.quote( | ));\n\n                LOGGER.info(line.toString());\n\n                String insertStatement =  INSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 (orderkey,linenumber,linestatus,returnflag,quantity,  +\n                         extendedprice,discount,tax,shipdate) VALUES  +\n                          ('  + lineFields[0] +\n                         ','  + lineFields[3] +\n                         ','  + lineFields[9] +\n                         ','  + lineFields[8] +\n                         ',  + Double.valueOf(lineFields[4]) +\n                         ,  + Double.valueOf(lineFields[5]) +\n                         ,  + Double.valueOf(lineFields[6]) +\n                         ,  + Double.valueOf(lineFields[7]) +\n                         ,'  + lineFields[10] +  ') IF NOT EXISTS; ;\n\n                LOGGER.info(insertStatement);\n\n                session.execute(insertStatement);\n            }\n\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n}  After loading the data, now if we run a \"select * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1;\" statement we will see the below:   As mentioned in a  previous sections , Cassandra supports some built in aggregate functions such as sum, avg, min, max and count. However if you want to perform some mathematical operations within the SELECT statement, you will have to create a separate user defined function for that. As seen in the SQL query, we need to perform the below mathematical operations in the SELECT statement:  sum(l_extendedprice*(1-l_discount)) as sum_disc_price\n// And\nsum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge  For that we need to create two user defined functions as shown below:  CREATE OR REPLACE FUNCTION  CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice (l_extendedprice double,l_discount double) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS 'return (Double.valueOf( l_extendedprice.doubleValue() *  (1.0 - l_discount.doubleValue() ) ));  CREATE OR REPLACE FUNCTION CASSANDRA_EXAMPLE_KEYSPACE.fSumChargePrice (l_extendedprice double,l_discount double,l_tax double) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS 'return (Double.valueOf( l_extendedprice.doubleValue() *  (1.0 - l_discount.doubleValue() ) * (1.0 + l_tax.doubleValue()) ));';  After creating the Keyspace, the CQL table, loaded the table with data ,  and created the two user defined functions fSumDiscPrice and fSumChargePrice, we can run a CQL SELECT query as shown below:  SELECT \n returnflag,\n linestatus, \n sum(quantity) as sum_qty,\n sum(extendedprice) as sum_base_price,\n sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(extendedprice,discount)) as sum_disc_price,\n sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumChargePrice(extendedprice,discount,tax)) as sum_charge, \n avg(quantity) as avg_qty, avg(extendedprice) as avg_price, \n avg(discount) as avg_disc, \n count(*) as count_order \nFROM \n CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 \nWHERE \n shipdate   '2000-01-01 22:00:00-0700'  \n and returnflag='N' \n and linestatus = 'O' ;  Notice that we have specified the partition key values (returnflag and linestatus) which not what we want on the original query. We had to specify the partition key values since if you don't, Cassandra will aggregate the results across all the partitions in the cluster and gives wrong results beside it will be very slow. If you don't specify the partition key values, you will get a warning from Cassandra as shown below:   But this is not useful since we want to aggregate across the whole data in all the partition to get some kind of overall analytic report and not for each partition separately. To overcome this challenge, you can get the partition key values across all partitions using the below query :  select distinct  returnflag, linestatus from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 ;  For our example data, you will get only one row as shown below, but for different data you might get more than one row:   Then you can iterate through the results and run the CQL aggregate query shown above for each partition keys as shown below in the java code sinppest:  \nString getAllPartitionKeyValues =  select distinct  returnflag, linestatus from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 ; ;\n\nResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n// get aggregate function per partition\nfor (Row row : rsAllPartitionKey.all()) {\n\nString q1Statement =  SELECT returnflag, linestatus, sum(quantity) as sum_qty,sum(extendedprice) as sum_base_price, sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(extendedprice,discount)) as sum_disc_price\\n  +                        , sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumChargePrice(extendedprice,discount,tax)) as sum_charge, avg(quantity) as avg_qty,\\n  +  avg(extendedprice) as avg_price, avg(discount) as avg_disc, count(*) as count_order \\n  + FROM CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 WHERE shipdate   '2000-01-01 22:00:00-0700'  and returnflag = '  + row.getString( returnflag ) +  ' and linestatus = '  + row.getString( linestatus ) +  ' ALLOW FILTERING ;  ;\n\nResultSet rsQ1 = this.model.executeStatement(q1Statement);\n\nfor (Row innerRow : rsQ1.all())\n   rowsResult.add(innerRow);\n}  You might be wondering about the \"order by\" clause in the SQL query. Unfortunately, you can't group and order by the same columns in Cassandra. Grouping in Cassandra is done using the partition key while ordering happens inside each partition using what is called the clustering columns. To put it simple, you can't group by a column (use it as a partition key) and then order by the same column using the ORDER BY clause since you can order by only a clustering column. Although there is a way to order cluster by the partition keys using the Byte Ordered Partitioner (BOP), however it is discouraged because it can lead to hot spots and load balancing problems.  The complete java code for the api that will return the results of TPCH Q1 is shown below:  @GET\n    @Path( /q1 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q1 using this model , notes =  Returns String , response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public String getQ1Results() {\n\n\n        try {\n\n            this.model.connect();\n\n            ArrayList Row  rowsResult = new ArrayList Row ();\n\n            String createIndexOnCommitDate =  CREATE INDEX IF NOT EXISTS shipdate_index ON CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 (shipdate) ;\n\n            this.model.executeStatement(createIndexOnCommitDate);\n\n\n            String getAllPartitionKeyValues =  select distinct  returnflag, linestatus from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 ; ;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n\n                String q1Statement =  SELECT returnflag, linestatus, sum(quantity) as sum_qty,sum(extendedprice) as sum_base_price, sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(extendedprice,discount)) as sum_disc_price\\n  +\n                         , sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumChargePrice(extendedprice,discount,tax)) as sum_charge, avg(quantity) as avg_qty,\\n  +\n                         avg(extendedprice) as avg_price, avg(discount) as avg_disc, count(*) as count_order \\n  +\n                         FROM CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q1 WHERE shipdate   '2000-01-01 22:00:00-0700'  and returnflag = '  + row.getString( returnflag ) +  ' and linestatus = '  + row.getString( linestatus ) +  ' ALLOW FILTERING ;  ;\n                ResultSet rsQ1 = this.model.executeStatement(q1Statement);\n\n                for (Row innerRow : rsQ1.all())\n                    rowsResult.add(innerRow);\n            }\n\n\n            this.model.close();\n\n            return rowsResult.toString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }", 
            "title": "Pricing Summary Report Query (Q1)"
        }, 
        {
            "location": "/Cassandra/Examples/tpch/#shipping-priority-query-q3", 
            "text": "This query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:  select\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate   date '[DATE]'\n and l_shipdate   date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;  Since joins aren't supported in Cassandra, we create a CQL table that has all the needed values from the different tables since duplicating your data is ok in Cassandra as mentioned in the  data layout section . We create the table as shown below:   CREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\n(\norderkey text,\nlinenumber text,\no_orderdate timestamp,\no_shippriority text,\nc_mktsegment text,\nl_extendedprice double,\nl_discount double,\nl_shipdate timestamp,\nPRIMARY KEY ((orderkey,o_orderdate,o_shippriority),c_mktsegment,l_shipdate,linenumber)\n);  Similar to what we have done in Q1, we have chosen the partition key to be the columns that we are going to group with as shown in the SQL query (l_orderkey,o_orderdate,o_shippriority). This will let us run aggregate functions on them since the aggregate functions are run in partition level in Cassandra. Then we have added the linenumber to the compound primary key so that the primary key is unique and to avoid overwriting. I have added the c_mktsegment and the l_shipdate as clustering columns to the compound primary key since we need to query against them. If you don't want to use it as a clustering column, you can also create a separate secondary indexes for the columns you want to filter with.   Now we need to load the table with data as shown below:  \nprivate void loadTpchQ3() {\n\n        File file = new File( src/main/java/org/cassandra/tpcH/data/order.txt );\n\n\n        try {\n            BufferedReader br = new BufferedReader(new FileReader(file));\n            String line;\n\n            while ((line = br.readLine()) != null) {\n                String[] lineFields = line.split(Pattern.quote( | ));\n\n                // get c_mktsegment based on the custkey\n                String c_mktsegment = null;\n\n                File customerFile = new File( src/main/java/org/cassandra/tpcH/data/customer.txt );\n                String customerLine;\n                BufferedReader customerBr = new BufferedReader(new FileReader(customerFile));\n\n                while ((customerLine = customerBr.readLine()) != null) {\n                    String[] customerLineFields = customerLine.split(Pattern.quote( | ));\n\n                    if (lineFields[1].equals(customerLineFields[0])) {\n                        c_mktsegment = customerLineFields[6];\n                    }\n                }\n\n\n                File lineItems = new File( src/main/java/org/cassandra/tpcH/data/lineitem.txt );\n                String lineItemLine;\n                BufferedReader lineItemBr = new BufferedReader(new FileReader(lineItems));\n\n                while ((lineItemLine = lineItemBr.readLine()) != null) {\n                    String[] lineItemLineFields = lineItemLine.split(Pattern.quote( | ));\n\n                    if (lineItemLineFields[0].equals(lineFields[0])) {\n\n                        String insertStatement =  INSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 (orderkey,linenumber,o_orderdate,o_shippriority,  +\n                                 c_mktsegment,l_extendedprice,l_discount,l_shipdate) VALUES  +\n                                  ('  + lineFields[0] +\n                                 ','  + lineItemLineFields[3] +\n                                 ','  + lineFields[4] +\n                                 ','  + lineFields[5] +\n                                 ','  + c_mktsegment +\n                                 ',  + Double.valueOf(lineItemLineFields[5]) +\n                                 ,  + Double.valueOf(lineItemLineFields[6]) +\n                                 ,'  + lineItemLineFields[10] +  ') IF NOT EXISTS ;\n\n                        ResultSet rs = session.execute(insertStatement);\n                        LOGGER.info(rs.toString());\n                    }\n                }\n            }\n\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n    }  After the data is loaded in the table. If you run a \"select * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3;\", you will get the below:   Now after we have created the table and loaded it with date, we can run aggregate query like the SQL version as shown below:  SELECT orderkey,sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(l_extendedprice,l_discount)) as revenue, o_orderdate,l_shipdate, o_shippriority,linenumber from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\n where orderkey= 'somekey'  and o_orderdate = 'somedate' and o_shippriority='somepriority'  and c_mktsegment='Segement' and l_shipdate   '1990-01-01';  As you can see, we have specified the partition key values since it is necessary to get correct aggregated results. However we can run it across all partitions by retrieving all the partition values and then iterate through them to run the above query in each partition as shown below:  \nString getAllPartitionKeyValues =  select distinct  orderkey, o_orderdate,o_shippriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 ; ;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n                String q3TempStatement =  SELECT orderkey,sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(l_extendedprice,l_discount)) as revenue, o_orderdate,l_shipdate, o_shippriority,linenumber from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\\n  +\n                          where orderkey= '  + row.getString( orderkey ) +  '  and o_orderdate = '  + formatter.format(row.getTimestamp( o_orderdate )) +  ' and o_shippriority='  + row.getString( o_shippriority ) +  '  and c_mktsegment='AUTOMOBILE' and l_shipdate   '1990-01-01' ; ;\n\n\n                ResultSet rsQ3Temp = this.model.executeStatement(q3TempStatement);\n\n\n                if (rsQ3Temp != null) {\n\n                    for (Row innerRow : rsQ3Temp.all()) {\n\n                        if (innerRow != null   !innerRow.isNull( orderkey )) {\n\n                        for (Row innerRow : rsQ1.all())\n                              rowsResult.add(innerRow);\n\n                        }\n                    }\n                }\n            }  As you can see from the SQL query, the results are sorted by the revenue which is a field that was created using an aggregate function during the SELECT statement. Unfortunately this is not easily done in Cassandra. The sorting columns (clustering columns) are defined during the table creation and not during the SELECT statement. One idea to solve this, is to create a Materialised View that will be created using a SELECT statement and then use the newly created column (revenue) to be the clustering column of the created Materialised View. However unfortunately Cassandra doesn't support user defined functions on the Materialised View that would be needed to create the revenue field ( sum(l_extendedprice*(1-l_discount)) as revenue). Another idea I used to overcome this challenge is by creating a temporary table to store the intermediate aggregate results, then query this table and finally delete it once finished. The temporary table has the below structure:  CREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP\norderkey text,\nrevenue double,\no_orderdate timestamp,\no_shippriority text,\nPRIMARY KEY (orderkey,revenue,o_orderdate) \n)WITH CLUSTERING ORDER BY (revenue DESC, o_orderdate ASC);  As you can see, it has a simple partition key (orderkey) as it would contain only the results of the aggregate query. The revenue and o_orderdate are the clustering columns used for sorting. I have also specified the sorting order using the WITH CLUSTERING ORDER BY clause.   Now you can load it with results of the aggregate query of each partition as show below:  \nString getAllPartitionKeyValues =  select distinct  orderkey, o_orderdate,o_shippriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 ; ;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n                String q3TempStatement =  SELECT orderkey,sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(l_extendedprice,l_discount)) as revenue, o_orderdate,l_shipdate, o_shippriority,linenumber from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\\n  +\n                          where orderkey= '  + row.getString( orderkey ) +  '  and o_orderdate = '  + formatter.format(row.getTimestamp( o_orderdate )) +  ' and o_shippriority='  + row.getString( o_shippriority ) +  '  and c_mktsegment='AUTOMOBILE' and l_shipdate   '1990-01-01' ; ;\n\n\n                ResultSet rsQ3Temp = this.model.executeStatement(q3TempStatement);\n\n\n                if (rsQ3Temp != null) {\n\n                    for (Row innerRow : rsQ3Temp.all()) {\n\n                        if (innerRow != null   !innerRow.isNull( orderkey )) {\n\n                            String insertStatement =  INSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP (orderkey,revenue,o_orderdate,  +\n                                     o_shippriority) VALUES  +\n                                      ('  + innerRow.getString( orderkey ) +\n                                     ',  + Double.valueOf(innerRow.getDouble( revenue )) +\n                                     ,'  + formatter.format(innerRow.getTimestamp( o_orderdate )) +\n                                     ','  + innerRow.getString( o_shippriority ) +  ') IF NOT EXISTS; ;\n\n                            this.model.executeStatement(insertStatement);\n\n                            LOGGER.info( insert executed );\n                        }\n                    }\n                }\n            }  Then you can run queries against this temporary table as below:  SELECT * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP ;  Notice that we don't use any ORDER BY clause since it will be sorted by the order specified when we created the table.  We get the results as shown below:   The complete api function used to retrieve Q3 is shown below:      @GET\n    @Path( /q3 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q3 using this model , notes =  Returns String , response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public String getQ2Results() {\n\n\n        try {\n\n            String tpchQ3TempTable =   CREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP\\n  +\n                      (\\n  +\n                      orderkey text,\\n  +\n                      revenue double,\\n  +\n                      o_orderdate timestamp,\\n  +\n                      o_shippriority text,\\n  +\n                      PRIMARY KEY (orderkey,revenue,o_orderdate) \\n  +\n                      )WITH CLUSTERING ORDER BY (revenue DESC, o_orderdate ASC); ;\n\n\n            this.model.connect();\n            this.model.executeStatement(tpchQ3TempTable);\n\n            String getAllPartitionKeyValues =  select distinct  orderkey, o_orderdate,o_shippriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 ; ;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n                String q3TempStatement =  SELECT orderkey,sum(CASSANDRA_EXAMPLE_KEYSPACE.fSumDiscPrice(l_extendedprice,l_discount)) as revenue, o_orderdate,l_shipdate, o_shippriority,linenumber from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3\\n  +\n                          where orderkey= '  + row.getString( orderkey ) +  '  and o_orderdate = '  + formatter.format(row.getTimestamp( o_orderdate )) +  ' and o_shippriority='  + row.getString( o_shippriority ) +  '  and c_mktsegment='AUTOMOBILE' and l_shipdate   '1990-01-01' ; ;\n\n\n                ResultSet rsQ3Temp = this.model.executeStatement(q3TempStatement);\n\n\n                if (rsQ3Temp != null) {\n\n                    for (Row innerRow : rsQ3Temp.all()) {\n\n                        if (innerRow != null   !innerRow.isNull( orderkey )) {\n\n                            String insertStatement =  INSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP (orderkey,revenue,o_orderdate,  +\n                                     o_shippriority) VALUES  +\n                                      ('  + innerRow.getString( orderkey ) +\n                                     ',  + Double.valueOf(innerRow.getDouble( revenue )) +\n                                     ,'  + formatter.format(innerRow.getTimestamp( o_orderdate )) +\n                                     ','  + innerRow.getString( o_shippriority ) +  ') IF NOT EXISTS; ;\n\n                            this.model.executeStatement(insertStatement);\n\n                            LOGGER.info( insert executed );\n                        }\n                    }\n                }\n            }\n\n\n            String q3FinalStatement =  SELECT * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP ;  ;\n\n\n            ResultSet rsQ3 = this.model.executeStatement(q3FinalStatement);\n\n            String dropQ3TempTable =  DROP TABLE CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3_TEMP; ;\n\n            ResultSet rsQ3DropMV = this.model.executeStatement(dropQ3TempTable);\n            LOGGER.info(rsQ3DropMV.toString());\n\n            this.model.close();\n\n            return rsQ3.all().toString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }", 
            "title": "Shipping Priority Query (Q3)"
        }, 
        {
            "location": "/Cassandra/Examples/tpch/#order-priority-checking-query-q4", 
            "text": "This query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:  select\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate  = date '[DATE]'\n and o_orderdate   date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate   l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;  In a similar way like we have done in Q1 and Q3 , we create a new CQL table that will be used to serve the above query. The table is shown below:  CREATE TABLE CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4\n(\norderkey text,\nlinenumber text,\no_orderpriority text,\no_orderdate timestamp,\nl_receiptdate timestamp,\nl_commitdate timestamp,\nPRIMARY KEY (o_orderpriority,o_orderdate,orderkey,linenumber)\n)  We have used the primary key to be a combination of o_orderpriority, o_orderdate, orderkey, and linenumber to ensure the uniqueness of the inserted data in each partition. The o_orderpriority is the partition key since it is the group by column in the original SQL query. And as explained before, aggregation in Cassandra is done in the partition level so we have to use the o_orderpriority as the partition key. Note that we have used o_orderdate as the first clustering column so that we can run range queries on it. However we will have to create a secondary index for the l_commitdate since we can't have two clustering columns and still be able to run range queries on both of them. Hence I have created a secondary index on the l_commitdate as shown below:  CREATE INDEX IF NOT EXISTS l_commitdate_index_on_TPCH_Q4 ON CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 (l_commitdate);  Then we load the table with data using the below method:  private void loadTpchQ4() {\n\n        File file = new File( src/main/java/org/cassandra/tpcH/data/order.txt );\n\n\n        try {\n            BufferedReader br = new BufferedReader(new FileReader(file));\n            String line;\n\n            while ((line = br.readLine()) != null) {\n                String[] lineFields = line.split(Pattern.quote( | ));\n\n                File lineItems = new File( src/main/java/org/cassandra/tpcH/data/lineitem.txt );\n                String lineItemLine;\n                BufferedReader lineItemBr = new BufferedReader(new FileReader(lineItems));\n\n                while ((lineItemLine = lineItemBr.readLine()) != null) {\n                    String[] lineItemLineFields = lineItemLine.split(Pattern.quote( | ));\n\n                    if (lineItemLineFields[0].equals(lineFields[0])) {\n\n                        String insertStatement =  INSERT INTO CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 (linenumber,orderkey,o_orderpriority,o_orderdate,l_receiptdate,  +\n                                 l_commitdate) VALUES  +\n                                  ('  + lineItemLineFields[3] +\n                                 ','  + lineFields[0] +\n                                 ','  + lineFields[5] +\n                                 ','  + lineFields[4] +\n                                 ','  + lineItemLineFields[12] +\n                                 ','  + lineItemLineFields[11] +  ') IF NOT EXISTS ;\n\n                        ResultSet rs = session.execute(insertStatement);\n                        LOGGER.info(rs.toString());\n                    }\n                }\n            }\n\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n    }  After loading the data, if you run a \"SELECT * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4;\" you will get the below results:   Now we can run an aggregate query against our table but first we need to get all the partition keys. Then we can run the query against each partition as we have done in Q1 and Q3 as shown below:  \nString getAllPartitionKeyValues =  select distinct  o_orderpriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 ; ;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n\n                String q4Statement =  SELECT o_orderpriority, count(*) as order_count from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 \\n  +\n                         where  o_orderpriority ='  + row.getString( o_orderpriority ) +  ' and o_orderdate  =  '1990-01-01' and o_orderdate   '2000-01-01' and l_commitdate   '2000-01-01' ALLOW FILTERING  ;\n\n\n                ResultSet rsQ4 = this.model.executeStatement(q4Statement);\n\n                for (Row innerRow : rsQ4.all())\n                    rowsResult.add(innerRow);\n            }  You can notice that I couldn't run the SQL condition ( and l_commitdate   l_receiptdate) in Cassandra since it isn't supported and you will get the below error:  [Syntax error in CQL query] message= line 1:253 no viable alternative at input ';' (...  '2000-01-01' and l_commitdate   [l_receiptdate] ;)\u201d \\  Cassandra doesn't support having a condition of a column compared to another column within the same table. So I had to use a normal static value (and l_commitdate   '2000-01-01') which not what the original query require.   Finally, similar to the first query Q1 you can't order by the same column that you are grouping by. So I couldn't order the results by the o_orderpriority as in the original query since this column is the partition key and can't be used as a culturing column.   Based on our data, the result of the query is as shown below:  [Row[1-URGENT, 1], Row[5-LOW, 4]]  The complete api function that will get the results of Q4 is shown below:  \n    @GET\n    @Path( /q4 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q4 using this model , notes =  Returns String , response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public String getQ3Results() {\n\n        try {\n\n            String createIndexOnCommitDate =  CREATE INDEX IF NOT EXISTS l_commitdate_index_on_TPCH_Q4 ON CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 (l_commitdate); ;\n\n            this.model.connect();\n\n            ArrayList Row  rowsResult = new ArrayList Row ();\n\n\n            String getAllPartitionKeyValues =  select distinct  o_orderpriority from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 ; ;\n\n\n            ResultSet rsAllPartitionKey = this.model.executeStatement(getAllPartitionKeyValues);\n\n            // get aggregate function per partition\n            for (Row row : rsAllPartitionKey.all()) {\n\n                String q4Statement =  SELECT o_orderpriority, count(*) as order_count from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q4 \\n  +\n                         where  o_orderpriority ='  + row.getString( o_orderpriority ) +  ' and o_orderdate  =  '1990-01-01' and o_orderdate   '2000-01-01' and l_commitdate   '2000-01-01' ALLOW FILTERING  ;\n\n\n                ResultSet rsQ4 = this.model.executeStatement(q4Statement);\n\n                for (Row innerRow : rsQ4.all())\n                    rowsResult.add(innerRow);\n            }\n\n\n            this.model.executeStatement(createIndexOnCommitDate);\n\n            this.model.close();\n\n            return rowsResult.toString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }", 
            "title": "Order Priority Checking Query (Q4)"
        }, 
        {
            "location": "/Cassandra/Getting Started/getting_started_main/", 
            "text": "back\n\n\nOverview\n\n\nInstallation\n\n\nBasic Concepts\n\n\nWhen to Use\n\n\nRefernces", 
            "title": "Getting started main"
        }, 
        {
            "location": "/Cassandra/Getting Started/getting_started_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Getting Started/getting_started_main/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/Cassandra/Getting Started/getting_started_main/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/Cassandra/Getting Started/getting_started_main/#basic-concepts", 
            "text": "", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/Cassandra/Getting Started/getting_started_main/#when-to-use", 
            "text": "", 
            "title": "When to Use"
        }, 
        {
            "location": "/Cassandra/Getting Started/getting_started_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Getting Started/installation/", 
            "text": "back\n\n\nInstalling Cassandra is relatively easy and you can follow few steps to install it. Below are the step that can be followed to install Cassandra on most of the operating systems :\n\n\n1- Download the Cassandra distribution binaries from the \ndownload page\n.\n\n\n2- Use the below terminal command to extract the downloaded compressed file:\n\n\ntar -xvzf datastax-ddc-version_number-bin.tar.gz\n\n\n\n\n\n3-  Now Cassandra is ready to use, you can either use the default configurations or  customise the configurations by modifying the file \"cassandra.yaml\" in the below path:\n\n\ncd datastax-ddc-version_number/conf\n\n\n\n\n\n4- Now you can start Cassandra by using the below terminal command:\n\n\nbin/cassandra\n\n\n\n\nIf you are using linux-based systems, then you have also the option to use some package managers to install cassandra such as Yum or apt-get as shown below:\n\n\nUsing apt-get, you need first to add the Cassandra distribution repository to the /etc/apt/sources.list.d/cassandra.sources.list by using the below terminal command:\n\n\necho \ndeb http://debian.datastax.com/datastax-ddc 3.version_number main\n | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list\n\n\n\n\nThen you can use below terminal commands to install Cassandra:\n\n\nsudo apt-get update\n\nsudo apt-get install datastax-ddc\n\n\n\n\nIn a similar way, you can install Cassandra using Yum package manager by first adding the  Cassandra distribution repository to the /etc/yum.repos.d/datastax.repo by using below terminal command:\n\n\n[datastax-ddc]\nname = DataStax Repo for Apache Cassandra\nbaseurl = http://rpm.datastax.com/datastax-ddc/3.version_number enabled = 1\ngpgcheck = 0\n\n\n\n\nThen you can easily install it by using below terminal command:\n\n\nsudo yum install datastax-ddc", 
            "title": "Installation"
        }, 
        {
            "location": "/Cassandra/Getting Started/installation/#back", 
            "text": "Installing Cassandra is relatively easy and you can follow few steps to install it. Below are the step that can be followed to install Cassandra on most of the operating systems :  1- Download the Cassandra distribution binaries from the  download page .  2- Use the below terminal command to extract the downloaded compressed file:  tar -xvzf datastax-ddc-version_number-bin.tar.gz  3-  Now Cassandra is ready to use, you can either use the default configurations or  customise the configurations by modifying the file \"cassandra.yaml\" in the below path:  cd datastax-ddc-version_number/conf  4- Now you can start Cassandra by using the below terminal command:  bin/cassandra  If you are using linux-based systems, then you have also the option to use some package managers to install cassandra such as Yum or apt-get as shown below:  Using apt-get, you need first to add the Cassandra distribution repository to the /etc/apt/sources.list.d/cassandra.sources.list by using the below terminal command:  echo  deb http://debian.datastax.com/datastax-ddc 3.version_number main  | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list  Then you can use below terminal commands to install Cassandra:  sudo apt-get update\n\nsudo apt-get install datastax-ddc  In a similar way, you can install Cassandra using Yum package manager by first adding the  Cassandra distribution repository to the /etc/yum.repos.d/datastax.repo by using below terminal command:  [datastax-ddc]\nname = DataStax Repo for Apache Cassandra\nbaseurl = http://rpm.datastax.com/datastax-ddc/3.version_number enabled = 1\ngpgcheck = 0  Then you can easily install it by using below terminal command:  sudo yum install datastax-ddc", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Getting Started/overview/", 
            "text": "back\n\n\nApache Cassandra is an open source distributed database that is built above the amazon Dynamo data distribution design and the bigtable data model. It is built to manage and handle very large amount of data that can be deployed across many servers and still provide high availability and performance. Cassandra's ring architecture is designed to be a peer-to-peer decentralised system where there is no single point of failure. The architecture is masterless and all the nodes are equally important. This distributed nature of its architecture allows for better scalability , high availability and operations simplicity.  Additionally, Cassandra supports a SQL-like query language called CQL that can be used easily during the modelling and retrieving of the stored data.", 
            "title": "Overview"
        }, 
        {
            "location": "/Cassandra/Getting Started/overview/#back", 
            "text": "Apache Cassandra is an open source distributed database that is built above the amazon Dynamo data distribution design and the bigtable data model. It is built to manage and handle very large amount of data that can be deployed across many servers and still provide high availability and performance. Cassandra's ring architecture is designed to be a peer-to-peer decentralised system where there is no single point of failure. The architecture is masterless and all the nodes are equally important. This distributed nature of its architecture allows for better scalability , high availability and operations simplicity.  Additionally, Cassandra supports a SQL-like query language called CQL that can be used easily during the modelling and retrieving of the stored data.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Getting Started/refernces/", 
            "text": "back\n\n\n1- http://docs.datastax.com/en/\n\n\n2- Cassandra: The Definitive Guide ,Eben Hewitt\n\n\n3- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Getting Started/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Getting Started/refernces/#1-httpdocsdatastaxcomen", 
            "text": "", 
            "title": "1- http://docs.datastax.com/en/"
        }, 
        {
            "location": "/Cassandra/Getting Started/refernces/#2-cassandra-the-definitive-guide-eben-hewitt", 
            "text": "", 
            "title": "2- Cassandra: The Definitive Guide ,Eben Hewitt"
        }, 
        {
            "location": "/Cassandra/Getting Started/refernces/#3-practical-cassandra-a-developers-approach-by-eric-lubow-and-russell-bradberry", 
            "text": "", 
            "title": "3- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/", 
            "text": "back\n\n\nCassandra bases its data distribution design on Amazon's Dynamo and its data model on Google's Bigtable. In this section I will explain Cassandra's main concepts related to its data model and data distribution designs.\n\n\nNode\n\n\nThis is the basic infrastructure component used in Cassandra where you store your data. \n\n\nData Center\n\n\nA data centre in Cassandra is basically a group of related nodes. Different data centres are usually used to serve different workloads to prevent transactions from being impacted by other unrelated transactions. The replication configuration in Cassandra can be set on the data centre level or we can replicate the data across multiple data centres based on the replication factor. An example is if you have two data centres one to serve customers on the east side of the country \"DC-EAST\" and one to serve customers in the west side of the country \"DC-WEST\", but still they both share the same dataset.\n\n\nCluster\n\n\nA cluster in cassandra is the outermost container and can contain multiple data centres that contain multiple nodes or machines. You can't do replicate the data across clusters.\n\n\nA peer-to-peer Gossip protocol\n\n\nCassandra uses a peer-to-peer Gossip protocol that can be used by each node to discover the information and the state of the other nodes in the cluster. The protocol is a gossip protocol since the information is transferred from node to node in the cluster in a \"Gossip\" manner where nodes are transferring information that they know about any other nodes to their neighbours until all nodes knows about all other nodes in the cluster. This gossip information is retained and persisted in each node so that it can be used again once it restarts.\n\n\nA partitioner\n\n\nWhen you write the data to any node in a Cassandra cluster, it will be automatically sharded and distributed across all the nodes. Cassandra uses a partitioner to determine how to distribute the data across the nodes. Cassandra support multiple partitioners that can be used such as the Murmur3Partitioner. The Murmur3Partitioner uses a hash function (MurmurHash) that can be used to uniformly distribute the data across the cluster and it the default partitioner used by Cassandra since it provides faster hashing and improves the performance. So basically, the partityioners are hash functions used to compute the tokens (hashing the partition key) and later these tokens are used to distribute the data as well as read/write requests uniformally across all the nodes in the cluster based on the hash ranges (Consistent hashing). \n\n\nThe replication factor\n\n\nWhen you configure Cassandra, you need to set a replication factor which is used to determine the number of copies that should be kept for each written data. The replication factor is set in the cluster level and the data will be replicated to equally important replicas (not like in a master-slave replication). Generally, you should make the replication factor greater than one but not more than the number of nodes in the cluster. For example, if you set the replication factor to 3, there will be 3 copies stored in 3 different nodes for each data written. \n\n\nReplication placement strategy\n\n\nCassandra uses a replication strategy to determine which nodes to choose for replicating the data. There are different replication strategies that can be configured in Cassandra such as the SimpleStrategy or the NetworkTopologyStrategy. The SimpleStrategy  is the simplest strategy where it picks the first node in the ring to be the first replica then the second node will be the next node in the ring and so forth.  The NetworkTopologyStrategy is the recommended strategy since as the name indicates, it is aware of the network topology (location of the nodes in the data centres and racks). This means it is more intelligent than the simple strategy and it will try to choose different nodes in different racks for distribution which reduces the failures because usually the nodes in the same rack tends to fail together. \n\n\nSnitch\n\n\nCassandra uses a snitch to listen to all the machines in a cluster and monitor nodes and network performance. Then it can provide this information about the state of the nodes to the replication strategy to decide the best nodes that can be used for replicating the data. Cassandra supports different type of snitches such as the dynamic snitch which is enabled by default. Dynamic snitch monitors the performance of the different nodes such as the read latency and then use this information to decide where to replicate and from which replica to read the data. For example if a read request has been received to one of the nodes that doesn't have the data, then this node needs to route this request to the nodes that are having the data. The node then will use the dynamic snitch information to decide to which replica node it should forward the read request.\n\n\nCQL\n\n\nCassandra uses a SQL-like query language called CQL which stands for Cassandra Query Language that can be used to communicate with the Cassandra database. Additionally, Cassandra provides a CQL shell called cqlsh that can be used to interact with CQL. Cassandra supports also a graphical tool called Datastax DevCenter and many drivers for different programming languages to programatically interact with CQL.\n\n\nKeyspace\n\n\nCassandra uses containers called Keyspaces which can be used to store multiple CQL tables and is closely compared to a database schema in relational databases. A good practice is to create one keyspace per application but it is also fine to create multiple keyspace per application. A Keyspace can be assigned a name and a set of attributes to define its behaviour. An example of the attributes that can be set for a Keyspace is the replication factor and the replication strategy. \n\n\nA CQL table\n\n\nIn Cassandra, the data is stored as columns that are accessible through a primary row key. The collection of multiple ordered columns that are having a primary row key is called a CQL table. \n\n\nSupported data types\n\n\nCQL supports many built-in data types including a support for various types of collections such as maps, lists, and sets.  Below are a list of the most used data types:\n\n\n\n\n\n\n\n\n\n\n\nCQL Type\n\n\n\nConstants\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nascii\n\n\n\nstrings\n\n\n\nUS-ASCII character string\n\n\n\n\n\n\n\n\n\nbigint\n\n\n\nintegers\n\n\n\n64-bit signed long\n\n\n\n\n\n\n\n\n\nblob\n\n\n\nblobs\n\n\n\nArbitrary bytes \n\n\n\n\n\n\n\n\n\nboolean\n\n\n\nbooleans\n\n\n\ntrue or false\n\n\n\n\n\n\n\n\n\ncounter\n\n\n\nintegers\n\n\n\nDistributed counter value (64-bit long)\n\n\n\n\n\n\n\n\n\ndecimal\n\n\n\nintegers, floats\n\n\n\nVariable-precision decimal\n\nJava type\n\n\n\n\n\n\n\n\n\n\n\n\ndouble\n\n\n\nintegers, floats\n\n\n\n64-bit IEEE-754 floating point\n\nJava type\n\n\n\n\n\n\n\n\n\n\n\nfloat\n\n\n\nintegers, floats\n\n\n\n32-bit IEEE-754 floating point\n\nJava type\n\n\n\n\n\n\n\n\n\ninet\n\n\n\nstrings\n\n\n\nIP address string in IPv4 or IPv6 format\n\n\n\n\n\n\n\n\n\nint\n\n\n\nintegers\n\n\n\n32-bit signed integer\n\n\n\n\n\n\n\n\n\nlist\n\n\n\nn/a\n\n\n\nA collection of one or more ordered elements\n\n\n\n\n\n\n\n\n\nmap\n\n\n\nn/a\n\n\n\nA JSON-style array of literals: { literal : literal, literal : literal ... }\n\n\n\n\n\n\n\n\n\nset\n\n\n\nn/a\n\n\n\nA collection of one or more elements\n\n\n\n\n\n\n\n\n\ntext\n\n\n\nstrings\n\n\n\nUTF-8 encoded string\n\n\n\n\n\n\n\n\n\ntimestamp\n\n\n\nintegers, strings\n\n\n\nDate plus time, encoded as 8 bytes since epoch\n\n\n\n\n\n\n\n\n\ntimeuuid\n\n\n\nuuids\n\n\n\nType 1 UUID only\n\n\n\n\n\n\n\n\n\ntuple\n\n\n\nn/a\n\n\n\n A group of 2-3 fields.\n\n\n\n\n\n\n\n\n\nuuid\n\n\n\nuuids\n\n\n\nA UUID\n\n\n\n\n\n\n\n\n\nvarchar\n\n\n\nstrings\n\n\n\nUTF-8 encoded string\n\n\n\n\n\n\n\n\n\nvarint\n\n\n\nintegers\n\n\n\nArbitrary-precision integer\n\nJava type\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting/reading path\n\n\nThe data in Cassandra is first written to a commit log in the nodes, then the data will be captured and stored in a mem-table. Later when the mem-table is full, it will be flushed and stored in the SStable data file for durability. In the same time, all data will be replicated across all replicas in the cluster. A read operation can be done from any of the nodes, where it will first check if the data exists in the mem-table , else it will be routed to the appropriate SStable that holds the requested data. Below I will show the steps followed in Cassandra whenever a data is written:\n\n\n1- First data will be appended to the commit log which will be persisted in disk and used later to recover data whenever the node crashed or shut down. The data in the commit log will be purged after the corresponding data in memtable is written to the SStable. \n\n\n2- The data will be written to the MemTable which is a write back cache that Cassandra uses to look up for data before looking into the disk. The data in the MemTable will be removed whenever the MemTable reach a configurable size limit.\n\n\n3- Whenever the MemTable is full and reached the size limit, the data will be flushed from the MemTable to the SSTable. Once the data is flushed from the MemTable, the corresponding data in the commit log will be also removed.\n\n\n4- Then the data will be stored in the SSTable. The SSTable is immutable which means it can't be overwritten. If a row column values are updated, then a new row with another timestamp version will be stored in the SSTable. The SSTable contains a partition index to be used in mapping partition keys with the row data in the SSTable files and a bloom filter which is a structure stored in memory than can be used to check if the row data exists in the MemTable or not before searching for it in the SSTable. \n\n\nThe below figure shows the write path in Cassandra:\n\n\n \n\n\nThe data is deleted in Cassandra by adding a flag to the data to be deleted in the SSTable. The flag called a tombstone and once a data is marked with a tombstone, it will be deleted after a configurable time called gc_grace_seconds. The data is removed during the compaction process which will be explained in the following section.\n\n\nCompaction\n\n\nThe written data in Cassandra aren't inserted or updated in place because the data in the SSTable can't be overwritten since the SSTable is immutable. Hence, Cassandra inserts a new timestamped version of the data whenever an insert or update occurs. The new timestamped version of the data will be kept in another SSTable. Additionally, the deleted data will not happen in place but instead the data is marked with a tombstone flag to be deleted after the the pre-configured gc_grace_seconds value. Because of this, after sometime there will be so many versions available for the same row in different SSTables. Therefore, Cassandra runs periodically a process called Compaction that will merges all the different versions of the data by selecting the latest version of the data based on the timestamps and create a new SSTable. The merging is done by the partition key and the data marked for deletion will be purged from the new SSTable. The old SSTable files will be deleted as soon as the process is completed and once the final merged SSTable is available. The compaction process is show in the below figure:", 
            "title": "Underline data structures"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#back", 
            "text": "Cassandra bases its data distribution design on Amazon's Dynamo and its data model on Google's Bigtable. In this section I will explain Cassandra's main concepts related to its data model and data distribution designs.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#node", 
            "text": "This is the basic infrastructure component used in Cassandra where you store your data.", 
            "title": "Node"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#data-center", 
            "text": "A data centre in Cassandra is basically a group of related nodes. Different data centres are usually used to serve different workloads to prevent transactions from being impacted by other unrelated transactions. The replication configuration in Cassandra can be set on the data centre level or we can replicate the data across multiple data centres based on the replication factor. An example is if you have two data centres one to serve customers on the east side of the country \"DC-EAST\" and one to serve customers in the west side of the country \"DC-WEST\", but still they both share the same dataset.", 
            "title": "Data Center"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#cluster", 
            "text": "A cluster in cassandra is the outermost container and can contain multiple data centres that contain multiple nodes or machines. You can't do replicate the data across clusters.", 
            "title": "Cluster"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#a-peer-to-peer-gossip-protocol", 
            "text": "Cassandra uses a peer-to-peer Gossip protocol that can be used by each node to discover the information and the state of the other nodes in the cluster. The protocol is a gossip protocol since the information is transferred from node to node in the cluster in a \"Gossip\" manner where nodes are transferring information that they know about any other nodes to their neighbours until all nodes knows about all other nodes in the cluster. This gossip information is retained and persisted in each node so that it can be used again once it restarts.", 
            "title": "A peer-to-peer Gossip protocol"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#a-partitioner", 
            "text": "When you write the data to any node in a Cassandra cluster, it will be automatically sharded and distributed across all the nodes. Cassandra uses a partitioner to determine how to distribute the data across the nodes. Cassandra support multiple partitioners that can be used such as the Murmur3Partitioner. The Murmur3Partitioner uses a hash function (MurmurHash) that can be used to uniformly distribute the data across the cluster and it the default partitioner used by Cassandra since it provides faster hashing and improves the performance. So basically, the partityioners are hash functions used to compute the tokens (hashing the partition key) and later these tokens are used to distribute the data as well as read/write requests uniformally across all the nodes in the cluster based on the hash ranges (Consistent hashing).", 
            "title": "A partitioner"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#the-replication-factor", 
            "text": "When you configure Cassandra, you need to set a replication factor which is used to determine the number of copies that should be kept for each written data. The replication factor is set in the cluster level and the data will be replicated to equally important replicas (not like in a master-slave replication). Generally, you should make the replication factor greater than one but not more than the number of nodes in the cluster. For example, if you set the replication factor to 3, there will be 3 copies stored in 3 different nodes for each data written.", 
            "title": "The replication factor"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#replication-placement-strategy", 
            "text": "Cassandra uses a replication strategy to determine which nodes to choose for replicating the data. There are different replication strategies that can be configured in Cassandra such as the SimpleStrategy or the NetworkTopologyStrategy. The SimpleStrategy  is the simplest strategy where it picks the first node in the ring to be the first replica then the second node will be the next node in the ring and so forth.  The NetworkTopologyStrategy is the recommended strategy since as the name indicates, it is aware of the network topology (location of the nodes in the data centres and racks). This means it is more intelligent than the simple strategy and it will try to choose different nodes in different racks for distribution which reduces the failures because usually the nodes in the same rack tends to fail together.", 
            "title": "Replication placement strategy"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#snitch", 
            "text": "Cassandra uses a snitch to listen to all the machines in a cluster and monitor nodes and network performance. Then it can provide this information about the state of the nodes to the replication strategy to decide the best nodes that can be used for replicating the data. Cassandra supports different type of snitches such as the dynamic snitch which is enabled by default. Dynamic snitch monitors the performance of the different nodes such as the read latency and then use this information to decide where to replicate and from which replica to read the data. For example if a read request has been received to one of the nodes that doesn't have the data, then this node needs to route this request to the nodes that are having the data. The node then will use the dynamic snitch information to decide to which replica node it should forward the read request.", 
            "title": "Snitch"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#cql", 
            "text": "Cassandra uses a SQL-like query language called CQL which stands for Cassandra Query Language that can be used to communicate with the Cassandra database. Additionally, Cassandra provides a CQL shell called cqlsh that can be used to interact with CQL. Cassandra supports also a graphical tool called Datastax DevCenter and many drivers for different programming languages to programatically interact with CQL.", 
            "title": "CQL"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#keyspace", 
            "text": "Cassandra uses containers called Keyspaces which can be used to store multiple CQL tables and is closely compared to a database schema in relational databases. A good practice is to create one keyspace per application but it is also fine to create multiple keyspace per application. A Keyspace can be assigned a name and a set of attributes to define its behaviour. An example of the attributes that can be set for a Keyspace is the replication factor and the replication strategy.", 
            "title": "Keyspace"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#a-cql-table", 
            "text": "In Cassandra, the data is stored as columns that are accessible through a primary row key. The collection of multiple ordered columns that are having a primary row key is called a CQL table.", 
            "title": "A CQL table"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#supported-data-types", 
            "text": "CQL supports many built-in data types including a support for various types of collections such as maps, lists, and sets.  Below are a list of the most used data types:     CQL Type  Constants  Description      ascii  strings  US-ASCII character string    bigint  integers  64-bit signed long    blob  blobs  Arbitrary bytes     boolean  booleans  true or false    counter  integers  Distributed counter value (64-bit long)    decimal  integers, floats  Variable-precision decimal\n\nJava type    double  integers, floats  64-bit IEEE-754 floating point\n\nJava type    float  integers, floats  32-bit IEEE-754 floating point\n\nJava type   inet  strings  IP address string in IPv4 or IPv6 format    int  integers  32-bit signed integer    list  n/a  A collection of one or more ordered elements    map  n/a  A JSON-style array of literals: { literal : literal, literal : literal ... }    set  n/a  A collection of one or more elements    text  strings  UTF-8 encoded string    timestamp  integers, strings  Date plus time, encoded as 8 bytes since epoch    timeuuid  uuids  Type 1 UUID only    tuple  n/a   A group of 2-3 fields.    uuid  uuids  A UUID    varchar  strings  UTF-8 encoded string    varint  integers  Arbitrary-precision integer\n\nJava type", 
            "title": "Supported data types"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#writingreading-path", 
            "text": "The data in Cassandra is first written to a commit log in the nodes, then the data will be captured and stored in a mem-table. Later when the mem-table is full, it will be flushed and stored in the SStable data file for durability. In the same time, all data will be replicated across all replicas in the cluster. A read operation can be done from any of the nodes, where it will first check if the data exists in the mem-table , else it will be routed to the appropriate SStable that holds the requested data. Below I will show the steps followed in Cassandra whenever a data is written:  1- First data will be appended to the commit log which will be persisted in disk and used later to recover data whenever the node crashed or shut down. The data in the commit log will be purged after the corresponding data in memtable is written to the SStable.   2- The data will be written to the MemTable which is a write back cache that Cassandra uses to look up for data before looking into the disk. The data in the MemTable will be removed whenever the MemTable reach a configurable size limit.  3- Whenever the MemTable is full and reached the size limit, the data will be flushed from the MemTable to the SSTable. Once the data is flushed from the MemTable, the corresponding data in the commit log will be also removed.  4- Then the data will be stored in the SSTable. The SSTable is immutable which means it can't be overwritten. If a row column values are updated, then a new row with another timestamp version will be stored in the SSTable. The SSTable contains a partition index to be used in mapping partition keys with the row data in the SSTable files and a bloom filter which is a structure stored in memory than can be used to check if the row data exists in the MemTable or not before searching for it in the SSTable.   The below figure shows the write path in Cassandra:     The data is deleted in Cassandra by adding a flag to the data to be deleted in the SSTable. The flag called a tombstone and once a data is marked with a tombstone, it will be deleted after a configurable time called gc_grace_seconds. The data is removed during the compaction process which will be explained in the following section.", 
            "title": "Writing/reading path"
        }, 
        {
            "location": "/Cassandra/Getting Started/underline_data_structures/#compaction", 
            "text": "The written data in Cassandra aren't inserted or updated in place because the data in the SSTable can't be overwritten since the SSTable is immutable. Hence, Cassandra inserts a new timestamped version of the data whenever an insert or update occurs. The new timestamped version of the data will be kept in another SSTable. Additionally, the deleted data will not happen in place but instead the data is marked with a tombstone flag to be deleted after the the pre-configured gc_grace_seconds value. Because of this, after sometime there will be so many versions available for the same row in different SSTables. Therefore, Cassandra runs periodically a process called Compaction that will merges all the different versions of the data by selecting the latest version of the data based on the timestamps and create a new SSTable. The merging is done by the partition key and the data marked for deletion will be purged from the new SSTable. The old SSTable files will be deleted as soon as the process is completed and once the final merged SSTable is available. The compaction process is show in the below figure:", 
            "title": "Compaction"
        }, 
        {
            "location": "/Cassandra/Getting Started/when_to_use/", 
            "text": "back\n\n\nMany big companies are already using Cassandra such as Apple, eBay and Netflix. One of the most common use cases for using Cassandra is to deal with time-series data. This mean storing large amount of data which are sequence of successive events that are related to some subject. For example in ecommerce applications, you can build a recommendation tool using Cassandra by tracking all the user events on the website (product clicks, checkout , likes , etc ) then use these events to create a more personalized experience and increase sales. The high scalability, availability and performance of Cassandra allows for many other use cases such as storing IOT or sensor related events, messaging, logging, and fraud detection. To check some of the companies that are currently using Cassandra and to read what exactly they are doing with Cassandra, please have a look at \nthis page\n.", 
            "title": "When to use"
        }, 
        {
            "location": "/Cassandra/Getting Started/when_to_use/#back", 
            "text": "Many big companies are already using Cassandra such as Apple, eBay and Netflix. One of the most common use cases for using Cassandra is to deal with time-series data. This mean storing large amount of data which are sequence of successive events that are related to some subject. For example in ecommerce applications, you can build a recommendation tool using Cassandra by tracking all the user events on the website (product clicks, checkout , likes , etc ) then use these events to create a more personalized experience and increase sales. The high scalability, availability and performance of Cassandra allows for many other use cases such as storing IOT or sensor related events, messaging, logging, and fraud detection. To check some of the companies that are currently using Cassandra and to read what exactly they are doing with Cassandra, please have a look at  this page .", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/", 
            "text": "In this section, I will talk about the strengths and weaknesses of Cassandra and when using Cassandra makes sense.\n\n\nStrength\n\n\nCassandra has many strengths that makes it the perfect choice for many uses cases , below I talk briefly about them.\n\n\nPerformance\n\n\nLike most NoSQL databases, Cassandra comes with all the high performance benefits that other NoSQL debases can give.  Cassandra provides great performance under large data sets and based on the End Point Benchmark for top NoSQL databases, Cassandra outperformed the other NoSQL databases in both throughput and latency. Cassandra is also designed for heavy writes where any insert or update will be written immediately without locking or reading existing data to check for constraints violation which makes writes very fast. Updates are also very fats and called upserts since a new data will be written with a different timestamp. Later a repair process will be run periodically to check for constraints violations and to merge the data and create the final data set.\n\n\nScalability\n\n\nCassandra supports linear and elastic scalability due to its distributed architecture. Linear scalability means that the capacity of the read/write throughputs is increased by simple adding or removing nodes to the cluster. Elastic scalability means that you can easily scale  up or down by just adding or removing nodes. Adding and deleting the nodes is smooth and will happen without any disturbance.  When a new node is added, it will get automatically an even portion of the data from other nodes. In the same way, if a node is removed or failed, the data and the load that was assigned to the node will be distributed evenly to the other nodes in the cluster. \n\n\nArchitecture\n\n\nCassandra is built as a peer-to-peer distributed database where all the node are equally important with no master or slave and has no single point of failure. Besides, having equally important nodes makes the architecture more robust where any node can accept read/write requests from the clients and hence Cassandra can provide better support for features such as scalability and availability.\n\n\nFault Tolerance \n Availability\n\n\nSince Cassandra is using the distributed architecture where all nodes are equal, Cassandra has no single point of failure and multiple nodes can fail without impacting the overall availability of the database. If a node failed, any other node can still be able to receive requests from the client and give back the results.  Cassandra has a multi datacenter support where nodes can span multiple data centres in different geographical locations which also improves the availability and fault tolerance of the database. Finally, Cassandra supports replication which stores duplicate copies of each written data. The replication is even done across data centres which means that even if a complete data centre is down for any reason, the data can be still safely replicated in other data centres.\n\n\nConsistency\n\n\nCassandra can be configured to have either eventual consistency or a strong consistency since it supports what is called a tunable consistency.  The consistency level is determined by the number of replicas that should acknowledge the write before it is considered as successful.  Cassandra is optimised to be AP system (highly available and partition tolerant) based on the CAP theorem that states that it is possible for a system to have only 2 out of the 3 features (Consistency, availability and partition tolerance). Hence Cassandra is usually configured to be eventual consistence. If you configure Cassandra to be strong consistent, you might impact the availability of the system based on the CAP theorem.\n\n\nWeaknesses\n\n\nBelow are the main weakness of Cassandra:\n\n\nQuery\n\n\nCassandra has a multiple weakness when it comes to querying the data. Below I will explain the main query weaknesses:\n\n\n\n\n\n\nNo joins or subquery support. If you have relational data and you want to run joins across them, then you would need to duplicate your data in Cassandra. Denormalising your data is encouraged by Cassandra to achieve better performance and it is the only way to be able to support relational queries such as complex joins. Denormalising your data is not so bad but it will require you to maintain the duplicated data manually with each update or insert which could be messy sometimes. Generally, you create a separate table for each query which means you would have to maintain these different tables each time a new data is inserted or updated.\n\n\n\n\n\n\nNo ad-hoc query support. As explained previously, you need to design your data model based on the query patterns. This means for each query, you need to think beforehand how to model it in a separate table. Then this table can serve efficiently only the queries that it was built for. This means you can't run ad-hoc queries.\n\n\n\n\n\n\nRange queries on partition key are not supported. Range queries on Cassandra can be done only on columns that are part of the clustering column (with conditions that will be explained later) or columns that have secondary indexes created on them. However if you have chosen a certain column to be part of the partition key, then you can't run range queries on it even if you create secondary index on it. You will get the below error if you try to run range queries on a partition key:\n\n\n\n\n\n\nOnly EQ and IN relation are supported on the partition key\n\n\n\n\n\n\nIf you want to run range query on a clustering column, then the preceding clustering column needs to be specified in the query. An example is shown below:\n\n\n\n\n// assuming we have the below table \n CREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.Lineitem\n(\norderkey text,\nlinenumber text,\no_orderdate timestamp,\no_shippriority text,\nc_mktsegment text,\nl_extendedprice double,\nl_discount double,\nl_shipdate timestamp,\nPRIMARY KEY ((orderkey,linenumber),o_orderdate,l_shipdate)\n);\n\n// as you can see the clustering columns are o_orderdate,l_shipdate\n// if we run the below query\nselect * from CASSANDRA_EXAMPLE_KEYSPACE.Lineitem where orderkey = '2' and linenumber = '1'  and  l_shipdate \n '1990-01-01';\n\n// this will fail with the below error\n\n// \nClustering column \nl_shipdate\n cannot be restricted (preceding column \no_orderdate\n is restricted by a non-EQ relation)\n\n\n// this mean, the o_orderdate need to be specified, the below query will succeed\n\nselect * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 where orderkey = '2' and linenumber = '1'  and o_orderdate = '1996-01-01' and l_shipdate \n '1990-01-01'; \n\n\n\n\n\n\nWhen you run a query, it is either you run it across all the partitions or you run it on a particular partition. If you run it on a particular partition then you need to specify all the columns values that are part of the partition key as shown below:\n\n\n\n\n// assuming the table in the previous example \n\nselect * from CASSANDRA_EXAMPLE_KEYSPACE.Lineitem where orderkey = '2' and linenumber = '1' \n\n// if you specify only the orderkey value in the above query, then you will get the error below \n// \nPartition key parts: orderkey, linenumber must be restricted as other parts are\n\n\n\n\n\n\n\n\nRunning queries that compare the values of two columns is not supported. An example is below:\n\n\n\n\n// assuming the table in the previous example \n\nselect * from CASSANDRA_EXAMPLE_KEYSPACE.Lineitem where orderkey = '2' and linenumber = '1' \nand o_orderdate \n l_shipdate\n\n// this will through the error below:\n// \n[Syntax error in CQL query] message=\nline 1:253 no viable alternative at input \n\n\n\n\n\n\n\n\n\n\nno Regexp support. No queries like \u201cselect * from table where column like \u201c%key%\u201d\u201d.\n\n\n\n\n\n\nSearching capabilities such as full-text search isn't supported internally but external plugins such as solr can be used.  \n\n\n\n\n\n\nAggregation\n\n\nCassandra has some weakness when it comes to data aggregation as will explained below:\n\n\n\n\n\n\nAggregation is on the partition level. If you want to run aggregate queries on Cassandra then you need to specify the partition key since the aggregation will happen on the partition level. Usually, aggregation are more useful if they can be run against the whole database. To do that, you might first need to get the all the partition keys, then run the aggregate query against each partition separately and then aggregate the results.\n\n\n\n\n\n\nMaterialised views doesn't support user defined function. Cassandra supports some built-in aggregate functions such as sum, avg, min, max and count. Besides it supports the use of a user defined functions that can be used to do a manipulation for the data before aggregation such as doing some mathematical operations. However, the user defined functions aren't supported if you want to create a materialised view. \n\n\n\n\n\n\nSorting\n\n\n\n\n\n\nCann't sort by a partition key column. In Cassandra it is not possible to group and order by the same column. Usually we group by a column if we want to do aggregation query. In Cassandra, aggregation is done in the partition level so we group by the partition key. However if you want to run an aggregation query across the database (across all partitions) and later sort the results by the \"grouped by\" column (the partition key in case of Cassandra), then this is not supported since the sorting is done only by the clustering columns in Cassandra.\n\n\n\n\n\n\nSorting order is specified at the table creation time. Sorting in Cassandra is done using the clustering columns which are specified at the table creation time. This mean you can't sort by a columns that was returned from a query as we do using SQL. \n\n\n\n\n\n\nThe ORDER BY clause can be used only when the partition key values are specified because the sorting is on the partition level (no sorting across database).\n\n\n\n\n\n\nStorage\n\n\nIn terms of storage, Cassandra has the below limitations:\n\n\n\n\n\n\nDistribution of the data is done only using the partition key which means if you choose wrong partition keys, it might result in hot spot issues where one node receives most of the load since each partition can be stored only in one node machine. Additionally, the maximum number of cells in each partition has an upper bound of 2 billion.\n\n\n\n\n\n\nEach column value can't be larger than 2GB.\n\n\n\n\nCollection values may not be larger than 64KB.\n\n\n\n\nData Modelling\n\n\nData modelling in Cassandra has the below main weaknesses:\n\n\n\n\n\n\nNot so easy to know all the possible query patterns of an application in advance.  Since the data model in Cassandra is based on the query patterns, you need to think of what are the possible queries that your database need to answer based on the application expected tasks. However, it might be difficult to think of and write down all the possible queries that the application might need at the early stages since it requires extensive knowledge about the application.\n\n\n\n\n\n\nIf the query patterns changes in the future, then you need to change your data model and that might involve data migration or a lot of work.\n\n\n\n\n\n\nChoosing the partition key that will be used for evenly distributing the data across the cluster is a critical task and could be difficult sometimes. Although thinking of which columns to consider as clustering columns and which columns should be indexed might be also difficult in some scenarios depending on the queries.\n\n\n\n\n\n\nSummary\n\n\nCassandra is suitable for enterprises that are having very large dataset and they are looking for a database to address problems related to performance, scalability and availability.  Cassandra is great when you have large workload that involves many writes but few reads such as storing and scaling millions of daily logs or sensor or IoT events . Cassandra has some limitations related to reading the data such as querying, searching, sorting or performing large scale aggregations or ad-hock queries. Therefore, Cassandra is not recommended when you need to run analytics or complex queries against your data or if your application involves many reads. Cassandra also expect an extensive knowledge of the current dataset since the data modelling is based on the query patterns. This means that if you are looking for an application transparency, then Cassandra isn't recommended for you. Additionally, Cassandra isn't recommended if you know that your data size is not large even in the future when your data grows. It is always possible to migrate to Cassandra when your data becomes so large and when you really need the features of Cassandra.  Finally, if you need a strong consistency most of the time, then Cassandra isn't of you since configuring Cassandra to be strong consistence will impact performance and reduce availability which what Cassandra is really good for.", 
            "title": "Benifits limitations"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#strength", 
            "text": "Cassandra has many strengths that makes it the perfect choice for many uses cases , below I talk briefly about them.", 
            "title": "Strength"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#performance", 
            "text": "Like most NoSQL databases, Cassandra comes with all the high performance benefits that other NoSQL debases can give.  Cassandra provides great performance under large data sets and based on the End Point Benchmark for top NoSQL databases, Cassandra outperformed the other NoSQL databases in both throughput and latency. Cassandra is also designed for heavy writes where any insert or update will be written immediately without locking or reading existing data to check for constraints violation which makes writes very fast. Updates are also very fats and called upserts since a new data will be written with a different timestamp. Later a repair process will be run periodically to check for constraints violations and to merge the data and create the final data set.", 
            "title": "Performance"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#scalability", 
            "text": "Cassandra supports linear and elastic scalability due to its distributed architecture. Linear scalability means that the capacity of the read/write throughputs is increased by simple adding or removing nodes to the cluster. Elastic scalability means that you can easily scale  up or down by just adding or removing nodes. Adding and deleting the nodes is smooth and will happen without any disturbance.  When a new node is added, it will get automatically an even portion of the data from other nodes. In the same way, if a node is removed or failed, the data and the load that was assigned to the node will be distributed evenly to the other nodes in the cluster.", 
            "title": "Scalability"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#architecture", 
            "text": "Cassandra is built as a peer-to-peer distributed database where all the node are equally important with no master or slave and has no single point of failure. Besides, having equally important nodes makes the architecture more robust where any node can accept read/write requests from the clients and hence Cassandra can provide better support for features such as scalability and availability.", 
            "title": "Architecture"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#fault-tolerance-availability", 
            "text": "Since Cassandra is using the distributed architecture where all nodes are equal, Cassandra has no single point of failure and multiple nodes can fail without impacting the overall availability of the database. If a node failed, any other node can still be able to receive requests from the client and give back the results.  Cassandra has a multi datacenter support where nodes can span multiple data centres in different geographical locations which also improves the availability and fault tolerance of the database. Finally, Cassandra supports replication which stores duplicate copies of each written data. The replication is even done across data centres which means that even if a complete data centre is down for any reason, the data can be still safely replicated in other data centres.", 
            "title": "Fault Tolerance &amp; Availability"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#consistency", 
            "text": "Cassandra can be configured to have either eventual consistency or a strong consistency since it supports what is called a tunable consistency.  The consistency level is determined by the number of replicas that should acknowledge the write before it is considered as successful.  Cassandra is optimised to be AP system (highly available and partition tolerant) based on the CAP theorem that states that it is possible for a system to have only 2 out of the 3 features (Consistency, availability and partition tolerance). Hence Cassandra is usually configured to be eventual consistence. If you configure Cassandra to be strong consistent, you might impact the availability of the system based on the CAP theorem.", 
            "title": "Consistency"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#weaknesses", 
            "text": "Below are the main weakness of Cassandra:", 
            "title": "Weaknesses"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#query", 
            "text": "Cassandra has a multiple weakness when it comes to querying the data. Below I will explain the main query weaknesses:    No joins or subquery support. If you have relational data and you want to run joins across them, then you would need to duplicate your data in Cassandra. Denormalising your data is encouraged by Cassandra to achieve better performance and it is the only way to be able to support relational queries such as complex joins. Denormalising your data is not so bad but it will require you to maintain the duplicated data manually with each update or insert which could be messy sometimes. Generally, you create a separate table for each query which means you would have to maintain these different tables each time a new data is inserted or updated.    No ad-hoc query support. As explained previously, you need to design your data model based on the query patterns. This means for each query, you need to think beforehand how to model it in a separate table. Then this table can serve efficiently only the queries that it was built for. This means you can't run ad-hoc queries.    Range queries on partition key are not supported. Range queries on Cassandra can be done only on columns that are part of the clustering column (with conditions that will be explained later) or columns that have secondary indexes created on them. However if you have chosen a certain column to be part of the partition key, then you can't run range queries on it even if you create secondary index on it. You will get the below error if you try to run range queries on a partition key:    Only EQ and IN relation are supported on the partition key   If you want to run range query on a clustering column, then the preceding clustering column needs to be specified in the query. An example is shown below:   // assuming we have the below table \n CREATE TABLE IF NOT EXISTS CASSANDRA_EXAMPLE_KEYSPACE.Lineitem\n(\norderkey text,\nlinenumber text,\no_orderdate timestamp,\no_shippriority text,\nc_mktsegment text,\nl_extendedprice double,\nl_discount double,\nl_shipdate timestamp,\nPRIMARY KEY ((orderkey,linenumber),o_orderdate,l_shipdate)\n);\n\n// as you can see the clustering columns are o_orderdate,l_shipdate\n// if we run the below query\nselect * from CASSANDRA_EXAMPLE_KEYSPACE.Lineitem where orderkey = '2' and linenumber = '1'  and  l_shipdate   '1990-01-01';\n\n// this will fail with the below error\n\n//  Clustering column  l_shipdate  cannot be restricted (preceding column  o_orderdate  is restricted by a non-EQ relation) \n\n// this mean, the o_orderdate need to be specified, the below query will succeed\n\nselect * from CASSANDRA_EXAMPLE_KEYSPACE.TPCH_Q3 where orderkey = '2' and linenumber = '1'  and o_orderdate = '1996-01-01' and l_shipdate   '1990-01-01';    When you run a query, it is either you run it across all the partitions or you run it on a particular partition. If you run it on a particular partition then you need to specify all the columns values that are part of the partition key as shown below:   // assuming the table in the previous example \n\nselect * from CASSANDRA_EXAMPLE_KEYSPACE.Lineitem where orderkey = '2' and linenumber = '1' \n\n// if you specify only the orderkey value in the above query, then you will get the error below \n//  Partition key parts: orderkey, linenumber must be restricted as other parts are    Running queries that compare the values of two columns is not supported. An example is below:   // assuming the table in the previous example \n\nselect * from CASSANDRA_EXAMPLE_KEYSPACE.Lineitem where orderkey = '2' and linenumber = '1' \nand o_orderdate   l_shipdate\n\n// this will through the error below:\n//  [Syntax error in CQL query] message= line 1:253 no viable alternative at input      no Regexp support. No queries like \u201cselect * from table where column like \u201c%key%\u201d\u201d.    Searching capabilities such as full-text search isn't supported internally but external plugins such as solr can be used.", 
            "title": "Query"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#aggregation", 
            "text": "Cassandra has some weakness when it comes to data aggregation as will explained below:    Aggregation is on the partition level. If you want to run aggregate queries on Cassandra then you need to specify the partition key since the aggregation will happen on the partition level. Usually, aggregation are more useful if they can be run against the whole database. To do that, you might first need to get the all the partition keys, then run the aggregate query against each partition separately and then aggregate the results.    Materialised views doesn't support user defined function. Cassandra supports some built-in aggregate functions such as sum, avg, min, max and count. Besides it supports the use of a user defined functions that can be used to do a manipulation for the data before aggregation such as doing some mathematical operations. However, the user defined functions aren't supported if you want to create a materialised view.", 
            "title": "Aggregation"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#sorting", 
            "text": "Cann't sort by a partition key column. In Cassandra it is not possible to group and order by the same column. Usually we group by a column if we want to do aggregation query. In Cassandra, aggregation is done in the partition level so we group by the partition key. However if you want to run an aggregation query across the database (across all partitions) and later sort the results by the \"grouped by\" column (the partition key in case of Cassandra), then this is not supported since the sorting is done only by the clustering columns in Cassandra.    Sorting order is specified at the table creation time. Sorting in Cassandra is done using the clustering columns which are specified at the table creation time. This mean you can't sort by a columns that was returned from a query as we do using SQL.     The ORDER BY clause can be used only when the partition key values are specified because the sorting is on the partition level (no sorting across database).", 
            "title": "Sorting"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#storage", 
            "text": "In terms of storage, Cassandra has the below limitations:    Distribution of the data is done only using the partition key which means if you choose wrong partition keys, it might result in hot spot issues where one node receives most of the load since each partition can be stored only in one node machine. Additionally, the maximum number of cells in each partition has an upper bound of 2 billion.    Each column value can't be larger than 2GB.   Collection values may not be larger than 64KB.", 
            "title": "Storage"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#data-modelling", 
            "text": "Data modelling in Cassandra has the below main weaknesses:    Not so easy to know all the possible query patterns of an application in advance.  Since the data model in Cassandra is based on the query patterns, you need to think of what are the possible queries that your database need to answer based on the application expected tasks. However, it might be difficult to think of and write down all the possible queries that the application might need at the early stages since it requires extensive knowledge about the application.    If the query patterns changes in the future, then you need to change your data model and that might involve data migration or a lot of work.    Choosing the partition key that will be used for evenly distributing the data across the cluster is a critical task and could be difficult sometimes. Although thinking of which columns to consider as clustering columns and which columns should be indexed might be also difficult in some scenarios depending on the queries.", 
            "title": "Data Modelling"
        }, 
        {
            "location": "/Cassandra/Results/benifits_limitations/#summary", 
            "text": "Cassandra is suitable for enterprises that are having very large dataset and they are looking for a database to address problems related to performance, scalability and availability.  Cassandra is great when you have large workload that involves many writes but few reads such as storing and scaling millions of daily logs or sensor or IoT events . Cassandra has some limitations related to reading the data such as querying, searching, sorting or performing large scale aggregations or ad-hock queries. Therefore, Cassandra is not recommended when you need to run analytics or complex queries against your data or if your application involves many reads. Cassandra also expect an extensive knowledge of the current dataset since the data modelling is based on the query patterns. This means that if you are looking for an application transparency, then Cassandra isn't recommended for you. Additionally, Cassandra isn't recommended if you know that your data size is not large even in the future when your data grows. It is always possible to migrate to Cassandra when your data becomes so large and when you really need the features of Cassandra.  Finally, if you need a strong consistency most of the time, then Cassandra isn't of you since configuring Cassandra to be strong consistence will impact performance and reduce availability which what Cassandra is really good for.", 
            "title": "Summary"
        }, 
        {
            "location": "/Cassandra/Results/reference/", 
            "text": "http://www.datastax.com/wp-content/themes/datastax-2014-08/files/NoSQL_Benchmarks_EndPoint.pdf\n\n\nhttps://wiki.apache.org/cassandra/CassandraLimitations", 
            "title": "Reference"
        }, 
        {
            "location": "/Cassandra/Results/reference/#httpwwwdatastaxcomwp-contentthemesdatastax-2014-08filesnosql_benchmarks_endpointpdf", 
            "text": "", 
            "title": "http://www.datastax.com/wp-content/themes/datastax-2014-08/files/NoSQL_Benchmarks_EndPoint.pdf"
        }, 
        {
            "location": "/Cassandra/Results/reference/#httpswikiapacheorgcassandracassandralimitations", 
            "text": "", 
            "title": "https://wiki.apache.org/cassandra/CassandraLimitations"
        }, 
        {
            "location": "/Cassandra/Results/results_main/", 
            "text": "back\n\n\nStrength and Weakness\n\n\nSummary\n\n\nReferences", 
            "title": "Results main"
        }, 
        {
            "location": "/Cassandra/Results/results_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Results/results_main/#strength-and-weakness", 
            "text": "", 
            "title": "Strength and Weakness"
        }, 
        {
            "location": "/Cassandra/Results/results_main/#summary", 
            "text": "", 
            "title": "Summary"
        }, 
        {
            "location": "/Cassandra/Results/results_main/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/Cassandra/Results/summary/", 
            "text": "Cassandra is a distributed database that can handle large amount of data and allows for high performance, scalability and availability because of its decentralised architecture. In the following sections, I will give a brief summary for most of the features that we look for in a database and how Cassandra supports them.\n\n\nUse cases\n\n\nCassandra is currently used by many large companies such as Ebay, Apple and Netflix because of its scalability, high performance and availability. The main use case for Cassandra is to help scaling large time series data (data stored so frequently such as events). Storing and efficiently scaling this type of data is so useful in many use cases such as to build a recommendation system, storing IoT or sensor events, logging and messaging, fraud detection and many other use cases.\n\n\nBasic Concepts\n\n\nMain Concepts in Cassandra are:\n\n\nNode: The basic infrastructure component where we store the data.\n\n\nData Center: A group of related nodes.\n\n\nCluster: The outermost container and can contain multiple data centres.\n\n\nGossip protocol: A protocol used by a node to discover information and the state of all other nodes in the cluster.\n\n\nA Partitioner: Is a hash function used to compute the tokens (hashing the partition key) and this token is used to distribute the data as well as read/write request across all the nodes in the cluster based on the hash ranges (Consistent hashing). \n\n\nThe replication factor: Used to determine how many copies of the written\ndata should be kept inside a cluster.\n\n\nThe Replication placement strategy: It is a strategy used by the node to determine where to replicate the data (in which nodes) when a write request received.\n\n\nA Snitch: The snitch used to monitor all the nodes and provides performance and state information to help the replication strategy to decide where to replicate the data.\n\n\nA Keyspace: A container that is used to group the different CQL tables of the application.\n\n\nA CQL table: Collection of multiple ordered columns having a primary row key.\n\n\nPrimary key: Each CQL table should have a primary key which is used to ensure that the data is unique and willn't be overwritten inside each partition. The primary key contains the partition key and any clustering columns. The primary key can be a single key or a composite key (partition key is more than one column) or a compound key (partition key and some clustering columns).\n\n\nPartition Key: This is the key used to divide your data into different partitions before distributing them across the nodes for scalability. The partition key can be either a single key or a composite key.\n\n\nClustering columns: Columns used to group and sort the data inside each partition.\n\n\nData Types: Many common data types support such as most numeric data types (int, float, double and decimal), text data types (text, varchar and even blob), collection data types (map, list and set), UUID and counter data types, boolean, timestamp and the support for custom defined data types which can contain any separate objects.\n\n\nCompaction: A process that starts frequently to merge all the different versions of the data and create a final version having the latest written data. It is needed since the written data in Cassandra is immutable and can't be overwritten during data update or deletion.\n\n\nInstalling\n\n\nCan be installed in most of the operating systems by building from the source using the  distribution binaries. In the linux-based systems, you can also install it using package managers such as apt-get and Yum. \n\n\nQuery language\n\n\nCassandra supports a query language called CQL (Cassandra\nQuery Language) which has a SQL-similar syntax by supporting clauses such as CREATE TABLE, SELECT, WHERE, and ORDER BY.\n\n\nTransaction support\n\n\nCassandra supports the ACID properties and trasnactions as explained below:\n\n\nConsistency: Cassandra supports a configurable consistency levels or what is called a tunable consistency beside supporting a linearisable consistency that can be used in transactions.\n\n\nAtomicity:  At the row level or the partition-level, the insert, update, and delete operations are all executed atomically depending on the consistency level configured.\n\n\nIsolation: Cassandra provides full isolation at the low-level which means that any writes from a client application will not be visible to other until it is completed.\n\n\nDurability: Cassandra is fully durable and all writes will be persisted on disk to a commit log first (for disaster recovery) then later to the on-disk SSTable files.\n\n\nTransactions: Supports a transaction mechanism called lightweight transactions which are implemented using the Paxos protocol. Paxos is used to make all nodes agree on proposals based on the majority voting and ensures that there exists only one agreed values between\nall the nodes.\n\n\nData Import and Export\n\n\nCQL supports a statement called \"COPY\" to either export data from a table into a CSV file or import data from a CSV file and into a table. Additionally you can execute multiple CQL statement from a file as a batch using the SOURCE command.\n\n\nData Layout\n\n\nData modelling in Cassandra is relatively different if you are coming from a relational database background. The data in Cassandra is modelled around the query patterns instead of the data entities and relationships. Besides, it is absolutely ok to duplicate your data in different tables to achieve high performance during the retrieval of your frequent queries. \n\n\nRelational data\n\n\nSince Cassandra doesn't support joins, relationships are modelled duplicating the data from the different related entities and group them into a separate CQL table that can be used to fulfil queries with the relationships. It is important to choose a correct primary key that contains proper partition key and any clustering columns if needed to be able to retrieve the related data efficiently later.\n\n\nNormalisation/Denormalisation\n\n\nIt is usually encouraged to denormalise the data to achieve better performance and scalability.\n\n\nReferential Integrity\n\n\nThere is no such concept as referential integrity, joins, or foreign keys in Cassandra.\n\n\nNested Data\n\n\nCassandra doesn't support nesting collections inside each other. However we can nest user defined data types inside collections.\n\n\nQuery\n\n\nCQL supports a SELECT statement similar to SQL that can be used to run parametrised or range queries. The SELECT statement is usually used with the WHERE clause for filtering and can return either all fields or just some selected fields. Additionally, Cassandra supports querying some system tables to get information about available tables and columns, keyspaces, user-defined functions and data types or cluster related info. \n\n\nAggregation\n\n\nCassandra has a built-in support for common aggregate functions such as min, max, avg, sum, and count. Aggregation is done in the partition-level in Cassandra, therefore choosing the appropriate partition key is necessary for a successful aggregation. Additionally, Cassandra  supports user defined functions that can be used to create a custom aggregate function based on the requirement.\n\n\nFull Text Search\n\n\nCassandra doesn't support internally full-text search. However it supports creating a custom index besides there are some external plugins available for full-text search such as the Stratio\u2019s Cassandra Lucene Index.\n\n\nRegular Expressions\n\n\nCassandra has no built-in support for regular expressions and usually external search engines plugins are used for regular expression or full-text search support.\n\n\nIndexing\n\n\nCassandra supports creating secondary indexes on any columns other than the primary key columns. The indexes are built in the background without blocking writes or reads and are maintained automatically by Cassandra. \n\n\nFiltering and Grouping\n\n\nCassandra supports the CQL WHERE clause to filter data. Unlike SQL, only columns that are part of the primary key or the secondary indexed columns can be filtered using the WHERE clause. You can use operators such as =, \n, \n=, \n, and \n= to filter the data. Additionally you can use the IN or CONTAINS clauses to filter the data based on a certain values in a collection.\n\n\nGrouping the data in Cassandra is done using the partition key. The partition key can be simple (single column) or composite (multiple columns). Clustering columns are also used to group and sort the data inside each partition. Cassandra also supports collection data types such as list, map and set that can be used to group related data together. Additionally, Cassandra support the user defined data type which can represent a complete separate object and can be used to group related data as an object data type (e.g the customer address).\n\n\nSorting\n\n\nCassandra supports sorting using the clustering columns that should be defined during the table creation. The clustering columns can be later used to sort the data inside each partition in either ascending or descending orders using the CQL ORDER BY clause with the ASC or DESC options.\n\n\nConfiguration\n\n\nCassandra uses a yaml configuration file called cassandra.yaml to store all configuration related parameters. The configuration file can be found under the install_location/conf folder. Cassandra doesn't support changing the configurations on the fly and you will need to restart the node for the new configurations to take effect. \n\n\nScalability\n\n\nThe client can read or write to any node in the cluster since Cassandra treats all nodes equally. This makes Cassandra highly scalable since scaling the system is as simple as adding or removing nodes or data centres. Additionally, the data is partitioned across all the nodes based on the partition key which allows even distribution of the data as well as the read/write throughputs. The client can read or write from any node and the request will be forwarded smoothly to the node having that portion of data.  Adding and removing the nodes to the cluster is easily done using virtual nodes paradigm.\n\n\nPersistency\n\n\nCassandra is fully durable data store since the data will be written directly to a commit log once received. The commit log is then persisted to disk and later it can be used as a crash recovery mechanism whenever the node accidentally failed. The write operation will be considered a successful only once it has been written to the commit log. After the write operation is written to the commit log, it will be written to the MemTable where it will be kept until a pre-configured size threshold is reached. Once this threshold is reached, data will be flushed to disk or what is called the SSTable files and be persisted permanently.  \n\n\nBackup\n\n\nCassandra supports taking backups through snapshots and by enabling the incremental backup feature. The snapshot is taken for the SSTable files in a node either for a single Keyspace or for all Keyspaces or even for the whole cluster. Generally, a system-wide snapshot is taken then the incremental backup feature is enabled in each node to backup only the delta data that has been changed since the last snapshot. \n\n\nSecurity\n\n\nCassandra provides security by providing features such as the ability to use SSL encryption for all the communications between the client and the nodes. Also Cassandra supports authentication based control by creating users and roles using SQL-similar statements such as CREATE USER and CREATE ROLE. Additionally, Cassandra allows granting or revoking permissions from users on specific objects using CQL GRANT or REVOKE statements.\n\n\nUpgrading\n\n\nUpgrading Cassandra is relatively easy which needs to be done for each node in the cluster  and it will require stoping, reconfiguring and restarting the node. \n\n\nAvailability\n\n\nCassandra is a highly available distributed system since it replicate the data across multiple nodes or even data centres that spans multiple locations. Additionally, Cassandra is a decentralised system that doesn't have a single point of failure since it is based on a distributed design where all nodes are equally important and any node can receive read/write requests from the client application. When creating a Keyspace, the replication factor and replication strategy needs to be configured. The replication factor will determine how many copies we need to keep for each written data. The replication strategy will decide where (in which node) to replicate the data based on information provided by a snitch that continuously monitor the nodes performance and status. The replication strategy needs to be selected carefully since it will directly impact the availability.", 
            "title": "Summary"
        }, 
        {
            "location": "/Cassandra/Results/summary/#use-cases", 
            "text": "Cassandra is currently used by many large companies such as Ebay, Apple and Netflix because of its scalability, high performance and availability. The main use case for Cassandra is to help scaling large time series data (data stored so frequently such as events). Storing and efficiently scaling this type of data is so useful in many use cases such as to build a recommendation system, storing IoT or sensor events, logging and messaging, fraud detection and many other use cases.", 
            "title": "Use cases"
        }, 
        {
            "location": "/Cassandra/Results/summary/#basic-concepts", 
            "text": "Main Concepts in Cassandra are:  Node: The basic infrastructure component where we store the data.  Data Center: A group of related nodes.  Cluster: The outermost container and can contain multiple data centres.  Gossip protocol: A protocol used by a node to discover information and the state of all other nodes in the cluster.  A Partitioner: Is a hash function used to compute the tokens (hashing the partition key) and this token is used to distribute the data as well as read/write request across all the nodes in the cluster based on the hash ranges (Consistent hashing).   The replication factor: Used to determine how many copies of the written\ndata should be kept inside a cluster.  The Replication placement strategy: It is a strategy used by the node to determine where to replicate the data (in which nodes) when a write request received.  A Snitch: The snitch used to monitor all the nodes and provides performance and state information to help the replication strategy to decide where to replicate the data.  A Keyspace: A container that is used to group the different CQL tables of the application.  A CQL table: Collection of multiple ordered columns having a primary row key.  Primary key: Each CQL table should have a primary key which is used to ensure that the data is unique and willn't be overwritten inside each partition. The primary key contains the partition key and any clustering columns. The primary key can be a single key or a composite key (partition key is more than one column) or a compound key (partition key and some clustering columns).  Partition Key: This is the key used to divide your data into different partitions before distributing them across the nodes for scalability. The partition key can be either a single key or a composite key.  Clustering columns: Columns used to group and sort the data inside each partition.  Data Types: Many common data types support such as most numeric data types (int, float, double and decimal), text data types (text, varchar and even blob), collection data types (map, list and set), UUID and counter data types, boolean, timestamp and the support for custom defined data types which can contain any separate objects.  Compaction: A process that starts frequently to merge all the different versions of the data and create a final version having the latest written data. It is needed since the written data in Cassandra is immutable and can't be overwritten during data update or deletion.", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/Cassandra/Results/summary/#installing", 
            "text": "Can be installed in most of the operating systems by building from the source using the  distribution binaries. In the linux-based systems, you can also install it using package managers such as apt-get and Yum.", 
            "title": "Installing"
        }, 
        {
            "location": "/Cassandra/Results/summary/#query-language", 
            "text": "Cassandra supports a query language called CQL (Cassandra\nQuery Language) which has a SQL-similar syntax by supporting clauses such as CREATE TABLE, SELECT, WHERE, and ORDER BY.", 
            "title": "Query language"
        }, 
        {
            "location": "/Cassandra/Results/summary/#transaction-support", 
            "text": "Cassandra supports the ACID properties and trasnactions as explained below:  Consistency: Cassandra supports a configurable consistency levels or what is called a tunable consistency beside supporting a linearisable consistency that can be used in transactions.  Atomicity:  At the row level or the partition-level, the insert, update, and delete operations are all executed atomically depending on the consistency level configured.  Isolation: Cassandra provides full isolation at the low-level which means that any writes from a client application will not be visible to other until it is completed.  Durability: Cassandra is fully durable and all writes will be persisted on disk to a commit log first (for disaster recovery) then later to the on-disk SSTable files.  Transactions: Supports a transaction mechanism called lightweight transactions which are implemented using the Paxos protocol. Paxos is used to make all nodes agree on proposals based on the majority voting and ensures that there exists only one agreed values between\nall the nodes.", 
            "title": "Transaction support"
        }, 
        {
            "location": "/Cassandra/Results/summary/#data-import-and-export", 
            "text": "CQL supports a statement called \"COPY\" to either export data from a table into a CSV file or import data from a CSV file and into a table. Additionally you can execute multiple CQL statement from a file as a batch using the SOURCE command.", 
            "title": "Data Import and Export"
        }, 
        {
            "location": "/Cassandra/Results/summary/#data-layout", 
            "text": "Data modelling in Cassandra is relatively different if you are coming from a relational database background. The data in Cassandra is modelled around the query patterns instead of the data entities and relationships. Besides, it is absolutely ok to duplicate your data in different tables to achieve high performance during the retrieval of your frequent queries.", 
            "title": "Data Layout"
        }, 
        {
            "location": "/Cassandra/Results/summary/#relational-data", 
            "text": "Since Cassandra doesn't support joins, relationships are modelled duplicating the data from the different related entities and group them into a separate CQL table that can be used to fulfil queries with the relationships. It is important to choose a correct primary key that contains proper partition key and any clustering columns if needed to be able to retrieve the related data efficiently later.", 
            "title": "Relational data"
        }, 
        {
            "location": "/Cassandra/Results/summary/#normalisationdenormalisation", 
            "text": "It is usually encouraged to denormalise the data to achieve better performance and scalability.", 
            "title": "Normalisation/Denormalisation"
        }, 
        {
            "location": "/Cassandra/Results/summary/#referential-integrity", 
            "text": "There is no such concept as referential integrity, joins, or foreign keys in Cassandra.", 
            "title": "Referential Integrity"
        }, 
        {
            "location": "/Cassandra/Results/summary/#nested-data", 
            "text": "Cassandra doesn't support nesting collections inside each other. However we can nest user defined data types inside collections.", 
            "title": "Nested Data"
        }, 
        {
            "location": "/Cassandra/Results/summary/#query", 
            "text": "CQL supports a SELECT statement similar to SQL that can be used to run parametrised or range queries. The SELECT statement is usually used with the WHERE clause for filtering and can return either all fields or just some selected fields. Additionally, Cassandra supports querying some system tables to get information about available tables and columns, keyspaces, user-defined functions and data types or cluster related info.", 
            "title": "Query"
        }, 
        {
            "location": "/Cassandra/Results/summary/#aggregation", 
            "text": "Cassandra has a built-in support for common aggregate functions such as min, max, avg, sum, and count. Aggregation is done in the partition-level in Cassandra, therefore choosing the appropriate partition key is necessary for a successful aggregation. Additionally, Cassandra  supports user defined functions that can be used to create a custom aggregate function based on the requirement.", 
            "title": "Aggregation"
        }, 
        {
            "location": "/Cassandra/Results/summary/#full-text-search", 
            "text": "Cassandra doesn't support internally full-text search. However it supports creating a custom index besides there are some external plugins available for full-text search such as the Stratio\u2019s Cassandra Lucene Index.", 
            "title": "Full Text Search"
        }, 
        {
            "location": "/Cassandra/Results/summary/#regular-expressions", 
            "text": "Cassandra has no built-in support for regular expressions and usually external search engines plugins are used for regular expression or full-text search support.", 
            "title": "Regular Expressions"
        }, 
        {
            "location": "/Cassandra/Results/summary/#indexing", 
            "text": "Cassandra supports creating secondary indexes on any columns other than the primary key columns. The indexes are built in the background without blocking writes or reads and are maintained automatically by Cassandra.", 
            "title": "Indexing"
        }, 
        {
            "location": "/Cassandra/Results/summary/#filtering-and-grouping", 
            "text": "Cassandra supports the CQL WHERE clause to filter data. Unlike SQL, only columns that are part of the primary key or the secondary indexed columns can be filtered using the WHERE clause. You can use operators such as =,  ,  =,  , and  = to filter the data. Additionally you can use the IN or CONTAINS clauses to filter the data based on a certain values in a collection.  Grouping the data in Cassandra is done using the partition key. The partition key can be simple (single column) or composite (multiple columns). Clustering columns are also used to group and sort the data inside each partition. Cassandra also supports collection data types such as list, map and set that can be used to group related data together. Additionally, Cassandra support the user defined data type which can represent a complete separate object and can be used to group related data as an object data type (e.g the customer address).", 
            "title": "Filtering and Grouping"
        }, 
        {
            "location": "/Cassandra/Results/summary/#sorting", 
            "text": "Cassandra supports sorting using the clustering columns that should be defined during the table creation. The clustering columns can be later used to sort the data inside each partition in either ascending or descending orders using the CQL ORDER BY clause with the ASC or DESC options.", 
            "title": "Sorting"
        }, 
        {
            "location": "/Cassandra/Results/summary/#configuration", 
            "text": "Cassandra uses a yaml configuration file called cassandra.yaml to store all configuration related parameters. The configuration file can be found under the install_location/conf folder. Cassandra doesn't support changing the configurations on the fly and you will need to restart the node for the new configurations to take effect.", 
            "title": "Configuration"
        }, 
        {
            "location": "/Cassandra/Results/summary/#scalability", 
            "text": "The client can read or write to any node in the cluster since Cassandra treats all nodes equally. This makes Cassandra highly scalable since scaling the system is as simple as adding or removing nodes or data centres. Additionally, the data is partitioned across all the nodes based on the partition key which allows even distribution of the data as well as the read/write throughputs. The client can read or write from any node and the request will be forwarded smoothly to the node having that portion of data.  Adding and removing the nodes to the cluster is easily done using virtual nodes paradigm.", 
            "title": "Scalability"
        }, 
        {
            "location": "/Cassandra/Results/summary/#persistency", 
            "text": "Cassandra is fully durable data store since the data will be written directly to a commit log once received. The commit log is then persisted to disk and later it can be used as a crash recovery mechanism whenever the node accidentally failed. The write operation will be considered a successful only once it has been written to the commit log. After the write operation is written to the commit log, it will be written to the MemTable where it will be kept until a pre-configured size threshold is reached. Once this threshold is reached, data will be flushed to disk or what is called the SSTable files and be persisted permanently.", 
            "title": "Persistency"
        }, 
        {
            "location": "/Cassandra/Results/summary/#backup", 
            "text": "Cassandra supports taking backups through snapshots and by enabling the incremental backup feature. The snapshot is taken for the SSTable files in a node either for a single Keyspace or for all Keyspaces or even for the whole cluster. Generally, a system-wide snapshot is taken then the incremental backup feature is enabled in each node to backup only the delta data that has been changed since the last snapshot.", 
            "title": "Backup"
        }, 
        {
            "location": "/Cassandra/Results/summary/#security", 
            "text": "Cassandra provides security by providing features such as the ability to use SSL encryption for all the communications between the client and the nodes. Also Cassandra supports authentication based control by creating users and roles using SQL-similar statements such as CREATE USER and CREATE ROLE. Additionally, Cassandra allows granting or revoking permissions from users on specific objects using CQL GRANT or REVOKE statements.", 
            "title": "Security"
        }, 
        {
            "location": "/Cassandra/Results/summary/#upgrading", 
            "text": "Upgrading Cassandra is relatively easy which needs to be done for each node in the cluster  and it will require stoping, reconfiguring and restarting the node.", 
            "title": "Upgrading"
        }, 
        {
            "location": "/Cassandra/Results/summary/#availability", 
            "text": "Cassandra is a highly available distributed system since it replicate the data across multiple nodes or even data centres that spans multiple locations. Additionally, Cassandra is a decentralised system that doesn't have a single point of failure since it is based on a distributed design where all nodes are equally important and any node can receive read/write requests from the client application. When creating a Keyspace, the replication factor and replication strategy needs to be configured. The replication factor will determine how many copies we need to keep for each written data. The replication strategy will decide where (in which node) to replicate the data based on information provided by a snitch that continuously monitor the nodes performance and status. The replication strategy needs to be selected carefully since it will directly impact the availability.", 
            "title": "Availability"
        }, 
        {
            "location": "/Cassandra/Search Data/filtering and grouping/", 
            "text": "back\n\n\nIn this section, I will talk about how to filter and group data in Cassandra.\n\n\nFiltering\n\n\nFiltering data in Cassandra is done by using the WHERE clause where you can query data either by its partition key, clustering columns or secondary indexed columns. The WHERE clause supports these conditional operators: CONTAINS, CONTAINS KEY, IN, =, \n, \n=, \n, and \n=.\n\n\nCONTAIN is used to filter the data based on a certain value in a collection as seen in the below example:\n\n\nAssuming we have already created the below product table:\n\n\nCREATE TABLE product ( \n id UUID,\n name int,\n size text,\n release_year text,\n price decimal, \n tags set\ntext\n,\n PRIMARY KEY (release_year, price) );\n\n\n\n\nThen we can check if the product is having a certain tag as below:\n\n\nSELECT * FROM product WHERE tags CONTAINS 'Electronics';\n\n\n\n\nOr use CONTAINS KEY in case we are searching a map key.  For example, if we have created the below table:\n\n\nCREATE TABLE product ( \nid UUID,\nname int,\nsize text, \nrelease_year text,\nprice decimal,\ntags map\ntext,text\n,\nPRIMARY KEY (release_year, price) );\n\n\n\n\nThen we can query to check if the map has a value on a certain key as seen below:\n\n\nSELECT * FROM product WHERE tags CONTAINS KEY 'isDeleted';\n\n\n\n\nGrouping\n\n\nThe data in Cassandra is grouped by the partition key and the clustering columns. The partition key can be simple (single column) or composite (multiple columns). Clustering columns are used to sort the data within the partition. Choosing the right grouping of your data can impact the read and write performance. For more details on how to choose the partition key and the clustering columns, please have a look to the \ndata layout section\n.\n\n\nAdditionally, the data can be grouped in Cassandra using the collection data type. Cassandra supports the use of a list, map or a set data types that can be used to group multiple related data together. For example, you can store the address of a customer using the map data type.  It is also possible to use user-defined data types that can be complete objects which can be so useful to store related data together. For instance, you can store the supplier details of a product as a user defined data type inside the product table. For more details about the collection and user defined data types, please have a look to the \ndata layout section\n.\n\n\nCassandra also supports the Tuple data type which can be used to store two or more values together in a column. For example if you want to store the product table which contains the longitude and latitude values. Then you can store it in a single column having two float values as shown below:\n\n\nCREATE TABLE product ( \nid UUID, \nname int, \nlocation tuple\nfloat,float\n,\nsize text, \nrelease_year text,\nprice decimal, \ntags map\ntext,text\n, \nPRIMARY KEY (release_year, price) );", 
            "title": "Filtering and grouping"
        }, 
        {
            "location": "/Cassandra/Search Data/filtering and grouping/#back", 
            "text": "In this section, I will talk about how to filter and group data in Cassandra.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Search Data/filtering and grouping/#filtering", 
            "text": "Filtering data in Cassandra is done by using the WHERE clause where you can query data either by its partition key, clustering columns or secondary indexed columns. The WHERE clause supports these conditional operators: CONTAINS, CONTAINS KEY, IN, =,  ,  =,  , and  =.  CONTAIN is used to filter the data based on a certain value in a collection as seen in the below example:  Assuming we have already created the below product table:  CREATE TABLE product ( \n id UUID,\n name int,\n size text,\n release_year text,\n price decimal, \n tags set text ,\n PRIMARY KEY (release_year, price) );  Then we can check if the product is having a certain tag as below:  SELECT * FROM product WHERE tags CONTAINS 'Electronics';  Or use CONTAINS KEY in case we are searching a map key.  For example, if we have created the below table:  CREATE TABLE product ( \nid UUID,\nname int,\nsize text, \nrelease_year text,\nprice decimal,\ntags map text,text ,\nPRIMARY KEY (release_year, price) );  Then we can query to check if the map has a value on a certain key as seen below:  SELECT * FROM product WHERE tags CONTAINS KEY 'isDeleted';", 
            "title": "Filtering"
        }, 
        {
            "location": "/Cassandra/Search Data/filtering and grouping/#grouping", 
            "text": "The data in Cassandra is grouped by the partition key and the clustering columns. The partition key can be simple (single column) or composite (multiple columns). Clustering columns are used to sort the data within the partition. Choosing the right grouping of your data can impact the read and write performance. For more details on how to choose the partition key and the clustering columns, please have a look to the  data layout section .  Additionally, the data can be grouped in Cassandra using the collection data type. Cassandra supports the use of a list, map or a set data types that can be used to group multiple related data together. For example, you can store the address of a customer using the map data type.  It is also possible to use user-defined data types that can be complete objects which can be so useful to store related data together. For instance, you can store the supplier details of a product as a user defined data type inside the product table. For more details about the collection and user defined data types, please have a look to the  data layout section .  Cassandra also supports the Tuple data type which can be used to store two or more values together in a column. For example if you want to store the product table which contains the longitude and latitude values. Then you can store it in a single column having two float values as shown below:  CREATE TABLE product ( \nid UUID, \nname int, \nlocation tuple float,float ,\nsize text, \nrelease_year text,\nprice decimal, \ntags map text,text , \nPRIMARY KEY (release_year, price) );", 
            "title": "Grouping"
        }, 
        {
            "location": "/Cassandra/Search Data/fulltext/", 
            "text": "back\n\n\nCassandra doesn't support internally full-text search. However it supports creating a custom index. Custom indexes can be created by writing a java class that implements Cassandra abstract class org.apache.cassandra.db.index.SecondaryIndex. There are also some external plugins available for full-text search such as the \nStratio\u2019s Cassandra Lucene Index\n.", 
            "title": "Fulltext"
        }, 
        {
            "location": "/Cassandra/Search Data/fulltext/#back", 
            "text": "Cassandra doesn't support internally full-text search. However it supports creating a custom index. Custom indexes can be created by writing a java class that implements Cassandra abstract class org.apache.cassandra.db.index.SecondaryIndex. There are also some external plugins available for full-text search such as the  Stratio\u2019s Cassandra Lucene Index .", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Search Data/", 
            "text": "back\n\n\nCassandra supports secondary indexes that are used to access the data using columns other than the primary key columns.  The indexes are built in the background by Cassandra without blocking writes or reads. The indexes provide efficient and fast lookup based on matching conditions for any secondary columns. \n\n\nUsually it is a good practice to create indexes on columns that aren't having high-cardinality (having many distinct or unique values). Creating indexes on high-cardinality will introduce many seeks for few results and it is not recommended. For example, it is better to create an index on the products release_year column instead of the product id since the product id has high-cardinality while the release_year has medium-cardinality and can group multiple results. In addition, creating an index on too low-cardinality columns aren't recommended since they don't make much sense. For example, if you create an index on a boolean data type that has only true or false values, then this will result on two huge rows having all the data. Therefore, it is recommended to use columns that doesn't have too high-cardinality or too low-cardinality but are having medium cardinality.\n\n\nThe other guidance principle that we should consider when creating indexes is to avoid creating indexes on columns that have frequent delete operations. The reason for that is that Cassandra stores up to 100K tombstones in the index, then the queries on the indexed columns will fail.\n\n\nIn Cassandra, you can create an index on any secondary column of a table after the table definition statement. For example, we can create an index on the product release_year as shown below:\n\n\ncreate table product (\n      id text,\n      name text,\n      price decimal,\n      release_year text,\n      size text,\n      PRIMARY KEY(id)      \n  );\n\n\n\n\nNow you can only query the product table using the partition key (id), if you want to query the table using the release_year, then you can create an index on the release_year as shown below:\n\n\nCREATE INDEX ryear ON product (release_year);\n\n\n\n\nSELECT * from product where id = 2 and release_year = \n2015\n \n\n\n\n\nIn the above example we have provided the product id since it is the partition key and the query will through an error if we don't provide it in the WHERE statement. However if you want to run query against the secondary index without specifying the partition key, then we can use the can use the \"ALLOW FILTERING\" clause as shown below:\n\n\nSELECT * from product where release_year = \n2015\n ALLOW FILTERING\n\n\n\n\nIn the same way, ff you have a composite partition key, then you should provide the values of all the keys in the SELECT statement, otherwise your query will fail. To query using only a single key of the composite partition key, then you need to create an index on this key as shown below:\n\n\ncreate table product (\n      id text,\n      name text,\n      price decimal,\n      release_year text,\n      size text,\n      PRIMARY KEY((size,release_year))      \n  );\n\n\n// The above query will fail since you need to provide the size in the WHERE statement:\n\nSELECT * from product where release_year = \n2015\n \n\n\n// However you can create an index on the release_year column as shown below:\n\n\nCREATE INDEX ryear ON product (release_year);\n\n// Then the SELECT query will succeed.\n\n\n\n\n\nIt is also possible to create multiple secondary indexes as shown below:\n\n\nCREATE INDEX name ON product ( name );\nCREATE INDEX price ON product ( price );\n\n\n\n\nThen you can run queries on both secondary indexes as below:\n\n\nSELECT * from product where price \n 1000 AND name = \nDell\n ALLOW FILTERING;\n\n\n\n\nFinally, you can create indexes on a collection such as a list, a map or a set as shown below:\n\n\ncreate table product (\n      id text,\n      name text,\n      price decimal,\n      tags set\ntext\n,\n      release_year text,\n      size text,\n      PRIMARY KEY((size,release_year))      \n  );\n\n\n\n\nCREATE INDEX tags ON product ( tags );\n\n\n\n\nThen you can run queries on the collection column as the below:\n\n\nSELECT * FROM product WHERE tags CONTAINS 'Electronics' ALLOW FILTERING;\n\n\n\n\nOr you create an index on a map like below:\n\n\ncreate table product (\n      id text,\n      name text,\n      price decimal,\n      tags set\ntext\n,\n      address map\ntext,text\n,\n      release_year text,\n      size text,\n      PRIMARY KEY((size,release_year))      \n  );\n\n\n\n\nCREATE INDEX address ON product (ENTRIES(address));\n\n\n\n\nThen you can run queries against the ma as the below:\n\n\nSELECT * FROM product WHERE address['street'] =  'Breslauer Str. 2' ALLOW FILTERING;\n\n\n\n\nFinally, you can create an index on a list as below:\n\n\ncreate table product (\n      id text,\n      name text,\n      price decimal,\n      tags set\ntext\n,\n      address map\ntext,text\n,\n      categories  List\nint\n,\n      release_year text,\n      size text,\n      PRIMARY KEY((size,release_year))      \n  );\n\n\n\n\nCREATE INDEX categories ON product (FULL(categories)) ALLOW FILTERING;\n\n\n\n\nThen you can run a query against the list like below:\n\n\nSELECT * FROM product WHERE categories =  [1,4,7] ALLOW FILTERING;", 
            "title": "Home"
        }, 
        {
            "location": "/Cassandra/Search Data/#back", 
            "text": "Cassandra supports secondary indexes that are used to access the data using columns other than the primary key columns.  The indexes are built in the background by Cassandra without blocking writes or reads. The indexes provide efficient and fast lookup based on matching conditions for any secondary columns.   Usually it is a good practice to create indexes on columns that aren't having high-cardinality (having many distinct or unique values). Creating indexes on high-cardinality will introduce many seeks for few results and it is not recommended. For example, it is better to create an index on the products release_year column instead of the product id since the product id has high-cardinality while the release_year has medium-cardinality and can group multiple results. In addition, creating an index on too low-cardinality columns aren't recommended since they don't make much sense. For example, if you create an index on a boolean data type that has only true or false values, then this will result on two huge rows having all the data. Therefore, it is recommended to use columns that doesn't have too high-cardinality or too low-cardinality but are having medium cardinality.  The other guidance principle that we should consider when creating indexes is to avoid creating indexes on columns that have frequent delete operations. The reason for that is that Cassandra stores up to 100K tombstones in the index, then the queries on the indexed columns will fail.  In Cassandra, you can create an index on any secondary column of a table after the table definition statement. For example, we can create an index on the product release_year as shown below:  create table product (\n      id text,\n      name text,\n      price decimal,\n      release_year text,\n      size text,\n      PRIMARY KEY(id)      \n  );  Now you can only query the product table using the partition key (id), if you want to query the table using the release_year, then you can create an index on the release_year as shown below:  CREATE INDEX ryear ON product (release_year);  SELECT * from product where id = 2 and release_year =  2015    In the above example we have provided the product id since it is the partition key and the query will through an error if we don't provide it in the WHERE statement. However if you want to run query against the secondary index without specifying the partition key, then we can use the can use the \"ALLOW FILTERING\" clause as shown below:  SELECT * from product where release_year =  2015  ALLOW FILTERING  In the same way, ff you have a composite partition key, then you should provide the values of all the keys in the SELECT statement, otherwise your query will fail. To query using only a single key of the composite partition key, then you need to create an index on this key as shown below:  create table product (\n      id text,\n      name text,\n      price decimal,\n      release_year text,\n      size text,\n      PRIMARY KEY((size,release_year))      \n  );\n\n\n// The above query will fail since you need to provide the size in the WHERE statement:\n\nSELECT * from product where release_year =  2015  \n\n\n// However you can create an index on the release_year column as shown below:\n\n\nCREATE INDEX ryear ON product (release_year);\n\n// Then the SELECT query will succeed.  It is also possible to create multiple secondary indexes as shown below:  CREATE INDEX name ON product ( name );\nCREATE INDEX price ON product ( price );  Then you can run queries on both secondary indexes as below:  SELECT * from product where price   1000 AND name =  Dell  ALLOW FILTERING;  Finally, you can create indexes on a collection such as a list, a map or a set as shown below:  create table product (\n      id text,\n      name text,\n      price decimal,\n      tags set text ,\n      release_year text,\n      size text,\n      PRIMARY KEY((size,release_year))      \n  );  CREATE INDEX tags ON product ( tags );  Then you can run queries on the collection column as the below:  SELECT * FROM product WHERE tags CONTAINS 'Electronics' ALLOW FILTERING;  Or you create an index on a map like below:  create table product (\n      id text,\n      name text,\n      price decimal,\n      tags set text ,\n      address map text,text ,\n      release_year text,\n      size text,\n      PRIMARY KEY((size,release_year))      \n  );  CREATE INDEX address ON product (ENTRIES(address));  Then you can run queries against the ma as the below:  SELECT * FROM product WHERE address['street'] =  'Breslauer Str. 2' ALLOW FILTERING;  Finally, you can create an index on a list as below:  create table product (\n      id text,\n      name text,\n      price decimal,\n      tags set text ,\n      address map text,text ,\n      categories  List int ,\n      release_year text,\n      size text,\n      PRIMARY KEY((size,release_year))      \n  );  CREATE INDEX categories ON product (FULL(categories)) ALLOW FILTERING;  Then you can run a query against the list like below:  SELECT * FROM product WHERE categories =  [1,4,7] ALLOW FILTERING;", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Search Data/query/", 
            "text": "back\n\n\nCQL supports a SELECT statement similar to SQL with many options that can be used to run simple or complex queries in Cassandra. In this section, I will explain how you can query your data in Cassandra using CQL SELECT statement.\n\n\nSELECT statement in CQL can be used with the WHERE clause to run parameterised or range queries. However in Cassandra, you can use the WHERE statement only with the columns that are either part of the primary key or having secondary indexes. Otherwise, the query will fail. \n\n\nAssuming we have created a product table using the statement below:\n\n\nCREATE TABLE product (\n id UUID, \n name text,\n size text, \n release_year text,\n price decimal, \n tags set\ntext\n,\n PRIMARY KEY (release_year, price) );\n\n\n\n\nThen we can run many queries against the product table as shown below:\n\n\n// get all from product\nSELECT * FROM product\n\n// Or return only the product name\nSELECT name FROM product\n\n// Or query using the partition key\nSELECT * FROM product WHERE release_year = '2015'\n\n// or run range queries using the clustering column:\nSELECT * FROM product WHERE release_year = '2015' AND price \n 1000\n\n// You can also return the results in a JSON format \nSELECT JSON FROM product WHERE release_year = '2015' AND price \n 1000\n\n\n\n\nCQL SELECT statement supports also an IN statement that can be used to run queries like below:\n\n\nSELECT * FROM product WHERE release_year IN ('2015' , '2014') \n\n\n\n\nCassandra supports querying some system tables that can be used to get information about keyspaces, tables and the available user-defined data types and functions. You can query these tables whenever you want to know about the current data model. Example is shown below:\n\n\nTo get information about the available keyspaces:\n\n\nSELECT * FROM system.schema_keyspaces;\n\n\n\n\nTo get information about the available tables:\n\n\nSELECT * FROM system.schema_columnfamilies \n\n\n\n\nTo search for table columns, you can query the below system table:\n\n\nSELECT * FROM system.schema_columns\n\n\n\n\nTo get the current cluster informations:\n\n\nSELECT * FROM system.peers;\n\n\n\n\nTo query the available user defined functions:\n\n\nSELECT * FROM system.schema_functions;\n\n\n\n\nTo query all available user defined aggregates functions:\n\n\nSELECT * FROM system.schema_aggregates;\n\n\n\n\nTo query all available user defined data types:\n\n\nSELECT * FROM system.schema_usertypes;", 
            "title": "Query"
        }, 
        {
            "location": "/Cassandra/Search Data/query/#back", 
            "text": "CQL supports a SELECT statement similar to SQL with many options that can be used to run simple or complex queries in Cassandra. In this section, I will explain how you can query your data in Cassandra using CQL SELECT statement.  SELECT statement in CQL can be used with the WHERE clause to run parameterised or range queries. However in Cassandra, you can use the WHERE statement only with the columns that are either part of the primary key or having secondary indexes. Otherwise, the query will fail.   Assuming we have created a product table using the statement below:  CREATE TABLE product (\n id UUID, \n name text,\n size text, \n release_year text,\n price decimal, \n tags set text ,\n PRIMARY KEY (release_year, price) );  Then we can run many queries against the product table as shown below:  // get all from product\nSELECT * FROM product\n\n// Or return only the product name\nSELECT name FROM product\n\n// Or query using the partition key\nSELECT * FROM product WHERE release_year = '2015'\n\n// or run range queries using the clustering column:\nSELECT * FROM product WHERE release_year = '2015' AND price   1000\n\n// You can also return the results in a JSON format \nSELECT JSON FROM product WHERE release_year = '2015' AND price   1000  CQL SELECT statement supports also an IN statement that can be used to run queries like below:  SELECT * FROM product WHERE release_year IN ('2015' , '2014')   Cassandra supports querying some system tables that can be used to get information about keyspaces, tables and the available user-defined data types and functions. You can query these tables whenever you want to know about the current data model. Example is shown below:  To get information about the available keyspaces:  SELECT * FROM system.schema_keyspaces;  To get information about the available tables:  SELECT * FROM system.schema_columnfamilies   To search for table columns, you can query the below system table:  SELECT * FROM system.schema_columns  To get the current cluster informations:  SELECT * FROM system.peers;  To query the available user defined functions:  SELECT * FROM system.schema_functions;  To query all available user defined aggregates functions:  SELECT * FROM system.schema_aggregates;  To query all available user defined data types:  SELECT * FROM system.schema_usertypes;", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Search Data/queryOptions/", 
            "text": "back\n\n\nCassandra has a built-in support for common aggregate functions such as min, max, avg, sum, and count.  Cassandra also supports the creation of custom user defined functions. In this section, I will explain how to use the built-in functions as well as the user defined functions.\n\n\nAssuming we have created the below order table:\n\n\nCREATE TABLE order (id UUID, items set\ntext\n, total decimal, order_year, status text\n PRIMARY KEY (order_year,total);\n\n\n\n\nThen you can get the sum of all orders totals for the year 2015 as shown below:\n\n\nSELECT sum(total) FROM order WHERE order_year='2015';\n\n\n\n\nOr get the average price of all the orders in the year 2015:\n\n\nSELECT avg(total) FROM order WHERE order_year='2015';\n\n\n\n\nOr get the min or max order total price for the year 2015:\n\n\nSELECT min(total) FROM order WHERE order_year='2015';\n\n\n\n\nSELECT max(total) FROM order WHERE order_year='2015';\n\n\n\n\nAnd to get the count of the orders in 2015:\n\n\nSELECT count(*) FROM order WHERE order_year='2015';\n\n\n\n\nFinally you can limit your returned results using the LIMIT clause as shown below:\n\n\nSELECT * FROM order WHERE order_year='2015' LIMIT 100;\n\n\n\n\nIn the above query, we are returning the first 100 results.\n\n\nCassandra also supports user defined functions. Assuming we want to get also the TAX for each order. Then we can create a custom user defined function to support this as seen below:\n\n\nCREATE OR REPLACE FUNCTION fTax (input double,taxPercentage int) CALLED\n ON NULL INPUT RETURNS double LANGUAGE java AS 'return\n Double.valueOf(input.doubleValue() * taxPercentage);';\n\n\n\n\nThen we can run a query as below:\n\n\nSELECT id,fTax(total, 0.06) FROM order WHERE order_year='2015' ;", 
            "title": "queryOptions"
        }, 
        {
            "location": "/Cassandra/Search Data/queryOptions/#back", 
            "text": "Cassandra has a built-in support for common aggregate functions such as min, max, avg, sum, and count.  Cassandra also supports the creation of custom user defined functions. In this section, I will explain how to use the built-in functions as well as the user defined functions.  Assuming we have created the below order table:  CREATE TABLE order (id UUID, items set text , total decimal, order_year, status text\n PRIMARY KEY (order_year,total);  Then you can get the sum of all orders totals for the year 2015 as shown below:  SELECT sum(total) FROM order WHERE order_year='2015';  Or get the average price of all the orders in the year 2015:  SELECT avg(total) FROM order WHERE order_year='2015';  Or get the min or max order total price for the year 2015:  SELECT min(total) FROM order WHERE order_year='2015';  SELECT max(total) FROM order WHERE order_year='2015';  And to get the count of the orders in 2015:  SELECT count(*) FROM order WHERE order_year='2015';  Finally you can limit your returned results using the LIMIT clause as shown below:  SELECT * FROM order WHERE order_year='2015' LIMIT 100;  In the above query, we are returning the first 100 results.  Cassandra also supports user defined functions. Assuming we want to get also the TAX for each order. Then we can create a custom user defined function to support this as seen below:  CREATE OR REPLACE FUNCTION fTax (input double,taxPercentage int) CALLED\n ON NULL INPUT RETURNS double LANGUAGE java AS 'return\n Double.valueOf(input.doubleValue() * taxPercentage);';  Then we can run a query as below:  SELECT id,fTax(total, 0.06) FROM order WHERE order_year='2015' ;", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Search Data/refernces/", 
            "text": "back\n\n\n1- http://docs.datastax.com/en/\n\n\n2- Cassandra: The Definitive Guide ,Eben Hewitt\n\n\n3- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Search Data/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Search Data/refernces/#1-httpdocsdatastaxcomen", 
            "text": "", 
            "title": "1- http://docs.datastax.com/en/"
        }, 
        {
            "location": "/Cassandra/Search Data/refernces/#2-cassandra-the-definitive-guide-eben-hewitt", 
            "text": "", 
            "title": "2- Cassandra: The Definitive Guide ,Eben Hewitt"
        }, 
        {
            "location": "/Cassandra/Search Data/refernces/#3-practical-cassandra-a-developers-approach-by-eric-lubow-and-russell-bradberry", 
            "text": "", 
            "title": "3- Practical Cassandra: A Developer's Approach by Eric Lubow and Russell Bradberry"
        }, 
        {
            "location": "/Cassandra/Search Data/regx/", 
            "text": "back\n\n\nUnfortunately CQL doesn't support anything similar to the SQL LIKE clause that can be used to  search the data using regular expressions. There is no built-in support in Cassandra for regular expressions and usually people use search engines plugins to use regular expression or to do full-text search.", 
            "title": "Regx"
        }, 
        {
            "location": "/Cassandra/Search Data/regx/#back", 
            "text": "Unfortunately CQL doesn't support anything similar to the SQL LIKE clause that can be used to  search the data using regular expressions. There is no built-in support in Cassandra for regular expressions and usually people use search engines plugins to use regular expression or to do full-text search.", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/", 
            "text": "back\n\n\nFull Text Search\n\n\nRegular Expressions\n\n\nAggregation\n\n\nIndexing\n\n\nQuery\n\n\nFiltering and Grouping\n\n\nSorting\n\n\nRefernces", 
            "title": "Search data main"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/#full-text-search", 
            "text": "", 
            "title": "Full Text Search"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/#regular-expressions", 
            "text": "", 
            "title": "Regular Expressions"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/#aggregation", 
            "text": "", 
            "title": "Aggregation"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/#indexing", 
            "text": "", 
            "title": "Indexing"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/#query", 
            "text": "", 
            "title": "Query"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/#filtering-and-grouping", 
            "text": "", 
            "title": "Filtering and Grouping"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/#sorting", 
            "text": "", 
            "title": "Sorting"
        }, 
        {
            "location": "/Cassandra/Search Data/search_data_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Search Data/sort/", 
            "text": "back\n\n\nCassandra supports sorting using the clustering columns. When you create a table, you can define  clustering columns which will be used to sort the data inside each partition in either ascending or descending orders. Then you can easily use the ORDER BY clause with the ASC or DESC options. An example is given blow:\n\n\nAssuming we have created the below table:\n\n\nCREATE TABLE product (\n id UUID, \n name text,\n size text, \n release_year text,\n price decimal, \n tags set\ntext\n, \n PRIMARY KEY (release_year, price) \n) WITH CLUSTERING ORDER BY (price DESC);\n\n\n\n\nNotice that we used the price column to be the clustering column and defined the default sorting order to be DESC using the \"WITH CLUSTERING ORDER BY\" clause. If we didn't use the \"WITH CLUSTERING ORDER BY\", the default order is ascending. Now if we run a query against the product table, we will get the data sorted descendingly by the price in each partition.\n\n\nSELECT * FROM product  where release_year = 2015\n\n\n\n\nOr to sort the data in an ascending order, we can run the below query:\n\n\nSELECT * FROM product  where release_year = 2015\nORDER BY price ASC", 
            "title": "Sort"
        }, 
        {
            "location": "/Cassandra/Search Data/sort/#back", 
            "text": "Cassandra supports sorting using the clustering columns. When you create a table, you can define  clustering columns which will be used to sort the data inside each partition in either ascending or descending orders. Then you can easily use the ORDER BY clause with the ASC or DESC options. An example is given blow:  Assuming we have created the below table:  CREATE TABLE product (\n id UUID, \n name text,\n size text, \n release_year text,\n price decimal, \n tags set text , \n PRIMARY KEY (release_year, price) \n) WITH CLUSTERING ORDER BY (price DESC);  Notice that we used the price column to be the clustering column and defined the default sorting order to be DESC using the \"WITH CLUSTERING ORDER BY\" clause. If we didn't use the \"WITH CLUSTERING ORDER BY\", the default order is ascending. Now if we run a query against the product table, we will get the data sorted descendingly by the price in each partition.  SELECT * FROM product  where release_year = 2015  Or to sort the data in an ascending order, we can run the below query:  SELECT * FROM product  where release_year = 2015\nORDER BY price ASC", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Special Features/graphgist/", 
            "text": "back", 
            "title": "Graphgist"
        }, 
        {
            "location": "/Cassandra/Special Features/graphgist/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Special Features/refernces/", 
            "text": "back\n\n\n1- https://docs.mongodb.org/manual\n\n\n2- https://www.mongodb.com/blog/post/building-mongodb-applications-binary-files-using-gridfs-part-1\n\n\n3- https://www.mongodb.com/blog/post/building-mongodb-applications-binary-files-using-gridfs-part-2", 
            "title": "Refernces"
        }, 
        {
            "location": "/Cassandra/Special Features/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Special Features/refernces/#1-httpsdocsmongodborgmanual", 
            "text": "", 
            "title": "1- https://docs.mongodb.org/manual"
        }, 
        {
            "location": "/Cassandra/Special Features/refernces/#2-httpswwwmongodbcomblogpostbuilding-mongodb-applications-binary-files-using-gridfs-part-1", 
            "text": "", 
            "title": "2- https://www.mongodb.com/blog/post/building-mongodb-applications-binary-files-using-gridfs-part-1"
        }, 
        {
            "location": "/Cassandra/Special Features/refernces/#3-httpswwwmongodbcomblogpostbuilding-mongodb-applications-binary-files-using-gridfs-part-2", 
            "text": "", 
            "title": "3- https://www.mongodb.com/blog/post/building-mongodb-applications-binary-files-using-gridfs-part-2"
        }, 
        {
            "location": "/Cassandra/Special Features/special_features_main/", 
            "text": "back\n\n\nORM support", 
            "title": "Special features main"
        }, 
        {
            "location": "/Cassandra/Special Features/special_features_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Cassandra/Special Features/special_features_main/#orm-support", 
            "text": "", 
            "title": "ORM support"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Availability/", 
            "text": "Availability measures the percentage of time an application is working for the client applications. It is quite common, that with time the database infrastructures suffer some hardware failures that might impact the availability of the database. MongoDB protects itself from such unavailability by supporting features to enable data redundancy through replication.  In the following section, I will talk about how replication is supported in MongoDB and how is the failover process is usually handled.\n\n\nReplication\n\n\nMongoDB supports a configuration called a replica set which is a group of mongod instances that use the same data set. Each replica set has only one a primary node and the rest are all secondaries. \n\n\n\n\nMongoDB supports also a special type of nodes called arbiters which are used only in case that there are even number of nodes in the replica set. Even number of nodes in a replication set can prevent a successful voting process which is needed in case the primary isn't working. The arbiter node doesn't contain any data and can't be selected as a primary node. The main task of an arbiter is to add one vote which makes the number of votes an odd number to allow for a successful majority voting in case of primary failure.\n\n\n\n\nThe primary node is the node responsible for all write operations and records all of its operations to an operation log called oplog. The oplog is then used by secondary nodes to replicate the primary data set. The primary node is also the one responsible for read operations, the application can chose a different read concern if it needs to read data from secondary nodes but it is not recommended since the data in secondary nodes might be stale. If you want to scale reads, you can use sharding and not replication. Replication in MongoDB is mainly supported for data redundancy and availability. \n\n\nSecondaries first do an initial sync when the secondary member doesn't have data such as when the member is new. During the initial sync, the secondary node will clone all the databases and build all indexes on all collections. After that the secondary nodes will just do a simple sync operation to replicate the primary node data using the oplog asynchronously. \n\n\nMongoDB also supports that you configure a secondary node so that it can't be promoted into a primary node in case of a failure by changing it is priority to zero. \n\n\ncfg.members[2].priority = 0\n\n\n\n\nIf you want to use some secondary nodes for specific tasks such as reporting or backup, you can configure the nodes to be hidden by setting its priority to zero to prevent a hidden member to become primary and setting the hidden flag to true as seen below:\n\n\n{\n  \n_id\n : \nnum\n\n  \nhost\n : \nhostname:port\n,\n  \npriority\n : 0,\n  \nhidden\n : true\n}\n\n\n\n\nThe hidden member can still participate in the voting process in case of primary failure.\n\n\n\n\nYou can also configure a secondary node to act as a delayed replica that contains an earlier version of the data set and can be used later for rollback or running historical snapshots. This can be done by setting priority to zero to prevent the node from becoming a primary node and setting the hidden flat to true and setting the delay time as seen from the configuration below:\n\n\n{\n   \n_id\n : \nnum\n,\n   \nhost\n : \nhostname:port\n,\n   \npriority\n : 0,\n   \nslaveDelay\n : \nseconds\n,\n   \nhidden\n : true\n}\n\n\n\n\n \n\n\nFailover\n\n\nMongoDB supports an automatic failover process that takes place when the primary node in a replica set isn't accessible for some reasons. If the other replica set members aren't able to communicate with the primary server for more than 10 seconds, an eligible secondary member will automatically be promoted to act as a primary node. The selection of the secondary node that will replace the failed primary node is based on a majority voting of the replica set members. \n\n\n\n\nIt is always recommended, to put all members that participate on the majority voting and all the members that can become primary in the same facility to prevent any network partition problems from impacting the failover process.", 
            "title": "Availability"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Availability/#replication", 
            "text": "MongoDB supports a configuration called a replica set which is a group of mongod instances that use the same data set. Each replica set has only one a primary node and the rest are all secondaries.    MongoDB supports also a special type of nodes called arbiters which are used only in case that there are even number of nodes in the replica set. Even number of nodes in a replication set can prevent a successful voting process which is needed in case the primary isn't working. The arbiter node doesn't contain any data and can't be selected as a primary node. The main task of an arbiter is to add one vote which makes the number of votes an odd number to allow for a successful majority voting in case of primary failure.   The primary node is the node responsible for all write operations and records all of its operations to an operation log called oplog. The oplog is then used by secondary nodes to replicate the primary data set. The primary node is also the one responsible for read operations, the application can chose a different read concern if it needs to read data from secondary nodes but it is not recommended since the data in secondary nodes might be stale. If you want to scale reads, you can use sharding and not replication. Replication in MongoDB is mainly supported for data redundancy and availability.   Secondaries first do an initial sync when the secondary member doesn't have data such as when the member is new. During the initial sync, the secondary node will clone all the databases and build all indexes on all collections. After that the secondary nodes will just do a simple sync operation to replicate the primary node data using the oplog asynchronously.   MongoDB also supports that you configure a secondary node so that it can't be promoted into a primary node in case of a failure by changing it is priority to zero.   cfg.members[2].priority = 0  If you want to use some secondary nodes for specific tasks such as reporting or backup, you can configure the nodes to be hidden by setting its priority to zero to prevent a hidden member to become primary and setting the hidden flag to true as seen below:  {\n   _id  :  num \n   host  :  hostname:port ,\n   priority  : 0,\n   hidden  : true\n}  The hidden member can still participate in the voting process in case of primary failure.   You can also configure a secondary node to act as a delayed replica that contains an earlier version of the data set and can be used later for rollback or running historical snapshots. This can be done by setting priority to zero to prevent the node from becoming a primary node and setting the hidden flat to true and setting the delay time as seen from the configuration below:  {\n    _id  :  num ,\n    host  :  hostname:port ,\n    priority  : 0,\n    slaveDelay  :  seconds ,\n    hidden  : true\n}", 
            "title": "Replication"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Availability/#failover", 
            "text": "MongoDB supports an automatic failover process that takes place when the primary node in a replica set isn't accessible for some reasons. If the other replica set members aren't able to communicate with the primary server for more than 10 seconds, an eligible secondary member will automatically be promoted to act as a primary node. The selection of the secondary node that will replace the failed primary node is based on a majority voting of the replica set members.    It is always recommended, to put all members that participate on the majority voting and all the members that can become primary in the same facility to prevent any network partition problems from impacting the failover process.", 
            "title": "Failover"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Backup/", 
            "text": "In any production environment, we should have a backup strategy to restore our data back in case of failures. MongoDB provides a tool called mongodump that can read data from the database and creates a backup BSON files. To restore the created backup files, you can use the mongorestore tool to populate the MongoDB database with the backup BSON files.\n\n\nAfter restoring the data using mongorestore, the mongod instance must rebuild all indexes. Mongodump tool can impact the mongod instance performance, so it is usually recommended to run this tool against a secondary member in a replica set. \n\n\nIf you are using a replica set configuration, it is also possible to take a point in time backup using mongodump with the --oplog option. This means that mongodump will still be able to capture the changes to the data if the application modifies the data while backing up. To restore a point in time backup, you can use the mongorestore with the --oplogReplay option.\n\n\nYou can take a backup for the whole server or for a specific collection. To create a backup on a certain server, you can run the below command:\n\n\nmongodump --host mongodb.example.net --port 27017\n\n\n\n\nThe above command will create the backup database with name dump/ in the current directory. \n\n\nTo take a backup for a particular collection in a database, you can use the below command:\n\n\nmongodump --collection myCollection --db test\n\n\n\n\nTo restore a specific backup, you can run the below command:\n\n\nmongorestore --port \nport number\n \npath to the backup", 
            "title": "Backup"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Configuration/", 
            "text": "MongoDB uses a configuration file for each mongod or mongos instance that contains all the settings and command options that will be used by this instance. The file has a \nYAML format\n. As an example, below are some basic configurations:\n\n\nprocessManagement:\n   fork: true\nnet:\n   bindIp: 127.0.0.1\n   port: 27017\nstorage:\n   dbPath: /srv/mongodb\nsystemLog:\n   destination: file\n   path: \n/var/log/mongodb/mongod.log\n\n   logAppend: true\nstorage:\n   journal:\n      enabled: true\n\n ````\n\n\nWhen you start your mongod or mongos instance, you need to specify the configuration file that will be used as shown below:\n\n\n\n\n\nmongod --config /etc/mongod.conf\n\n\nmongos --config /etc/mongos.conf\n````\n\n\nIf you want to change a configuration option in the configuration file of a mongod or mongos instances, you will need to restart the instance to pick up the new changes.\n\n\nFor a complete list of all the configuration options available for MongoDB, please review \nMongoDB documentation.", 
            "title": "Configuration"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Persistency/", 
            "text": "MongoDB supports two persistent storage engines: the WiredTiger storage engine or the MMAPv1 storage engine. Another non-persistent storage engine is also supported which is called the In-Memory storage engine where all data will be lost if the mongod instance is shut down for any reason. The in-memory storage engine is still in beta and is mainly used to get more predictable latency of database operations. Usually, persistency is supported by flushing the data from the memory to disk periodically (default is each 60 seconds). In addition, a mechanism called journalling is also used to provide more durable solution in the event of failure. MonogoDB uses journal files to store the in-memory changes and can be used later to recover the data when the server crashes before flushing data to disk files. In the following sections, I will talk about how persistency is supported in the WiredTiger and MMAPv1 storage engines.\n\n\nThe WiredTiger storage engine\n\n\nThis is storage engine is the default engine used by MongoDB version 3.2. In this storage engine, MongoDB takes a point-on-time snapshots for the user's data each 60 seconds or when 2 GB of journal data has been written if journalling is enabled. Journalling is enabled by default and if not you can enable it by changing the \"storage.journal.enabled\" option to false. Then If a new checkpoint is successfully written, the old checkpoint will be freed. This means that the WiredTiger storage engine persists all the data modifications between checkpoints, however if the server exits between the checkpoints, it will use the journal files to recover the modifications since last checkpoint. \n\n\nThe MMAPv1 storage engine\n\n\nUsing this storage engine, MongoDB first writes the changes to the private in-memory view when a write operation occurs. Then the private view changes will be written to the journal files. And then upon a journal commit, the data will be written to the shared view. The shared view flushes the data to the on-disk files periodically (usually each 60 seconds). So if the mongod instance crashed before flushing the shared view to disk, the journal files can be used to recover the shared view. If the journal files contain only already flushed data, then it can be recycled for reuse.\n\n\nFrom the above, we can see that the write operations in MongoDB are durable when journalling is enabled. Otherwise, write operations could be lost between checkpoints or flushing data to disk. So the write operation is considered durable when it has been written to the journal files in case of a single server mongod. In case of a replica set, the write operation is durable if it has been written to the journal files of the majority voting nodes.", 
            "title": "Persistency"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Persistency/#the-wiredtiger-storage-engine", 
            "text": "This is storage engine is the default engine used by MongoDB version 3.2. In this storage engine, MongoDB takes a point-on-time snapshots for the user's data each 60 seconds or when 2 GB of journal data has been written if journalling is enabled. Journalling is enabled by default and if not you can enable it by changing the \"storage.journal.enabled\" option to false. Then If a new checkpoint is successfully written, the old checkpoint will be freed. This means that the WiredTiger storage engine persists all the data modifications between checkpoints, however if the server exits between the checkpoints, it will use the journal files to recover the modifications since last checkpoint.", 
            "title": "The WiredTiger storage engine"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Persistency/#the-mmapv1-storage-engine", 
            "text": "Using this storage engine, MongoDB first writes the changes to the private in-memory view when a write operation occurs. Then the private view changes will be written to the journal files. And then upon a journal commit, the data will be written to the shared view. The shared view flushes the data to the on-disk files periodically (usually each 60 seconds). So if the mongod instance crashed before flushing the shared view to disk, the journal files can be used to recover the shared view. If the journal files contain only already flushed data, then it can be recycled for reuse.  From the above, we can see that the write operations in MongoDB are durable when journalling is enabled. Otherwise, write operations could be lost between checkpoints or flushing data to disk. So the write operation is considered durable when it has been written to the journal files in case of a single server mongod. In case of a replica set, the write operation is durable if it has been written to the journal files of the majority voting nodes.", 
            "title": "The MMAPv1 storage engine"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Scalability/", 
            "text": "As your data grows, your sever won't be able to handle the high rate of queries and the larger sets will exceed the server storage capacity. To address these issues, you will have to do either a vertical or horizontal scaling. You can scale your server vertically by adding more CPUs and storage resources but there is always a limit to what you can add to a single server machine. In the other hand, you can scale horizontally by distributing your data over multiple servers which is also called Sharding. Each shard will act as a separate database and all the shards make up the complete logical database. By using sharding, you can meet the demands of the high throughput read/write operations as well as the very large data sets.\n\n\nFortunately, MongoDB supports sharding by using a sharded cluster which is composed of three components: the query routers, the configuration servers and the shards. In the following sections, I will explain these components in more details. Sharding in MongoDB is done only on the collection level. You can also either shard one collection, multiple collections or the whole data as you choose.\n\n\n\n\nQuery Routers\n\n\nThese are mongos instances which are used by the client applications to interact with the sharded data. Client applications can't access the shards directly and can only submit read and write requests to mongos instance. Mongos instance is the one responsible for routing these reads and writes requests to the respective shards.  \n\n\nMongos has no persistent state and knows only what data is in which shard by tracking the metadata information stored in the configuration servers. \n\n\nMongos performs operations in sharded data by broadcasting to all the shards that holds the documents of a collection or target only shard based on the shard key. Usually, multi-update and remove operations are a broadcast operations. \n\n\nIn general, most sharded clusters contain multiple mongos instances to divide the client requests load, but you also use only a single instance.\n\n\nConfig Servers\n\n\nThe config servers holds the metadata of the sharded cluster such as the shards locations of each sharded data. Config servers contain very important information for the sharded cluster to work and if the servers are down for any reason, the sharded cluster becomes inoperable. For this reason, it is a good idea to replicate the data in the config server using a replica set configuration which allows the sharded cluster to have more than 3 config servers and up to 50 servers to ensure availability.\n\n\nMongoDB stores the sharded cluster metadata information in the config servers and update this information whenever there is a chunk split or chunk migration. A chunk split happens when chunk's data grows beyond the chunk size which will make the mongos instance split this chunk into halves. This can lead to an unevenly data distribution among shards which starts the balancing process that leads to chunk migration. A chunk migration is the process of moving one chunk from a particular shard to another to achieve even data distribution among shards.\n\n\n\n\n\n\nShards\n\n\nThe shards are a replica set or a single server to hold part of the sharded data. Usually, each shard is a replica set which provides high availability and data redundancy that can be used in case of disaster recovery. \n\n\nIn any sharded cluster, there is a primary set that contains the unsharded collections. \n\n\n \n\n\nTo find out which shard is a primary in a sharded cluster, you can run the mongo shell command sh.status().\n\n\nData Distribution in a Sharded Cluster\n\n\nFirst to shard the data, a shard key needs to be selected. The sharded key should be either a single indexed field or a compound index. Mongodb then divid the sharded key values evenly over the shards as chunks.\n\n\nTo ensure that the data in the different shards are distributed evenly. MongoDB use a splitting and a balancing background processes. The splitting process splits the chunks data whenever the data grows beyond a pre-defined chunk size. And the balancing process moves the chunks between different shards whenever the data in unevenly distributed. \n\n\nThe splitting and balancing processes usually triggered when a new data is added or removed, and when a new shard is added or removed. \n\n\nFor a step by step tutorial on how to deploy a sharded cluster, please have a look at \nMongoDB documentation.\n\n\nData Partitioning\n\n\nAs explained previously, MongoDB partition the data based on the sharded key. To partition the data using the sharded key, MongoDB either use range based partitioning or hash based partitioning as explained below:\n\n\nRange Based Sharding\n\n\nMongoDB divides the data into ranges based on the sharded key values. For instance, if we have a numeric sharded key values, we will divid the data based on the possible range of this numeric value. Then MongoDB partitions the data based on the value of the sharded key  and distribute it to the shard responsible for that value range as shown below:\n\n\n\n\nRange based sharding is better for range queries because MongoDB will check which shards are within the requested range. However, range based sharding can result in an uneven distribution which is not good for scalability. \n\n\nHash Based Sharding\n\n\nMongoDB supports also Hash Based Sharding which compute a hash value for the sharded key and then use this hash value to select the shard. This ensures better data distribution where data is evenly distributed across the cluster. However it is not efficient when it comes to range queries since the data is randomly distributed across the sharded cluster based on the hash function. An example is shown below:", 
            "title": "Scalability"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Scalability/#query-routers", 
            "text": "These are mongos instances which are used by the client applications to interact with the sharded data. Client applications can't access the shards directly and can only submit read and write requests to mongos instance. Mongos instance is the one responsible for routing these reads and writes requests to the respective shards.    Mongos has no persistent state and knows only what data is in which shard by tracking the metadata information stored in the configuration servers.   Mongos performs operations in sharded data by broadcasting to all the shards that holds the documents of a collection or target only shard based on the shard key. Usually, multi-update and remove operations are a broadcast operations.   In general, most sharded clusters contain multiple mongos instances to divide the client requests load, but you also use only a single instance.", 
            "title": "Query Routers"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Scalability/#config-servers", 
            "text": "The config servers holds the metadata of the sharded cluster such as the shards locations of each sharded data. Config servers contain very important information for the sharded cluster to work and if the servers are down for any reason, the sharded cluster becomes inoperable. For this reason, it is a good idea to replicate the data in the config server using a replica set configuration which allows the sharded cluster to have more than 3 config servers and up to 50 servers to ensure availability.  MongoDB stores the sharded cluster metadata information in the config servers and update this information whenever there is a chunk split or chunk migration. A chunk split happens when chunk's data grows beyond the chunk size which will make the mongos instance split this chunk into halves. This can lead to an unevenly data distribution among shards which starts the balancing process that leads to chunk migration. A chunk migration is the process of moving one chunk from a particular shard to another to achieve even data distribution among shards.", 
            "title": "Config Servers"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Scalability/#shards", 
            "text": "The shards are a replica set or a single server to hold part of the sharded data. Usually, each shard is a replica set which provides high availability and data redundancy that can be used in case of disaster recovery.   In any sharded cluster, there is a primary set that contains the unsharded collections.      To find out which shard is a primary in a sharded cluster, you can run the mongo shell command sh.status().", 
            "title": "Shards"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Scalability/#data-distribution-in-a-sharded-cluster", 
            "text": "First to shard the data, a shard key needs to be selected. The sharded key should be either a single indexed field or a compound index. Mongodb then divid the sharded key values evenly over the shards as chunks.  To ensure that the data in the different shards are distributed evenly. MongoDB use a splitting and a balancing background processes. The splitting process splits the chunks data whenever the data grows beyond a pre-defined chunk size. And the balancing process moves the chunks between different shards whenever the data in unevenly distributed.   The splitting and balancing processes usually triggered when a new data is added or removed, and when a new shard is added or removed.   For a step by step tutorial on how to deploy a sharded cluster, please have a look at  MongoDB documentation.", 
            "title": "Data Distribution in a Sharded Cluster"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Scalability/#data-partitioning", 
            "text": "As explained previously, MongoDB partition the data based on the sharded key. To partition the data using the sharded key, MongoDB either use range based partitioning or hash based partitioning as explained below:", 
            "title": "Data Partitioning"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Scalability/#range-based-sharding", 
            "text": "MongoDB divides the data into ranges based on the sharded key values. For instance, if we have a numeric sharded key values, we will divid the data based on the possible range of this numeric value. Then MongoDB partitions the data based on the value of the sharded key  and distribute it to the shard responsible for that value range as shown below:   Range based sharding is better for range queries because MongoDB will check which shards are within the requested range. However, range based sharding can result in an uneven distribution which is not good for scalability.", 
            "title": "Range Based Sharding"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Scalability/#hash-based-sharding", 
            "text": "MongoDB supports also Hash Based Sharding which compute a hash value for the sharded key and then use this hash value to select the shard. This ensures better data distribution where data is evenly distributed across the cluster. However it is not efficient when it comes to range queries since the data is randomly distributed across the sharded cluster based on the hash function. An example is shown below:", 
            "title": "Hash Based Sharding"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Security/", 
            "text": "In this section I will talk about what are the security measures that MongoDB provides in order to ensure that the data is securely stored. MongoDB offers many security features such as Authentication, Role-Based access control, and communication encryption. In the following sections, I will talk about these features:\n\n\nAuthentication\n\n\nMongoDB supports a way to verify the identity of clients who are trying to connect to the database. To authenticate the user, the db.auth() method can be used as shown below:\n\n\ndb.auth( \nusername\n, \npassword\n )\n\n\n\n\nThis method will return 1 when authentication is successful or 0 if the authentication failed.\n\n\nRole-Based access control\n\n\nMongoDB supports a Role-Based Access Control (RBAC) that can be used to access the database. Each user can be granted a set of roles that determine the user access permissions to the database resources. To enable the role based access control, you need to use the \"--auth\" option using mongod or setting the \"security.authorization \" to \"enabled\" in the configuration file. \n\n\nEach user can be granted roles which are a set of privileges to perform some actions on specific resources. A resource is either a database, a collection, set of collection or a cluster. The action is the operation that is allowed on a certain resource. The roles are assigned to the user during user creation.\n\n\nCommunication encryption\n\n\nMongoDB supports the use of TLS/SSL protocols for encrypting the connections to mongod or mongos instances.", 
            "title": "Security"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Security/#authentication", 
            "text": "MongoDB supports a way to verify the identity of clients who are trying to connect to the database. To authenticate the user, the db.auth() method can be used as shown below:  db.auth(  username ,  password  )  This method will return 1 when authentication is successful or 0 if the authentication failed.", 
            "title": "Authentication"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Security/#role-based-access-control", 
            "text": "MongoDB supports a Role-Based Access Control (RBAC) that can be used to access the database. Each user can be granted a set of roles that determine the user access permissions to the database resources. To enable the role based access control, you need to use the \"--auth\" option using mongod or setting the \"security.authorization \" to \"enabled\" in the configuration file.   Each user can be granted roles which are a set of privileges to perform some actions on specific resources. A resource is either a database, a collection, set of collection or a cluster. The action is the operation that is allowed on a certain resource. The roles are assigned to the user during user creation.", 
            "title": "Role-Based access control"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Security/#communication-encryption", 
            "text": "MongoDB supports the use of TLS/SSL protocols for encrypting the connections to mongod or mongos instances.", 
            "title": "Communication encryption"
        }, 
        {
            "location": "/MongoDB/Administration and Maintenance/Upgrading/", 
            "text": "In this section I will show how to upgrade MongoDB version 3.0 to the latest version 3.2.  To upgrade a MongoDB instance, you should start by shutting it down using mongod --shutdown option. After that you can either manually upgrading by downloading the new 3.2 binaries and then just replace them with the old 3.0 binaries or you can use the package managers such as apt, dnf, yum, or brew. For instruction on how to install MongoDB 3.2, please have  a look to the \ninstallation section\n. \n\n\nIf you are using more complex deployment such as using sharded clusters or replication. Please have a look to \nMongoDB documentations\n for more detailed instructions.", 
            "title": "Upgrading"
        }, 
        {
            "location": "/MongoDB/Basics/Data Import and Export/", 
            "text": "As explained in the \nQuery Language section\n, MongoDB provides methods to update,remove,replace or insert multiple documents at the same time such as db.collection.insertMany(), db.collection.updateMany(), and db.collection.deleteMany(). In addition, MongoDB supports another method that can be used to execute a batch of write operations such as insert, update and delete at the same time. In the following section, I will explain how to do a bulk write in MongoDB.\n\n\ndb.collection.bulkWrite()\n\n\nThis method provides you with a bulk write capability in a certain collection. You can use this command if you want to run multiple insert,update,replace and delete commands at the same time. These commands can be executed in order or out of order. Executing commands in order means that they will be executed sequentially and when an error occur, it will stop and no further command will be executed while executing commands out of order will just through an error and continue executing other commands. However executing commands in order is slower than out of order specially if you have multiple shards. An example is given below to show how to use this command:\n\n\ntry {\n   db.products.bulkWrite(\n      [\n         { insertOne :\n            {\n               \ndocument\n :\n               {\n                  \n_id\n : 1, \nname\n : \nCool TShirt\n, size\n: \n22\n, \nprice\n : 20\n               }\n            }\n         },\n         { insertOne :\n            {\n               \ndocument\n :\n               {\n                  \n_id\n : 2, \nname\n : \nlaptop\n, \nsize\n : \n125\n, \nprice\n : 230\n               }\n            }\n         },\n         { updateOne :\n            {\n               \nfilter\n : { \nsize\n : \n$lt:20\n },\n               \nupdate\n : { $set : { \ntype\n : \nsmall\n } }\n            }\n         },\n         { deleteOne :\n            { \nfilter\n : { \nname\n : \nCool TShirt\n} }\n         },\n         { replaceOne :\n            {\n               \nfilter\n : { \nname\n : \nLaptop\n },\n               \nreplacement\n : { \nname\n : \nDell Laptop\n, \nsize\n : \n222\n, \nprice\n : 500 }\n            }\n         }\n      ]\n   );\n}\ncatch (e) {\n   print(e);\n}\n\n\n\n\nBy default MongoDB consider the bulk write as ordered unless specified otherwise. So the above example will be executed sequentially and will return a result like below:\n\n\n{\n   \nacknowledged\n : true,\n   \ndeletedCount\n : 1,\n   \ninsertedCount\n : 2,\n   \nmatchedCount\n : 2,\n   \nupsertedCount\n : 0,\n   \ninsertedIds\n : {\n      \n0\n : 1,\n      \n1\n : 2\n   },\n   \nupsertedIds\n : {\n\n   }\n}\n\n\n\n\nYou can also use a write concern which describe the level of acknowledgement request from MongoDB for these write operations. For more details on using the bulk write method, please have a look to \nMongoDB documentations\n. \n\n\nAdditionally, MongoDB supports two useful tools (mongoexport and mongoimport) that can be used to migrate data between different mongodb instances. By using mongoexport and mongoimport, you can easily export or import data in a CSV or BSON format. It is also possible to provide a query so that you can export only a part of the data in a certain collection. Examples are given below:\n\n\nExport data in csv format:\n\n\nmongoexport --db users --collection contacts --type=csv --fields name,address --out /opt/backups/contacts.csv\n\n\n\n\nAs seen above you can specify which fields to export as well as the export path.\n\n\nExport data in BSON format:\n\n\nmongoexport --db sales --collection contacts --out contacts.json\n\n\n\n\nThen to import the data, you can run the import command as shown below:\n\n\nmongoimport --db users --collection contacts --file contacts.json", 
            "title": "Data Import and Export"
        }, 
        {
            "location": "/MongoDB/Basics/Data Import and Export/#dbcollectionbulkwrite", 
            "text": "This method provides you with a bulk write capability in a certain collection. You can use this command if you want to run multiple insert,update,replace and delete commands at the same time. These commands can be executed in order or out of order. Executing commands in order means that they will be executed sequentially and when an error occur, it will stop and no further command will be executed while executing commands out of order will just through an error and continue executing other commands. However executing commands in order is slower than out of order specially if you have multiple shards. An example is given below to show how to use this command:  try {\n   db.products.bulkWrite(\n      [\n         { insertOne :\n            {\n                document  :\n               {\n                   _id  : 1,  name  :  Cool TShirt , size :  22 ,  price  : 20\n               }\n            }\n         },\n         { insertOne :\n            {\n                document  :\n               {\n                   _id  : 2,  name  :  laptop ,  size  :  125 ,  price  : 230\n               }\n            }\n         },\n         { updateOne :\n            {\n                filter  : {  size  :  $lt:20  },\n                update  : { $set : {  type  :  small  } }\n            }\n         },\n         { deleteOne :\n            {  filter  : {  name  :  Cool TShirt } }\n         },\n         { replaceOne :\n            {\n                filter  : {  name  :  Laptop  },\n                replacement  : {  name  :  Dell Laptop ,  size  :  222 ,  price  : 500 }\n            }\n         }\n      ]\n   );\n}\ncatch (e) {\n   print(e);\n}  By default MongoDB consider the bulk write as ordered unless specified otherwise. So the above example will be executed sequentially and will return a result like below:  {\n    acknowledged  : true,\n    deletedCount  : 1,\n    insertedCount  : 2,\n    matchedCount  : 2,\n    upsertedCount  : 0,\n    insertedIds  : {\n       0  : 1,\n       1  : 2\n   },\n    upsertedIds  : {\n\n   }\n}  You can also use a write concern which describe the level of acknowledgement request from MongoDB for these write operations. For more details on using the bulk write method, please have a look to  MongoDB documentations .   Additionally, MongoDB supports two useful tools (mongoexport and mongoimport) that can be used to migrate data between different mongodb instances. By using mongoexport and mongoimport, you can easily export or import data in a CSV or BSON format. It is also possible to provide a query so that you can export only a part of the data in a certain collection. Examples are given below:  Export data in csv format:  mongoexport --db users --collection contacts --type=csv --fields name,address --out /opt/backups/contacts.csv  As seen above you can specify which fields to export as well as the export path.  Export data in BSON format:  mongoexport --db sales --collection contacts --out contacts.json  Then to import the data, you can run the import command as shown below:  mongoimport --db users --collection contacts --file contacts.json", 
            "title": "db.collection.bulkWrite()"
        }, 
        {
            "location": "/MongoDB/Basics/Installation/", 
            "text": "MongoDB can be installed on most platforms. I will explain below how to install it on Windows, Linux Red Hat \n CentOS, and Mac OS X. For the full detailed installation instructions for all other platform, please have a look to \nmongoDB documentations\n.  \n\n\nWindows Systems\n\n\nMongoDB is supported on most of Windows versions except Windows XP. To install MongoDB, first you need to download the latest production version of MongoDB that is suitable for your Windows architecture (32 or 64 bit) from \nMondoDB website\n. \nAfter downloading the MongoDB installation file (a file with .msi extension), double click the file to start the installation. Follow the guided installation screens till the end which should be easy to follow. \nWhen the installation is completed, you can start MongoDB using the following commands assuming you have installed MongoDB in your C driver:\n\n\nFirst create a data directory to store your data:\n\n\nmd \\data\\db\n\n\n\n\nThen start MongoDB server:\n\n\nC:\\mongodb\\bin\\mongod.exe\n\n\n\n\nTo connect to MongoDB command run the below:\n\n\nC:\\mongodb\\bin\\mongo.exe\n\n\n\n\nLinux Red Hat \n CentOS\n\n\nMongoDB can be installed on Linux Red Hat Enterprise or CentOS versions 5, 6, and 7 using the package management system (yum). First we need to configure the yum by creating the below file:\n\n\nvi /etc/yum.repos.d/mongodb-org-3.2.repo\n\n\n\n\nPaste the below inside the file and save it:\n\n\n[mongodb-org-3.2]\nname=MongoDB Repository\nbaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.2/x86_64/\ngpgcheck=0\nenabled=1\n\n\n\n\nFinally install MongoDB using below command:\n\n\nsudo yum install -y mongodb-org\n\n\n\n\nAfter installation is complete, you can run MongoDB easily using below command:\n\n\nsudo service mongod start\n\n\n\n\nHowever you should make sure that you have configured SELinux before running MongoDB. You just need to change the SELINUX settings by editing the file /etc/selinux/config with below two settings:\n\n\nSELINUX=disabled\nSELINUX=permissive\n\n\n\n\nAnd don't forget to give permissions for the folders that will be used by MongoDB \"/var/lib/mongo and /var/log/mongodb\".\n\n\nMac OS X\n\n\nYou can install MongoDB on Mac OS X using either the Homebrew package manager or by building it directly from source. To Install it using Homebrew package manager, please use the below simple commands:\n\n\nbrew update\n\n\n\n\nbrew install mongodb\n\n\n\n\nTo install MongoDB from source, you first need to download the binary files from \nMongoDB download page\n. Then extract the downloaded files using below command:\n\n\ntar -zxvf mongodb-downloaded-files.tgz\n\n\n\n\nFinally export the path of the environment configuration using:\n\n\nexport PATH=\nmongodb-install-directory\n/bin:$PATH\n\n\n\n\nTo run MongoDB, first create a directory to store MongoDB's data and give it the right permissions:\n\n\nmkdir -p /data/db\n\n\n\n\nThen run MongoDB using:\n\n\nmongod", 
            "title": "Installation"
        }, 
        {
            "location": "/MongoDB/Basics/Installation/#windows-systems", 
            "text": "MongoDB is supported on most of Windows versions except Windows XP. To install MongoDB, first you need to download the latest production version of MongoDB that is suitable for your Windows architecture (32 or 64 bit) from  MondoDB website . \nAfter downloading the MongoDB installation file (a file with .msi extension), double click the file to start the installation. Follow the guided installation screens till the end which should be easy to follow. \nWhen the installation is completed, you can start MongoDB using the following commands assuming you have installed MongoDB in your C driver:  First create a data directory to store your data:  md \\data\\db  Then start MongoDB server:  C:\\mongodb\\bin\\mongod.exe  To connect to MongoDB command run the below:  C:\\mongodb\\bin\\mongo.exe", 
            "title": "Windows Systems"
        }, 
        {
            "location": "/MongoDB/Basics/Installation/#linux-red-hat-centos", 
            "text": "MongoDB can be installed on Linux Red Hat Enterprise or CentOS versions 5, 6, and 7 using the package management system (yum). First we need to configure the yum by creating the below file:  vi /etc/yum.repos.d/mongodb-org-3.2.repo  Paste the below inside the file and save it:  [mongodb-org-3.2]\nname=MongoDB Repository\nbaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.2/x86_64/\ngpgcheck=0\nenabled=1  Finally install MongoDB using below command:  sudo yum install -y mongodb-org  After installation is complete, you can run MongoDB easily using below command:  sudo service mongod start  However you should make sure that you have configured SELinux before running MongoDB. You just need to change the SELINUX settings by editing the file /etc/selinux/config with below two settings:  SELINUX=disabled\nSELINUX=permissive  And don't forget to give permissions for the folders that will be used by MongoDB \"/var/lib/mongo and /var/log/mongodb\".", 
            "title": "Linux Red Hat &amp; CentOS"
        }, 
        {
            "location": "/MongoDB/Basics/Installation/#mac-os-x", 
            "text": "You can install MongoDB on Mac OS X using either the Homebrew package manager or by building it directly from source. To Install it using Homebrew package manager, please use the below simple commands:  brew update  brew install mongodb  To install MongoDB from source, you first need to download the binary files from  MongoDB download page . Then extract the downloaded files using below command:  tar -zxvf mongodb-downloaded-files.tgz  Finally export the path of the environment configuration using:  export PATH= mongodb-install-directory /bin:$PATH  To run MongoDB, first create a directory to store MongoDB's data and give it the right permissions:  mkdir -p /data/db  Then run MongoDB using:  mongod", 
            "title": "Mac OS X"
        }, 
        {
            "location": "/MongoDB/Basics/Overview/", 
            "text": "MongoDB is an open source document-based database that is built on an architecture of documents and collections. All the data is stored as documents that comprise sets of key-value pairs. Documents are grouped inside separate collections equivalent to the tables in relational databases. The documents are stored using a json-similar rich format called BSON.  MongoDB provides high performance, availability and scalability. MongoDB uses the document data model which is more flexible than the relational model since it allows for more flexible representation of complex hierarchical relationships using embedded documents.  MongoDB is also designed to scale out by supporting automatic scaling using sharding and replication. MongoDB is also designed to be a general database offering many nice features such as generic secondary indexes, aggregation pipeline, and automatic failover.", 
            "title": "Overview"
        }, 
        {
            "location": "/MongoDB/Basics/Possible Use Cases/", 
            "text": "MongoDB is based on a flexible document model and uses the rich BSON format representation. Additionally it provides high performance, horizontal scalability, high availability and fault tolerance. This makes it suitable for many use cases such as event logging, content management, blogging platforms, real time analytics, or web and ecommerce applications.", 
            "title": "Possible Use Cases"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/", 
            "text": "MongoDB provides normal CRUD operations to interact easily with your stored data. These operations can run only at the collection level. In the following sections I will talk briefly about how to query and modify the stored data.\n\n\nQuery Data\n\n\nYou can easily retrieve the documents stored in a certain collections in MongoDB using the db.collection.find() method. The method accepts a query criteria and projection to return your data in a form of an iterator or a cursor and you can also apply some modifiers such as skips, sorts, and limits. An example is shown below:\n\n\ndb.products.find( \n{size:{$gt: 20}}, // The query\n{ProductName:1, Price: 1, _id: 0 } // The projection \n).limit(10) // A modifier\n\n\n\n\nAs seen in the example above, we want to find all products with size 20 but we want to return only the first 10 documents and only the product name and price. By default MongoDB returns the _id field and can be excluded in the projection as seen above. \n\n\nMongoDB provides many options to query your data and will be covered in more details when talking about \nhow to search your data\n.\n\n\nCreate Data\n\n\nTo create new documents, MongoDB provides three methods as explained below:\n\n\ndb.collection.insertOne()\n\n\nThis method inserts only a single document in a certain collection. It has a simple structure as seen in the below example:\n\n\ndb.products.insertOne( \n{name:\nTShirt\n, size:20 , price: 30 } // The document to be inserted\n)\n\n\n\n\nThe _id will be generated automatically by MongoDB if not specified in the inserted document. \n\n\ndb.collection.insertMany()\n\n\nThis method will insert many documents into the collection. You need just to specify an array of documents to be inserted as seen in the example below:\n\n\ndb.products.insertMany([\n{name:\nTShirt\n, size:20 , price: 30 },\n{name:\nTV\n, size:150 , price: 200 },\n{name:\nPhone\n, size:100 , price: 120 },\n{name:\nLaptop\n, size:170 , price: 500 }\n])\n\n\n\n\ndb.collection.insert()\n\n\nThis is a general method to insert a single document or multiple documents into a certain collection as seen in the example below:\n\n\ndb.products.insert( \n{name:\nTShirt\n, size:20 , price: 30 } \n)\n\n// Or\n\ndb.products.insert([\n{name:\nTShirt\n, size:20 , price: 30 },\n{name:\nTV\n, size:150 , price: 200 },\n{name:\nPhone\n, size:100 , price: 120 },\n{name:\nLaptop\n, size:170 , price: 500 }\n])\n\n\n\n\nThere are other methods that can create documents in MongoDB such as updateOne(), updateMany(), and replaceOne(). These methods will create a new document if it couldn't find a document with the same specified filter and when the \"upsert\" option is set to true.\n\n\nUpdate Data\n\n\nSimilar to creating data in MongoDB, there exists three different methods to update documents in a certain collection as will be explained below:\n\n\ndb.collection.updateOne()\n\n\nThis method will update a single document in a certain collection based on a particular filter and action as seen in the example below:\n\n\ndb.products.updateOne( \n{size: {$lt: 20}},  // This is the filter \n{$set:{type : \nlarge\n } } // The action to be applied \n)\n\n\n\n\nIn the above example, we are updating the first document returned if we apply a filter to get products with size less than 20. Then we update the type field to \"large\". We can also use other actions such as $unset to delete a particular field from a document or $rename to rename the field name which are very useful actions. \n\n\ndb.collection.updateMany()\n\n\nThis method can be used to update multiple documents in a collection at the same time. Example is shown below:\n\n\ndb.products.updateMany( \n{size: {$lt: 20}}, \n{$set:{type : \nlarge\n } } \n)\n\n\n\n\nThis is the same example we used in updateOne method but it will now update all the documents returned by the filter instead of only the first document.\n\n\ndb.collection.update()\n\n\nThis is the general method and can be used to either update a single document or multiple documents inside a certain collection based on a particular filter and action. Both examples we saw previously can be used here but we need to specify whether we want to update a single document or multiple documents using the 'multi' option  as seen in the example below:\n\n\ndb.products.update( \n{size: {$lt: 20}}, \n{$set:{type : \nlarge\n } },\n{multi: false} \n)\n\n// Or\n\ndb.products.update( \n{size: {$lt: 20}}, \n{$set:{type : \nlarge\n } },\n{multi: true} \n)\n\n\n\n\n\nMongoDB also provides two useful commands if you don't want just to update some values in a single document rather you want to replace the whole document. These command are explained below:\n\n\ndb.collection.replaceOne()\n\n\nThis method is used to replace a single document in a collection that was returned based on a particular filter. In this method, you don't need to specify an action but you need to give a new complete document to be replaced with the old one. Example is shown below:\n\n\ndb.products.replaceOne( \n{size: {$lt: 20}}, \n{name:\nTShirt\n, size:20 , price: 30 } // The new document\n)\n\n\n\n\ndb.collection.findOneAndReplace()\n\n\nThis method is similar to the previous one but it offers a sorting of the filtered result which gives you more control offer which document to replace. Example is if you want to replace only the product with the minimum price with size less than 20, you can do that as seen below:\n\n\ndb.products.findOneAndReplace( \n{size: {$lt: 20}}, \n{name:\nTShirt\n, size:20 , price: 30 },\n{ sort: { \nprice\n : 1 } }\n)\n\n\n\n\nDelete Data\n\n\nMongoDB also offers similar methods to delete documents from a particular collection based on a particular filter. I will explain below briefly how to use these methods.\n\n\ndb.collection.deleteOne()\n\n\nThis method is used to delete a single document from the collection based on a certain filter. The first document in the returned results will be deleted. An example is shown below:\n\n\ndb.products.deleteOne( \n{size: {$lt: 20}}\n)\n\n\n\n\nIn the above example we are removing the first document returned if we apply a filter to get all products with size less than 20.\n\n\ndb.collection.deleteMany()\n\n\nThis method can be used to delete multiple documents at the same time from a certain collection and based on a particular filter. The previous example can still apply here, however all the documents with size less than 20 will be deleted:\n\n\ndb.products.deleteMany( \n{size: {$lt: 20}}\n)\n\n\n\n\ndb.collection.remove()\n\n\nThis is the general method used by MongoDB to delete documents from a collection based on a certain filter. You can use it to delete either a single or multiple documents by using the \"justOne\" option which is by default set to false.\n\n\ndb.products.remove( \n{size: {$lt: 20}}, \n{justOne: true} \n)\n\n// Or\n\ndb.products.remove( \n{size: {$lt: 20}}, \n{justOne: false} \n)\n\n\n\n\n\nTo remove all documents in the products collection, you can call the below method:\n\n\ndb.products.remove( { } )\n\n\n\n\nHere the filter is empty set which will return all documents and remove them all.  \n\n\ndb.collection.findOneAndDelete() is also available to find a single document based on a filter and sorting criteria and remove it.\n\n\nAnother general method used by MongoDB is db.collection.save() which can either updates an existing document if the document exists or creates a new document if it doesn't exist.\n\n\nFinally, you can search for a document and modify it in the same round trip atomically by using db.collection.findAndModify. This method is very useful especially in transaction related tasks since it is like you are running a find and modify command atomically at the same time. More details about this method are in \nMongoDB documentation.", 
            "title": "Query Language"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#query-data", 
            "text": "You can easily retrieve the documents stored in a certain collections in MongoDB using the db.collection.find() method. The method accepts a query criteria and projection to return your data in a form of an iterator or a cursor and you can also apply some modifiers such as skips, sorts, and limits. An example is shown below:  db.products.find( \n{size:{$gt: 20}}, // The query\n{ProductName:1, Price: 1, _id: 0 } // The projection \n).limit(10) // A modifier  As seen in the example above, we want to find all products with size 20 but we want to return only the first 10 documents and only the product name and price. By default MongoDB returns the _id field and can be excluded in the projection as seen above.   MongoDB provides many options to query your data and will be covered in more details when talking about  how to search your data .", 
            "title": "Query Data"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#create-data", 
            "text": "To create new documents, MongoDB provides three methods as explained below:", 
            "title": "Create Data"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectioninsertone", 
            "text": "This method inserts only a single document in a certain collection. It has a simple structure as seen in the below example:  db.products.insertOne( \n{name: TShirt , size:20 , price: 30 } // The document to be inserted\n)  The _id will be generated automatically by MongoDB if not specified in the inserted document.", 
            "title": "db.collection.insertOne()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectioninsertmany", 
            "text": "This method will insert many documents into the collection. You need just to specify an array of documents to be inserted as seen in the example below:  db.products.insertMany([\n{name: TShirt , size:20 , price: 30 },\n{name: TV , size:150 , price: 200 },\n{name: Phone , size:100 , price: 120 },\n{name: Laptop , size:170 , price: 500 }\n])", 
            "title": "db.collection.insertMany()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectioninsert", 
            "text": "This is a general method to insert a single document or multiple documents into a certain collection as seen in the example below:  db.products.insert( \n{name: TShirt , size:20 , price: 30 } \n)\n\n// Or\n\ndb.products.insert([\n{name: TShirt , size:20 , price: 30 },\n{name: TV , size:150 , price: 200 },\n{name: Phone , size:100 , price: 120 },\n{name: Laptop , size:170 , price: 500 }\n])  There are other methods that can create documents in MongoDB such as updateOne(), updateMany(), and replaceOne(). These methods will create a new document if it couldn't find a document with the same specified filter and when the \"upsert\" option is set to true.", 
            "title": "db.collection.insert()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#update-data", 
            "text": "Similar to creating data in MongoDB, there exists three different methods to update documents in a certain collection as will be explained below:", 
            "title": "Update Data"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectionupdateone", 
            "text": "This method will update a single document in a certain collection based on a particular filter and action as seen in the example below:  db.products.updateOne( \n{size: {$lt: 20}},  // This is the filter \n{$set:{type :  large  } } // The action to be applied \n)  In the above example, we are updating the first document returned if we apply a filter to get products with size less than 20. Then we update the type field to \"large\". We can also use other actions such as $unset to delete a particular field from a document or $rename to rename the field name which are very useful actions.", 
            "title": "db.collection.updateOne()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectionupdatemany", 
            "text": "This method can be used to update multiple documents in a collection at the same time. Example is shown below:  db.products.updateMany( \n{size: {$lt: 20}}, \n{$set:{type :  large  } } \n)  This is the same example we used in updateOne method but it will now update all the documents returned by the filter instead of only the first document.", 
            "title": "db.collection.updateMany()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectionupdate", 
            "text": "This is the general method and can be used to either update a single document or multiple documents inside a certain collection based on a particular filter and action. Both examples we saw previously can be used here but we need to specify whether we want to update a single document or multiple documents using the 'multi' option  as seen in the example below:  db.products.update( \n{size: {$lt: 20}}, \n{$set:{type :  large  } },\n{multi: false} \n)\n\n// Or\n\ndb.products.update( \n{size: {$lt: 20}}, \n{$set:{type :  large  } },\n{multi: true} \n)  MongoDB also provides two useful commands if you don't want just to update some values in a single document rather you want to replace the whole document. These command are explained below:", 
            "title": "db.collection.update()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectionreplaceone", 
            "text": "This method is used to replace a single document in a collection that was returned based on a particular filter. In this method, you don't need to specify an action but you need to give a new complete document to be replaced with the old one. Example is shown below:  db.products.replaceOne( \n{size: {$lt: 20}}, \n{name: TShirt , size:20 , price: 30 } // The new document\n)", 
            "title": "db.collection.replaceOne()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectionfindoneandreplace", 
            "text": "This method is similar to the previous one but it offers a sorting of the filtered result which gives you more control offer which document to replace. Example is if you want to replace only the product with the minimum price with size less than 20, you can do that as seen below:  db.products.findOneAndReplace( \n{size: {$lt: 20}}, \n{name: TShirt , size:20 , price: 30 },\n{ sort: {  price  : 1 } }\n)", 
            "title": "db.collection.findOneAndReplace()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#delete-data", 
            "text": "MongoDB also offers similar methods to delete documents from a particular collection based on a particular filter. I will explain below briefly how to use these methods.", 
            "title": "Delete Data"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectiondeleteone", 
            "text": "This method is used to delete a single document from the collection based on a certain filter. The first document in the returned results will be deleted. An example is shown below:  db.products.deleteOne( \n{size: {$lt: 20}}\n)  In the above example we are removing the first document returned if we apply a filter to get all products with size less than 20.", 
            "title": "db.collection.deleteOne()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectiondeletemany", 
            "text": "This method can be used to delete multiple documents at the same time from a certain collection and based on a particular filter. The previous example can still apply here, however all the documents with size less than 20 will be deleted:  db.products.deleteMany( \n{size: {$lt: 20}}\n)", 
            "title": "db.collection.deleteMany()"
        }, 
        {
            "location": "/MongoDB/Basics/Query Language/#dbcollectionremove", 
            "text": "This is the general method used by MongoDB to delete documents from a collection based on a certain filter. You can use it to delete either a single or multiple documents by using the \"justOne\" option which is by default set to false.  db.products.remove( \n{size: {$lt: 20}}, \n{justOne: true} \n)\n\n// Or\n\ndb.products.remove( \n{size: {$lt: 20}}, \n{justOne: false} \n)  To remove all documents in the products collection, you can call the below method:  db.products.remove( { } )  Here the filter is empty set which will return all documents and remove them all.    db.collection.findOneAndDelete() is also available to find a single document based on a filter and sorting criteria and remove it.  Another general method used by MongoDB is db.collection.save() which can either updates an existing document if the document exists or creates a new document if it doesn't exist.  Finally, you can search for a document and modify it in the same round trip atomically by using db.collection.findAndModify. This method is very useful especially in transaction related tasks since it is like you are running a find and modify command atomically at the same time. More details about this method are in  MongoDB documentation.", 
            "title": "db.collection.remove()"
        }, 
        {
            "location": "/MongoDB/Basics/Transaction Support/", 
            "text": "On the level of a single document, MongoDB guarantees that the write operation is atomic. This means that if you try to update, insert or delete a single document in MongoDB, the operation will be applied in the form of \"all or nothing\". However if you try to do a write operations for multiple documents, the operations will be applied atomically for each single document alone but the operations as whole may interleave and are not atomic. Hence the transaction is possible using MongoDB if you do it in a single document and not supported if you plan to do it across multiple documents.  \n\n\nAlthough MongoDB supports a way to isolate a write operation if it will be applied across multiple documents using the $isolated option as seen in the below example, this is not always enough to support transaction across multiple documents. \n\n\ndb.products.update(\n    { color : \nRed\n , $isolated : 1 },\n    { $inc : { price : 1 } },\n    { multi: true }\n)\n\n\n\n\nIn the above example, we are increasing the price of all products having the red colour. This method might update multiple documents since we are using the \"multi\" option, however the operation will take a write lock for all affected documents to prevent any concurrent read or write access to them during the operation. Although this will provide an isolation level for the affected documents, but it doesn't execute the operation atomically \"all or nothing\" and hence doesn't totally support a transaction.\n\n\nTo summarised, MongoDB supports transaction well on the level of a single document. However for transactions across multiple documents, transaction  is not internally supported by MongoDB but there is a way to get around this limitation as I will explain in the following sections. \n\n\nTransaction using a single document\n\n\nSince MongoDB supports embedding documents inside other documents, you can model your data in a single document to support atomic operations. To give an example, let's assume you want to check out some product in an e-commerce application. We would first check if there is still amount of this product in the inventory before checking out. If we store all this information in a single document as seen below, then we can ensure an atomic transaction.\n\n\n{\n    sku: SomeSkuID,\n    name: \"TShirt\",\n    size: 20,\n    quantity: 3,\n    checkout: [ { by: \"SomeUserID\", date: ISODate(\"2015-12-15\") } ]\n}\n\n\nAs seen above, we are storing the quantity of this product in the same document and the information about the users who have checked out this product previously.\nA transaction will be atomically executed using the below method:\n\n\ndb.products.update (\n   { sku: SomeSkuID, quantity: { $gt: 0 } },\n   {\n     $inc: { quantity: -1 },\n     $push: { checkout: { by: \"OtherUserID\", date: new Date() } }\n   }\n)\n\n\nThen you can simply check the WriteResult() returned from this operation to check if it has been successful or not.\n\n\nTransaction across multiple documents\n\n\nWe can do a transaction across multiple documents if we use the Two Phases Commits pattern which is very well explained in \nMongoDB documentations.\n\nThe idea behind this pattern is to use state transition for each stage you want to execute. This can be also easily extended to provide rollback-like functionality. A simple example is explained below:\n\n\nTo show an example for how to support transaction across multiple documents in MongoDB, we will show how you can add a product to the cart and modify the inventory accordingly. Assuming we have the below schemas for cart and inventory documents:\n\n\nCart document schema:\n\n\n````\n{\n    cartID: SomeID,\n    items: [\n        { sku: '1', qty: 1, item_details: {...} },\n        { sku: '2', qty: 2, item_details: {...} }\n    ]\n} \n\n\n\n Inventory document schema:\n\n\n  ````\n {\n    sku: 'ProductSKU',\n    qty: 10,\n }\n ````\n\nThen to add a product into the cart, you will add it to the cart and then you decrease the quantity from the inventory if the quantity is available and if not you rollback as seen below:\n\n\n\n\n\n\n\n\nboolean addToCart(String cartID,String sku,int quantity, HashMap\n details)\n{\n   Result result;\n\n\n// Add to cart\n\n\ntry{\n\n\nresult = db.cart.update(\n        {'cartID': cartID },\n        {'$push':{ 'items': {'sku': sku, 'qty':quantity, 'details': details}}});\n\n\n }Catch(Exception e)\n {\n\n raiseNoCartExist();\n return false;\n\n }\n\n// Update the inventory\n\ntry{\n\nresult = db.inventory.update(\n    {'sku':sku, 'qty': {'$gte': quantity}},\n    {'$inc': {'qty': -quantity});\n\n}Catch(Exception e)\n{\n\ndb.cart.update(\n   {'cartID': cartID },\n   { '$pull': { 'items': {'sku': sku } } })\nraiseNoEnoughInventory(); \nreturn false;\n\n}\n\nreturn true;\n\n\n\n}      \n\n ````\n\n\nConcurrency\n\n\nConcurrency control in MongoDB means how to allow multiple applications to access documents concurrently without causing inconsistency or conflicts. This is usually done by creating a unique index on one of the document field so that any concurrent insert or update for this document won't result in a duplicate document.", 
            "title": "Transaction Support"
        }, 
        {
            "location": "/MongoDB/Basics/Transaction Support/#transaction-using-a-single-document", 
            "text": "Since MongoDB supports embedding documents inside other documents, you can model your data in a single document to support atomic operations. To give an example, let's assume you want to check out some product in an e-commerce application. We would first check if there is still amount of this product in the inventory before checking out. If we store all this information in a single document as seen below, then we can ensure an atomic transaction.  {\n    sku: SomeSkuID,\n    name: \"TShirt\",\n    size: 20,\n    quantity: 3,\n    checkout: [ { by: \"SomeUserID\", date: ISODate(\"2015-12-15\") } ]\n}  As seen above, we are storing the quantity of this product in the same document and the information about the users who have checked out this product previously.\nA transaction will be atomically executed using the below method:  db.products.update (\n   { sku: SomeSkuID, quantity: { $gt: 0 } },\n   {\n     $inc: { quantity: -1 },\n     $push: { checkout: { by: \"OtherUserID\", date: new Date() } }\n   }\n)  Then you can simply check the WriteResult() returned from this operation to check if it has been successful or not.", 
            "title": "Transaction using a single document"
        }, 
        {
            "location": "/MongoDB/Basics/Transaction Support/#transaction-across-multiple-documents", 
            "text": "We can do a transaction across multiple documents if we use the Two Phases Commits pattern which is very well explained in  MongoDB documentations. \nThe idea behind this pattern is to use state transition for each stage you want to execute. This can be also easily extended to provide rollback-like functionality. A simple example is explained below:  To show an example for how to support transaction across multiple documents in MongoDB, we will show how you can add a product to the cart and modify the inventory accordingly. Assuming we have the below schemas for cart and inventory documents:  Cart document schema:  ````\n{\n    cartID: SomeID,\n    items: [\n        { sku: '1', qty: 1, item_details: {...} },\n        { sku: '2', qty: 2, item_details: {...} }\n    ]\n}   \n Inventory document schema:\n\n\n  ````\n {\n    sku: 'ProductSKU',\n    qty: 10,\n }\n ````\n\nThen to add a product into the cart, you will add it to the cart and then you decrease the quantity from the inventory if the quantity is available and if not you rollback as seen below:  boolean addToCart(String cartID,String sku,int quantity, HashMap  details)\n{\n   Result result;  // Add to cart  try{  result = db.cart.update(\n        {'cartID': cartID },\n        {'$push':{ 'items': {'sku': sku, 'qty':quantity, 'details': details}}});   }Catch(Exception e)\n {\n\n raiseNoCartExist();\n return false;\n\n }\n\n// Update the inventory\n\ntry{\n\nresult = db.inventory.update(\n    {'sku':sku, 'qty': {'$gte': quantity}},\n    {'$inc': {'qty': -quantity});\n\n}Catch(Exception e)\n{\n\ndb.cart.update(\n   {'cartID': cartID },\n   { '$pull': { 'items': {'sku': sku } } })\nraiseNoEnoughInventory(); \nreturn false;\n\n}\n\nreturn true;  }       \n ````", 
            "title": "Transaction across multiple documents"
        }, 
        {
            "location": "/MongoDB/Basics/Transaction Support/#concurrency", 
            "text": "Concurrency control in MongoDB means how to allow multiple applications to access documents concurrently without causing inconsistency or conflicts. This is usually done by creating a unique index on one of the document field so that any concurrent insert or update for this document won't result in a duplicate document.", 
            "title": "Concurrency"
        }, 
        {
            "location": "/MongoDB/Basics/Underline Structure/", 
            "text": "MongoDB stores its data as documents which is a data structure similar to the structures that maps values to keys such as hashes and dictionaries. These documents contain a JSON like data or what is called a BSON data. Many documents can be grouped into a one container called a collection which is similar to the table concept in the traditional databases. \n\n\nBelow I will explain the three main concepts used in MongoDB which are the documents, collection and the BSON format.\n\n\nDocuments\n\n\nAll the data stored in MongoDB is stored in the form of a document. A document stores the data in a JSON-Like format called BSON which will be explained in more details later. This format stores the data as key-value pairs as seen in the below small example for a product object in an ecommerce application:\n\n\n{\n  sku: \nxxxxx\n,\n  type: \nTShirt\n,\n  title: \nStylish TShirt \n,\n  description: \nby Boss\n,\n\n  shipping: {\n    weight: 6,\n    dimensions: {\n      width: 10,\n      height: 10,\n      depth: 1\n    },\n  },\n\n  pricing: {\n    list: 1200,\n    retail: 1100,\n    savings: 100,\n    pct_savings: 8\n  },\n\n  tags: [\n      \nShirt\n,\n      \nClothes\n,\n      \nSummer\n,\n      \nStylish\n\n    ],\n}\n\n\n\n\nAs you can see above, the document in MongoDB acts as a row in the traditional database. The above document can store values in many data types, for example sku,type and description are of type String. Shipping field is a document type that contains another nested document, and tags field is an array.  However the fields are always of String data type.  \n\n\nThe documents can grow to a very large size but the maximum size is limited to 16 MB. This ensures the document won't grow to a degree that can use excessive amount of memory or excessive amount of bandwidth during transmission. \n\n\nMongoDB creates by default a unique index on the _id of each document. The _id is usually a unique identifier for each document and can be of any BSON data type except for array. \n\n\nYou can access any field in the document using the dot notation as shown in the example below to access the sku of a particular product:\n\n\ndb.products.product.sku\n\n\n\n\nThe \"products\" above is the name of the collection as will be explained in the following section.\n\n\nCollections\n\n\nA collection in MongoDB is an important concept which is used to group multiple documents. If we say that a document in MongoDB is similar to a row in tradition databases then the collection is analog of a table. \n\n\nA collection in MongoDB can contain document with different schema or what is called a dynamic schema support. This means that inside a single collection we can store documents with completely different values as shown in the example below:\n\n\n{sku:\nxxxx\n , productName: \nName\n , price: \n200\n}\n{categoryID: \nxxxx\n , categoryName:\nCategoryName\n}\n\n\n\n\nAs seen above we can store a product document and a category document in the same collection. However it makes sense only to store similar documents in the same collection for the following reasons:\n\n\n\n\nFor grouping and querying reasons, it would be easier to query a collection with a similar structure. For example if you want to query all products that are having the same manufacturer, you just query the \"products\" collection.\n\n\nGrouping similar documents in the same collection allows for data locality.\n\n\nYou can index a collection more efficiently.\n\n\n\n\nYou can use also namespace to define a collection for grouping purposes. For example you define a collection with the name history.orders. This doesn't mean that you can use a sub collection, it is just a collection name and there is no difference between historyOrders or history.orders. \n\n\nBSON Format\n\n\nMongoDB uses a BSON format to represent its documents. BSON is a binary encoded format that extends the well-known JSON model to provide more data types support. BSON also is a lightweight efficient format. It is also very fast and traversable. MongoDB can support embedding objects and arrays just like JSON but can also give access to its inside objects which is used by MongoDB to build indexes and nested keys. BSON can support many data types as can seen in more details in \nMongoDB documentation\n. I will give below a brief description for the common data types used in MongoDB:\n\n\nDouble, String, Boolean, Date and Timestamp\n\n\nMongoDB supports the common data types that are used on most programming languages such double, boolean, string, and Date. The timestamp data type is a special timestamp type used internally by MongoDB which is not associated with the Date type. The timestamp is a 64 bit value where the first 32 bits stores the seconds since the Unix epoch and the second 32 bits stores an incrementing ordinal that are used for operations within a given second. Timestamps are always unique. However the Date data type is a 64-bit integer that contains the number of milliseconds since the Unix epoch.\n\n\nObject, Object id\n\n\nThe object data type stores an embedded document and the object id is a unique key consists of 12-bytes, and the first four bytes are a timestamp that reflects the ObjectId\u2019s creation date.\n\n\nArray and Binary data\n\n\nYou can store an array inside a MongoDB BSON document that can contains other embedded documents or any other data types values. MongoDB supports also storing binary data such as an image or a file inside the BSON document.\n\n\nRegular Expressions\n\n\nThis datatype is used to store regular expression.\n\n\nOther data types:\n\n\nMin/ Max keys : This is used if you want to compute a value against either the min or max BSON fields.\n\n\nJavaScript : This datatype is used if you want to store some javascript code into the BSON document.", 
            "title": "Underline Structure"
        }, 
        {
            "location": "/MongoDB/Basics/Underline Structure/#documents", 
            "text": "All the data stored in MongoDB is stored in the form of a document. A document stores the data in a JSON-Like format called BSON which will be explained in more details later. This format stores the data as key-value pairs as seen in the below small example for a product object in an ecommerce application:  {\n  sku:  xxxxx ,\n  type:  TShirt ,\n  title:  Stylish TShirt  ,\n  description:  by Boss ,\n\n  shipping: {\n    weight: 6,\n    dimensions: {\n      width: 10,\n      height: 10,\n      depth: 1\n    },\n  },\n\n  pricing: {\n    list: 1200,\n    retail: 1100,\n    savings: 100,\n    pct_savings: 8\n  },\n\n  tags: [\n       Shirt ,\n       Clothes ,\n       Summer ,\n       Stylish \n    ],\n}  As you can see above, the document in MongoDB acts as a row in the traditional database. The above document can store values in many data types, for example sku,type and description are of type String. Shipping field is a document type that contains another nested document, and tags field is an array.  However the fields are always of String data type.    The documents can grow to a very large size but the maximum size is limited to 16 MB. This ensures the document won't grow to a degree that can use excessive amount of memory or excessive amount of bandwidth during transmission.   MongoDB creates by default a unique index on the _id of each document. The _id is usually a unique identifier for each document and can be of any BSON data type except for array.   You can access any field in the document using the dot notation as shown in the example below to access the sku of a particular product:  db.products.product.sku  The \"products\" above is the name of the collection as will be explained in the following section.", 
            "title": "Documents"
        }, 
        {
            "location": "/MongoDB/Basics/Underline Structure/#collections", 
            "text": "A collection in MongoDB is an important concept which is used to group multiple documents. If we say that a document in MongoDB is similar to a row in tradition databases then the collection is analog of a table.   A collection in MongoDB can contain document with different schema or what is called a dynamic schema support. This means that inside a single collection we can store documents with completely different values as shown in the example below:  {sku: xxxx  , productName:  Name  , price:  200 }\n{categoryID:  xxxx  , categoryName: CategoryName }  As seen above we can store a product document and a category document in the same collection. However it makes sense only to store similar documents in the same collection for the following reasons:   For grouping and querying reasons, it would be easier to query a collection with a similar structure. For example if you want to query all products that are having the same manufacturer, you just query the \"products\" collection.  Grouping similar documents in the same collection allows for data locality.  You can index a collection more efficiently.   You can use also namespace to define a collection for grouping purposes. For example you define a collection with the name history.orders. This doesn't mean that you can use a sub collection, it is just a collection name and there is no difference between historyOrders or history.orders.", 
            "title": "Collections"
        }, 
        {
            "location": "/MongoDB/Basics/Underline Structure/#bson-format", 
            "text": "MongoDB uses a BSON format to represent its documents. BSON is a binary encoded format that extends the well-known JSON model to provide more data types support. BSON also is a lightweight efficient format. It is also very fast and traversable. MongoDB can support embedding objects and arrays just like JSON but can also give access to its inside objects which is used by MongoDB to build indexes and nested keys. BSON can support many data types as can seen in more details in  MongoDB documentation . I will give below a brief description for the common data types used in MongoDB:", 
            "title": "BSON Format"
        }, 
        {
            "location": "/MongoDB/Basics/Underline Structure/#double-string-boolean-date-and-timestamp", 
            "text": "MongoDB supports the common data types that are used on most programming languages such double, boolean, string, and Date. The timestamp data type is a special timestamp type used internally by MongoDB which is not associated with the Date type. The timestamp is a 64 bit value where the first 32 bits stores the seconds since the Unix epoch and the second 32 bits stores an incrementing ordinal that are used for operations within a given second. Timestamps are always unique. However the Date data type is a 64-bit integer that contains the number of milliseconds since the Unix epoch.", 
            "title": "Double, String, Boolean, Date and Timestamp"
        }, 
        {
            "location": "/MongoDB/Basics/Underline Structure/#object-object-id", 
            "text": "The object data type stores an embedded document and the object id is a unique key consists of 12-bytes, and the first four bytes are a timestamp that reflects the ObjectId\u2019s creation date.", 
            "title": "Object, Object id"
        }, 
        {
            "location": "/MongoDB/Basics/Underline Structure/#array-and-binary-data", 
            "text": "You can store an array inside a MongoDB BSON document that can contains other embedded documents or any other data types values. MongoDB supports also storing binary data such as an image or a file inside the BSON document.", 
            "title": "Array and Binary data"
        }, 
        {
            "location": "/MongoDB/Basics/Underline Structure/#regular-expressions", 
            "text": "This datatype is used to store regular expression.", 
            "title": "Regular Expressions"
        }, 
        {
            "location": "/MongoDB/Basics/Underline Structure/#other-data-types", 
            "text": "Min/ Max keys : This is used if you want to compute a value against either the min or max BSON fields.  JavaScript : This datatype is used if you want to store some javascript code into the BSON document.", 
            "title": "Other data types:"
        }, 
        {
            "location": "/MongoDB/Data Model/Data Layout/", 
            "text": "Unlike relational databases where you should define a table schema before inserting any data, MongoDB is schema-less and its collections don't enforce documents structure. Hence data modelling is more flexible. \n\n\nHow to model your data into MongoDB's documents is defined based on many criteria and depends on your application requirements as well as the underline data structure.  Important criteria are:\n\n\n\n\nWhat are your data access patterns?\n\n\nHow do you query and update your data?\n\n\nHow is your data look like?\n\n\nWhat are your application requirements?\n\n\nWhat are the relationships between your data?\n\n\n\n\nThere are two ways to represent the relationships between your data, either by preferring the normalisation way and modelling your data using references or by accepting some denormalisation of your data and using embedded documents. \n\n\nModelling relations using references\n\n\nYou can store a reference in one document to act like a link to another document, then the client can use this reference to retrieve the other document when needed. Using references can normalise your data and allows for more flexibilities in data modelling.\n\n\nAn example is shown below:\n\n\nCategories:\n\n\n{\n    id: \u201c1\u201d,\n    name: \u201cDell Laptops\u201d,\n    parent: \u201cLaptops\u201d,\n}\n\n\n{\n    id: \u201c2\u201d,\n    name: \u201cComputers \n Laptops\u201d,\n    parent: \u201cElectronics\u201d,\n}\n\n\n\n\nProduct:\n\n\n{\n    sku: \u201cSomeSkuID\u201d,\n    name: \u201cDell XPS\u201d,\n    size: \u201c20\u201d,\n    colour: \nRed\n,\n    cat: [ \n1\n , \n2\n]\n}\n\n\n\n\nAs you can see, we are referencing a product to category using the category ID. \n\n\nWe usually use normalised data model using references as seen above if we want to prevent duplicating our data across documents. For instance, in the above example we are not duplicating the category information in each product inside this category. Instead we store just a reference to the category document and the client can use this reference to retrieve the category information if needed. \n\n\nAnother important benefit of modelling data by references is the flexibility that you will get to model complex relations such as many-to-many relationships or large hierarchical data.\n\n\nHowever the downside of using references is that the client side application needs to do a follow-up queries to resolve these references which might impact performance or increase the client side code and complexity.\n\n\nModelling relations using embedded documents\n\n\nSince MongoDB allows you to embed related data in a single document, you can design your model using the denormalised model pattern. Using this pattern you can take full advantage of MongoDB's rich documents. Instead of using references to other related documents, you can embed these documents in a single document so that the client need to issue fewer queries and updates. \n\n\nAn example is shown below:\n\n\n{\nsku: \u201cSomeSkuID\u201d,\nname: \u201cDell XPS\u201d,\nsize: \u201c20\u201d,\ncolour: \nRed\n,\ncat: [\n{\nid: \u201c1\u201d,\nname: \u201cDell Laptops\u201d,\nparent: \u201cLaptops\u201d,\n}\n,\n{\nid: \u201c2\u201d,\nname: \u201cComputers \n Laptops\u201d,\nparent: \u201cElectronics\u201d,\n} ]\n}\n\n\n\n\n\nIt is usually a good idea to store one-to-one relationships as embedded documents as seen in the example above. Also if you have a one-to-many relationship, you can store the \"many\" documents inside the \"one\" document as an array for embedded documents. \n\n\nIn general, modelling relations using embedded documents will give a better read performance and you can retrieve related data only in a single read operation. \n\n\nAnother important benefit is that you can update related data in a single atomic write operation which can be used to support transaction related tasks. \n\n\nHowever, a downside of using embedded document is when your document grows and becomes very large. This will cause performance issue in the storage engine such as MMAPv1 and might lead to data fragmentation. Because of this, it is not a good idea to use embedded documents when you think that these embedded documents can grow to an unbounded limit since this might cause problems in the future when your data becomes larger.", 
            "title": "Data Layout"
        }, 
        {
            "location": "/MongoDB/Data Model/Data Layout/#modelling-relations-using-references", 
            "text": "You can store a reference in one document to act like a link to another document, then the client can use this reference to retrieve the other document when needed. Using references can normalise your data and allows for more flexibilities in data modelling.  An example is shown below:  Categories:  {\n    id: \u201c1\u201d,\n    name: \u201cDell Laptops\u201d,\n    parent: \u201cLaptops\u201d,\n}\n\n\n{\n    id: \u201c2\u201d,\n    name: \u201cComputers   Laptops\u201d,\n    parent: \u201cElectronics\u201d,\n}  Product:  {\n    sku: \u201cSomeSkuID\u201d,\n    name: \u201cDell XPS\u201d,\n    size: \u201c20\u201d,\n    colour:  Red ,\n    cat: [  1  ,  2 ]\n}  As you can see, we are referencing a product to category using the category ID.   We usually use normalised data model using references as seen above if we want to prevent duplicating our data across documents. For instance, in the above example we are not duplicating the category information in each product inside this category. Instead we store just a reference to the category document and the client can use this reference to retrieve the category information if needed.   Another important benefit of modelling data by references is the flexibility that you will get to model complex relations such as many-to-many relationships or large hierarchical data.  However the downside of using references is that the client side application needs to do a follow-up queries to resolve these references which might impact performance or increase the client side code and complexity.", 
            "title": "Modelling relations using references"
        }, 
        {
            "location": "/MongoDB/Data Model/Data Layout/#modelling-relations-using-embedded-documents", 
            "text": "Since MongoDB allows you to embed related data in a single document, you can design your model using the denormalised model pattern. Using this pattern you can take full advantage of MongoDB's rich documents. Instead of using references to other related documents, you can embed these documents in a single document so that the client need to issue fewer queries and updates.   An example is shown below:  {\nsku: \u201cSomeSkuID\u201d,\nname: \u201cDell XPS\u201d,\nsize: \u201c20\u201d,\ncolour:  Red ,\ncat: [\n{\nid: \u201c1\u201d,\nname: \u201cDell Laptops\u201d,\nparent: \u201cLaptops\u201d,\n}\n,\n{\nid: \u201c2\u201d,\nname: \u201cComputers   Laptops\u201d,\nparent: \u201cElectronics\u201d,\n} ]\n}  It is usually a good idea to store one-to-one relationships as embedded documents as seen in the example above. Also if you have a one-to-many relationship, you can store the \"many\" documents inside the \"one\" document as an array for embedded documents.   In general, modelling relations using embedded documents will give a better read performance and you can retrieve related data only in a single read operation.   Another important benefit is that you can update related data in a single atomic write operation which can be used to support transaction related tasks.   However, a downside of using embedded document is when your document grows and becomes very large. This will cause performance issue in the storage engine such as MMAPv1 and might lead to data fragmentation. Because of this, it is not a good idea to use embedded documents when you think that these embedded documents can grow to an unbounded limit since this might cause problems in the future when your data becomes larger.", 
            "title": "Modelling relations using embedded documents"
        }, 
        {
            "location": "/MongoDB/Data Model/Nested Data Structures/", 
            "text": "MongoDB supports very well nested structures by allowing documents to embed single or multiple documents. This feature allows for great flexibility during data modelling.", 
            "title": "Nested Data Structures"
        }, 
        {
            "location": "/MongoDB/Data Model/Normalization or Denormalization/", 
            "text": "Traditional databases always try to optimise data using normalisation to achieve the most possible storage efficiency. However, most of NoSQL data stores don't think that storage is the most expensive component in nowadays systems. For instance, MongoDB encourage denormalising the data by using the rich document structure it supports. Denormalising data in MongoDB can reduce development complexity and speed time to market as well as increasing performance. In the other hand, MongoDB supports normalised or denormalised models when representing relationships between your data as have been already explained in previous sections.", 
            "title": "Normalization or Denormalization"
        }, 
        {
            "location": "/MongoDB/Data Model/Referential Integrity/", 
            "text": "There is no referential integrity enforced by MongoDB.", 
            "title": "Referential Integrity"
        }, 
        {
            "location": "/MongoDB/Data Model/Relational Data Support/", 
            "text": "In this section, I will show how you can use MongoDB to model common relations between you data such as one-to-one, one-to-many, and many-to-many relations. For each relation, we can use either the normalised model or the denormalised model. I have explained in the \nprevious section\n the difference between the two models and the benefits and shortcomings for each.\n\n\nOne-To-One Relationship\n\n\nUsually, it is a good idea to model such relation using embedded documents. An example is if you want to model the relationship between product and product details models. Each product has one product details. Assuming we are modelling them using references, they will look like this:\n\n\nProduct:\n\n\n{\n    sku: \u201cSomeSkuID\u201d\n    name: \u201cDell XPS\u201d\n}\n\n\n\n\nProductDetails:\n\n\n{\n    productSku: \u201cSomeSkuID\u201d,\n    size: \u201c20\u201d,\n    colour: \nRed\n\n}\n\n\n\n\nHowever if you model it using the embedded document way, you can do that as shown below:\n\n\nProduct:\n\n\n{\n    sku: \u201cSomeSkuID\u201d\n    name: \u201cDell XPS\u201d\n    productDetails:\n    {\n    productSku: \u201cSomeSkuID\u201d,\n    size: \u201c20\u201d,\n    colour: \nRed\n\n    }\n}\n\n\n\n\nOne-To-Many Relationship\n\n\nAn example of a one to many relationship, is the relationship between customer and his orders. Each customer can have many addresses but each address is mapped only to one customer. To map this relationship using references, we can do that as shown below:\n\n\nCustomer:\n\n\n{\n    id: \nSomeCustomerID\n\n    firstName: \u201cJohn\u201d,\n    lastName: \u201cRobert\u201d,\n    phoneNumber: \u201c121212\u201d,\n    email: \nemail@yahoo.com\n\n}\n\n\n\n\nAddress:\n\n\n{\n    id: \nSomeAddressID\n\n    street: \u201cstreet 1\u201d,\n    city: \u201cMunich\u201d,\n    country: \nGermany\n,\n    postCode: \n212\n,\n    customerID: \nSomeCustomerID\n\n}\n\n{\n    id: \nSomeOtherAddressID\n\n    street: \u201cstreet 2\u201d,\n    city: \u201cMunich\u201d,\n    country: \nGermany\n,\n    postCode: \n3333\n,\n    customerID: \nSomeCustomerID\n\n}\n\n\n\n\nIf we want to model this relation using the denormalised model. We will do that by embedding the addresses inside the customer documents as shown below:\n\n\n{\n    id: \nSomeCustomerID\n\n    firstName: \u201cJohn\u201d,\n    lastName: \u201cRobert\u201d,\n    phoneNumber: \u201c121212\u201d,\n    email: \nemail@yahoo.com\n\n    addresses: [\n    {\n    id: \nSomeAddressID\n\n    street: \u201cstreet 1\u201d,\n    city: \u201cMunich\u201d,\n    country: \nGermany\n,\n    postCode: \n212\n\n    },\n\n    {\n    id: \nSomeOtherAddressID\n\n    street: \u201cstreet 2\u201d,\n    city: \u201cMunich\u201d,\n    country: \nGermany\n,\n    postCode: \n3333\n\n    }   ]\n}\n\n\n\n\nMany-To-Many Relationship\n\n\nIn general, it would be a better idea to use the normalised model using references in order to model many-to-many relationship to prevent data redundancy. Below I will show how it will be if we model this relationship using both normalised and denormalised models.\n\n\nAn example, if you want to model the many-to-many relationship between products and categories. Each product can be in more than one category and each category can have more than one product. If we use the normalised model to define this relationship using references, we can do something like this:\n\n\nCategories:\n\n\n{\n    id: \u201c1\u201d,\n    name: \u201cDell Laptops\u201d,\n    parent: \u201cLaptops\u201d,\n}\n\n\n{\n    id: \u201c2\u201d,\n    name: \u201cComputers \n Laptops\u201d,\n    parent: \u201cElectronics\u201d,\n}\n\n\n\n\nProduct:\n\n\n{\n    sku: \u201cSomeSkuID\u201d,\n    name: \u201cDell XPS\u201d,\n    size: \u201c20\u201d,\n    colour: \nRed\n,\n    cat: [ \n1\n , \n2\n]\n}\n\n\n\n\nHowever if you want to embed the categories data inside each products as seen below, the category information will be unnecessary duplicated in each product. \n\n\n{\n    sku: \u201cSomeSkuID\u201d,\n    name: \u201cDell XPS\u201d,\n    size: \u201c20\u201d,\n    colour: \nRed\n,\n    cat: [\n    {\n    id: \u201c1\u201d,\n    name: \u201cDell Laptops\u201d,\n    parent: \u201cComputers \n Laptops\u201d,\n     } , \n     {\n    id: \u201c2\u201d,\n    name: \u201cComputers \n Laptops\u201d,\n    parent: \u201cElectronics\u201d,\n     } ]\n}\n\n\n{\n    sku: \u201cSomeOtherSkuID\u201d,\n    name: \u201cIBM lenovo\u201d,\n    size: \u201c20\u201d,\n    colour: \nRed\n,\n    cat: [\n    {\n    id: \u201c3\u201d,\n    name: \u201cIBM Laptops\u201d,\n    parent: \u201cComputers \n Laptops\u201d,\n     } , \n     {\n    id: \u201c2\u201d,\n    name: \u201cComputers \n Laptops\u201d,\n    parent: \u201cElectronics\u201d,\n     } ]\n}\n\n\n\n\nAs seen above the category with id 2 was repeated twice in both products.", 
            "title": "Relational Data Support"
        }, 
        {
            "location": "/MongoDB/Data Model/Relational Data Support/#one-to-one-relationship", 
            "text": "Usually, it is a good idea to model such relation using embedded documents. An example is if you want to model the relationship between product and product details models. Each product has one product details. Assuming we are modelling them using references, they will look like this:  Product:  {\n    sku: \u201cSomeSkuID\u201d\n    name: \u201cDell XPS\u201d\n}  ProductDetails:  {\n    productSku: \u201cSomeSkuID\u201d,\n    size: \u201c20\u201d,\n    colour:  Red \n}  However if you model it using the embedded document way, you can do that as shown below:  Product:  {\n    sku: \u201cSomeSkuID\u201d\n    name: \u201cDell XPS\u201d\n    productDetails:\n    {\n    productSku: \u201cSomeSkuID\u201d,\n    size: \u201c20\u201d,\n    colour:  Red \n    }\n}", 
            "title": "One-To-One Relationship"
        }, 
        {
            "location": "/MongoDB/Data Model/Relational Data Support/#one-to-many-relationship", 
            "text": "An example of a one to many relationship, is the relationship between customer and his orders. Each customer can have many addresses but each address is mapped only to one customer. To map this relationship using references, we can do that as shown below:  Customer:  {\n    id:  SomeCustomerID \n    firstName: \u201cJohn\u201d,\n    lastName: \u201cRobert\u201d,\n    phoneNumber: \u201c121212\u201d,\n    email:  email@yahoo.com \n}  Address:  {\n    id:  SomeAddressID \n    street: \u201cstreet 1\u201d,\n    city: \u201cMunich\u201d,\n    country:  Germany ,\n    postCode:  212 ,\n    customerID:  SomeCustomerID \n}\n\n{\n    id:  SomeOtherAddressID \n    street: \u201cstreet 2\u201d,\n    city: \u201cMunich\u201d,\n    country:  Germany ,\n    postCode:  3333 ,\n    customerID:  SomeCustomerID \n}  If we want to model this relation using the denormalised model. We will do that by embedding the addresses inside the customer documents as shown below:  {\n    id:  SomeCustomerID \n    firstName: \u201cJohn\u201d,\n    lastName: \u201cRobert\u201d,\n    phoneNumber: \u201c121212\u201d,\n    email:  email@yahoo.com \n    addresses: [\n    {\n    id:  SomeAddressID \n    street: \u201cstreet 1\u201d,\n    city: \u201cMunich\u201d,\n    country:  Germany ,\n    postCode:  212 \n    },\n\n    {\n    id:  SomeOtherAddressID \n    street: \u201cstreet 2\u201d,\n    city: \u201cMunich\u201d,\n    country:  Germany ,\n    postCode:  3333 \n    }   ]\n}", 
            "title": "One-To-Many Relationship"
        }, 
        {
            "location": "/MongoDB/Data Model/Relational Data Support/#many-to-many-relationship", 
            "text": "In general, it would be a better idea to use the normalised model using references in order to model many-to-many relationship to prevent data redundancy. Below I will show how it will be if we model this relationship using both normalised and denormalised models.  An example, if you want to model the many-to-many relationship between products and categories. Each product can be in more than one category and each category can have more than one product. If we use the normalised model to define this relationship using references, we can do something like this:  Categories:  {\n    id: \u201c1\u201d,\n    name: \u201cDell Laptops\u201d,\n    parent: \u201cLaptops\u201d,\n}\n\n\n{\n    id: \u201c2\u201d,\n    name: \u201cComputers   Laptops\u201d,\n    parent: \u201cElectronics\u201d,\n}  Product:  {\n    sku: \u201cSomeSkuID\u201d,\n    name: \u201cDell XPS\u201d,\n    size: \u201c20\u201d,\n    colour:  Red ,\n    cat: [  1  ,  2 ]\n}  However if you want to embed the categories data inside each products as seen below, the category information will be unnecessary duplicated in each product.   {\n    sku: \u201cSomeSkuID\u201d,\n    name: \u201cDell XPS\u201d,\n    size: \u201c20\u201d,\n    colour:  Red ,\n    cat: [\n    {\n    id: \u201c1\u201d,\n    name: \u201cDell Laptops\u201d,\n    parent: \u201cComputers   Laptops\u201d,\n     } , \n     {\n    id: \u201c2\u201d,\n    name: \u201cComputers   Laptops\u201d,\n    parent: \u201cElectronics\u201d,\n     } ]\n}\n\n\n{\n    sku: \u201cSomeOtherSkuID\u201d,\n    name: \u201cIBM lenovo\u201d,\n    size: \u201c20\u201d,\n    colour:  Red ,\n    cat: [\n    {\n    id: \u201c3\u201d,\n    name: \u201cIBM Laptops\u201d,\n    parent: \u201cComputers   Laptops\u201d,\n     } , \n     {\n    id: \u201c2\u201d,\n    name: \u201cComputers   Laptops\u201d,\n    parent: \u201cElectronics\u201d,\n     } ]\n}  As seen above the category with id 2 was repeated twice in both products.", 
            "title": "Many-To-Many Relationship"
        }, 
        {
            "location": "/MongoDB/Examples/Denormalised Model/", 
            "text": "In this model, we are modelling the TPC-H data model in one single document called order. The order will have all lineitems documents embedded inside it as shown below:\n\n\n\n\nOrder document:\n\n\n {\n    \n_id\n: \nObjectID\n,\n    \norderstatus\n: \nString\n,\n    \ntotalprice\n: \nDouble\n,\n    \norderdate\n: \nDate\n,\n    \norderpriority\n: \nString\n,\n    \nclerk\n: \nString\n,\n    \nshippriority\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \ncustomer\n: \nObject\nCustomer\n,\n    \nLineItems\n: \nArray\nlineitem\n\n}\n\n\n\n\nAs shown above, the order will embed the customer as well all the lineitem documents. The customer and document will embed the nation document which will embed the region document as shown below:\n\n\nCustomer document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \naddress\n: \nString\n,\n    \nphone\n: \nDouble\n,\n    \nacctbal\n: \nDouble\n,\n    \nmktsegment\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \nnation\n: \nObject\nNation\n\n}\n\n\n\n\nNation document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \nregion\n: \nObject\nregion\n\n}\n\n\n\n\nRegion document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \ncomment\n: \nString\n\n}\n\n\n\n\nThe order document will embed all the lineitem documents which have the below structure:\n\n\nLineitem document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nquantity\n: \nDouble\n,\n    \nextende dprice\n: \nDouble\n,\n    \ndiscount\n: \nDouble\n,\n    \ntax\n: \nDouble\n,\n    \nreturnflag\n: \nBoolean\n,\n    \nlinestatus\n: \nString\n,\n    \nshipdate\n: \nDate\n,\n    \ncommitdate\n: \nDate\n,\n    \nreceiptdate\n: \nDate\n,\n    \nshipinstruct\n: \nString\n,\n    \nshipmode\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \npartsupp\n: \nObject\npartsupp\n\n\n}\n\n\n\n\nAs seen above, the the lineitem document is embedding the partsupp document which has the below structure:\n\n\nPartsupp document :\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \navailqty\n: \nDouble\n,\n    \nsupplycost\n: \nDouble\n,\n    \ncomment\n: \nString\n,\n    \npart\n: \nObject\npart\n,\n    \nsupplier\n: \nObject\nsupplier\n\n}\n\n\n\n\nThe partsupp document is embedding the part and supplier documents which are shown below:\n\n\nPart document:\n\n\n{\n    \n_id\n: \nString\n,\n    \nname\n: \nString\n,\n    \nmfgr\n: \nString\n,\n    \nbrand\n: \nString\n,\n    \ntype\n: \nString\n,\n    \nsize\n: \nDouble\n,\n    \ncontainer\n: \nString\n,\n    \nretailprice\n: \nDouble\n,\n    \ncomment\n: \nString\n\n}\n\n\n\n\nSupplier document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \naddress\n: \nString\n,\n    \nphone\n: \nDouble\n,\n    \nacctbal\n: \nDouble\n,\n    \ncomment\n: \nString\n,\n    \nnation\n: \nObject\nnation\n\n}\n\n\n\n\nAs seen above, we will store the complete data model in one document called order. All relationships between the different objects in the data model are represented with embedded documents. A complete order document will be as shown below as an example:\n\n\njson\n{\n    \"_id\": \"7821ef4d-8e8c-46e0-950d-83c245b9bee8\",\n    \"orderstatus\": \"Open\",\n    \"totalprice\": \"233\",\n    \"orderdate\": \"2015-12-21 10:51:25\",\n    \"orderpriority\": \"High\",\n    \"clerk\": \"John\",\n    \"shippriority\": \"High\",\n    \"comment\": \"This is an order\",\n    \"customer\": {\n        \"_id\": \"f4005128-a7d0-11e5-bf7f-feff819cdc9f\",\n        \"name\": \"Bilal\",\n        \"address\": \"Street 1\",\n        \"phone\": \"1223456\",\n        \"acctbal\": \"212\",\n        \"mktsegment\": \"some text\",\n        \"comment\": \"some text\",\n        \"nation\": {\n            \"_id\": \"0aa770e6-a7d1-11e5-bf7f-feff819cdc9f\",\n            \"name\": \"US\",\n            \"comment\": \"some text\",\n            \"region\": {\n                \"_id\": \"18f228bc-a7d1-11e5-bf7f-feff819cdc9f\",\n                \"name\": \"Texas\",\n                \"comment\": \"some text\"\n            }\n        }\n    },\n    \"LineItems\": [{\n        \"_id\": \"2ddbd282-a7d1-11e5-bf7f-feff819cdc9f\",\n        \"quantity\": \"1\",\n        \"extende dprice\": \"200\",\n        \"discount\": \"20\",\n        \"tax\": \"2\",\n        \"returnflag\": \"false\",\n        \"linestatus\": \"available\",\n        \"shipdate\": \"2015-12-21 10:51:25\",\n        \"commitdate\": \"2015-12-21 10:51:25\",\n        \"receiptdate\": \"2015-12-21 10:51:25\",\n        \"shipinstruct\": \"some text\",\n        \"shipmode\": \"DHL\",\n        \"comment\": \"some text\",\n        \"partsupp\": {\n            \"_id\": \"62353e7e-a7d1-11e5-bf7f-feff819cdc9f\",\n            \"availqty\": \"20\",\n            \"supplycost\": \"220\",\n            \"comment\": \"some text\",\n            \"part\": {\n                \"_id\": \"75f28eda-a7d1-11e5-bf7f-feff819cdc9f\",\n                \"name\": \"Tshirt\",\n                \"mfgr\": \"Boss\",\n                \"brand\": \"Boss\",\n                \"type\": \"sport\",\n                \"size\": \"40\",\n                \"container\": \"some text\",\n                \"retailprice\": \"230\",\n                \"comment\": \"some text\"\n            },\n            \"supplier\": {\n                \"_id\": \"968a0d3a-a7d1-11e5-bf7f-feff819cdc9f\",\n                \"name\": \"Boss Supplier\",\n                \"address\": \"street 2\",\n                \"phone\": \"212323\",\n                \"acctbal\": \"2933\",\n                \"comment\": \"some text\",\n                \"nation\": {\n                    \"_id\": \"0aa770e6-a7d1-11e5-bf7f-feff819cdc9f\",\n                    \"name\": \"US\",\n                    \"comment\": \"some text\",\n                    \"region\": {\n                        \"_id\": \"18f228bc-a7d1-11e5-bf7f-feff819cdc9f\",\n                        \"name\": \"Texas\",\n                        \"comment\": \"some text\"\n                    }\n                }\n            }\n        }\n    }]\n }\n\n\nSince we have modelled our data in one single document, then we will just easily run queries against one mongoDB collection \"the order collection\".\n\n\nIn the following sections, we will show how to represent three complex TPC-H sql queries in MongoDB using this data model.\n\n\nPricing Summary Report Query (Q1)\n\n\nThis query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:\n\n\nselect\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate \n= date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;\n\n\n\n\nTo execute the above query in MongoDB, we will use the pipeline aggregation framework. We will divide the above query to different stages then execute them at once. The stages that we will need are a \"match\" stage, an \"unwind\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that are having the lineitem shipdate less than or equal some date value or the sql query part shown below:\n\n\nwhere\n   l_shipdate \n= date '1998-12-01' - interval '[DELTA]' day (3)\n\n\n\n\nThe equivalent match stage is shown below:\n\n\n{  \n   \n$match\n:{  \n      \nItems\n:{  \n         \n$elemMatch\n:{  \n            \nSHIPDATE\n:{  \n               \n$lte\n:ISODate(\n2000-01-01T00:00:00.000Z\n)\n            }\n         }\n      }\n   }\n}\n\n\n\n\nThen since the returned documents will contain an array of lineitem documents, we need to unwind these document so that they can be processed by the other stages. The unwind stage query is shown below:\n\n\n{  \n   \n$unwind\n:\n$Items\n\n}\n\n\n\n\nAfter unwinding the lineitem documents, we can run the project stage which will return only the attributes that we need as per the original sql query. The project stage query is shown below:\n\n\n{  \n   \n$project\n:{  \n      \nItems.RETURNFLAG\n:1,\n      \nItems.LINESTATUS\n:1,\n      \nItems.QUANTITY\n:1,\n      \nItems.EXTENDEDPRICE\n:1,\n      \nItems.DISCOUNT\n:1,\n      \nl_dis_min_1\n:{  \n         \n$subtract\n:[  \n            1,\n            \n$Items.DISCOUNT\n\n         ]\n      },\n      \nl_tax_plus_1\n:{  \n         \n$add\n:[  \n            \n$Items.TAX\n,\n            1\n         ]\n      }\n   }\n}\n\n\n\n\nThe above stage will return the attributes that we need to use in the following stages. We can also perform any required mathematical operations on any of the attributes.  Then we can perform the group stage which will group the results by certain attributes as shown below:\n\n\n{  \n   \n$group\n:{  \n      \n_id\n:{  \n         \nRETURNFLAG\n:\n$Items.RETURNFLAG\n,\n         \nLINESTATUS\n:\n$Items.LINESTATUS\n\n      },\n      \nsum_qty\n:{  \n         \n$sum\n:\n$Items.QUANTITY\n\n      },\n      \nsum_base_price\n:{  \n         \n$sum\n:\n$Items.EXTENDEDPRICE\n\n      },\n      \nsum_disc_price\n:{  \n         \n$sum\n:{  \n            \n$multiply\n:[  \n               \n$Items.EXTENDEDPRICE\n,\n               \n$l_dis_min_1\n\n            ]\n         }\n      },\n      \nsum_charge\n:{  \n         \n$sum\n:{  \n            \n$multiply\n:[  \n               \n$Items.EXTENDEDPRICE\n,\n               {  \n                  \n$multiply\n:[  \n                     \n$l_tax_plus_1\n,\n                     \n$l_dis_min_1\n\n                  ]\n               }\n            ]\n         }\n      },\n      \navg_price\n:{  \n         \n$avg\n:\n$Items.EXTENDEDPRICE\n\n      },\n      \navg_disc\n:{  \n         \n$avg\n:\n$Items.DISCOUNT\n\n      },\n      \ncount_order\n:{  \n         \n$sum\n:1\n      }\n   }\n}\n\n\n\n\nFinally we sort the results using the sort stage as shown below:\n\n\n{  \n   \n$sort\n:{  \n      \nItems.RETURNFLAG\n:1,\n      \nItems.LINESTATUS\n:1\n   }\n}\n\n\n\n\nThe complete query in mongoDB is shown below:\n\n\n[  \n   {  \n      \n$match\n:{  \n         \nItems\n:{  \n            \n$elemMatch\n:{  \n               \nSHIPDATE\n:{  \n                  \n$lte\n: ISODate(\n2000-01-01T00:00:00.000Z\n)\n               }\n            }\n         }\n      }\n   },\n   {  \n      \n$unwind\n:\n$Items\n\n   },\n   {  \n      \n$project\n:{  \n         \nItems.RETURNFLAG\n:1,\n         \nItems.LINESTATUS\n:1,\n         \nItems.QUANTITY\n:1,\n         \nItems.EXTENDEDPRICE\n:1,\n         \nItems.DISCOUNT\n:1,\n         \nl_dis_min_1\n:{  \n            \n$subtract\n:[  \n               1,\n               \n$Items.DISCOUNT\n\n            ]\n         },\n         \nl_tax_plus_1\n:{  \n            \n$add\n:[  \n               \n$Items.TAX\n,\n               1\n            ]\n         }\n      }\n   },\n   {  \n      \n$group\n:{  \n         \n_id\n:{  \n            \nRETURNFLAG\n:\n$Items.RETURNFLAG\n,\n            \nLINESTATUS\n:\n$Items.LINESTATUS\n\n         },\n         \nsum_qty\n:{  \n            \n$sum\n:\n$Items.QUANTITY\n\n         },\n         \nsum_base_price\n:{  \n            \n$sum\n:\n$Items.EXTENDEDPRICE\n\n         },\n         \nsum_disc_price\n:{  \n            \n$sum\n:{  \n               \n$multiply\n:[  \n                  \n$Items.EXTENDEDPRICE\n,\n                  \n$l_dis_min_1\n\n               ]\n            }\n         },\n         \nsum_charge\n:{  \n            \n$sum\n:{  \n               \n$multiply\n:[  \n                  \n$Items.EXTENDEDPRICE\n,\n                  {  \n                     \n$multiply\n:[  \n                        \n$l_tax_plus_1\n,\n                        \n$l_dis_min_1\n\n                     ]\n                  }\n               ]\n            }\n         },\n         \navg_price\n:{  \n            \n$avg\n:\n$Items.EXTENDEDPRICE\n\n         },\n         \navg_disc\n:{  \n            \n$avg\n:\n$Items.DISCOUNT\n\n         },\n         \ncount_order\n:{  \n            \n$sum\n:1\n         }\n      }\n   },\n   {  \n      \n$sort\n:{  \n         \nItems.RETURNFLAG\n:1,\n         \nItems.LINESTATUS\n:1\n      }\n   }\n]\n\n\n\n\nThe complete java code for the api that will return the results of TPCH Q1 is shown below:\n\n\n    @GET\n    @Path(\n/q1\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q1 using this model\n, notes = \nReturns mongoDB document(s)\n, response = Document.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public ArrayList\nDocument\n getQ1Results() {\n\n        AggregateIterable\nDocument\n result;\n\n        try {\n\n            String matchStringQuery = \n{\\\n$match\\\n:{\\\nItems\\\n:{\\\n$elemMatch\\\n:{\\\nSHIPDATE\\\n:{\\\n$lte\\\n:ISODate(\\\n2000-01-01T00:00:00.000Z\\\n)}}}}}\n;\n\n            String unWindStringQuery = \n{$unwind: \\\n$Items\\\n}\n;\n\n            String projectStringQuery = \n{\\\n$project\\\n:{\\\nItems.RETURNFLAG\\\n:1,\\\nItems.LINESTATUS\\\n:1,\\\nItems.QUANTITY\\\n:1,\\\nItems.EXTENDEDPRICE\\\n:1,\\\nItems.DISCOUNT\\\n:1,\\\nl_dis_min_1\\\n:{\\\n$subtract\\\n:[1,\\\n$Items.DISCOUNT\\\n]},\\\nl_tax_plus_1\\\n:{\\\n$add\\\n:[\\\n$Items.TAX\\\n,1]}}}\n;\n\n            String groupStringQuery = \n{\\\n$group\\\n:{\\\n_id\\\n:{\\\nRETURNFLAG\\\n:\\\n$Items.RETURNFLAG\\\n,\\\nLINESTATUS\\\n:\\\n$Items.LINESTATUS\\\n},\\\nsum_qty\\\n:{\\\n$sum\\\n:\\\n$Items.QUANTITY\\\n},\\\nsum_base_price\\\n:{\\\n$sum\\\n:\\\n$Items.EXTENDEDPRICE\\\n},\\\nsum_disc_price\\\n:{\\\n$sum\\\n:{\\\n$multiply\\\n:[\\\n$Items.EXTENDEDPRICE\\\n,\\\n$l_dis_min_1\\\n]}},\\\nsum_charge\\\n:{\\\n$sum\\\n:{\\\n$multiply\\\n:[\\\n$Items.EXTENDEDPRICE\\\n,{\\\n$multiply\\\n:[\\\n$l_tax_plus_1\\\n,\\\n$l_dis_min_1\\\n]}]}},\\\navg_price\\\n:{\\\n$avg\\\n:\\\n$Items.EXTENDEDPRICE\\\n},\\\navg_disc\\\n:{\\\n$avg\\\n:\\\n$Items.DISCOUNT\\\n},\\\ncount_order\\\n:{\\\n$sum\\\n:1}}}\n;\n\n            String sortStringQuery = \n{\\\n$sort\\\n:{\\\nItems.RETURNFLAG\\\n:1,\\\nItems.LINESTATUS\\\n:1}}\n;\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList\nBson\n aggregateQuery = new ArrayList\nBson\n();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.denormalized_order.aggregate(aggregateQuery);\n\n            MongoCursor\nDocument\n iterator = result.iterator();\n\n            ArrayList\nDocument\n results = new ArrayList\nDocument\n();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIf you call the above query, you will get the results below:\n\n\n\n\nShipping Priority Query (Q3)\n\n\nThis query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:\n\n\nselect\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate \n date '[DATE]'\n and l_shipdate \n date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;\n\n\n\n\nSimilar to Q1, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages that we will need are also a \"match\" stage, an \"unwind\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that corresponds to the sql query part shown below:\n\n\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate \n date '[DATE]'\n and l_shipdate \n date '[DATE]'\n\n\n\n\nExcept that no need to query the join conditions since we have already represented the relationship between the different objects by embedding documents. The match stage query is shown below:\n\n\n{  \n   \n$match\n:{  \n      \nCustomer.MKTSEGMENT\n:\nAUTOMOBILE\n,\n      \nORDERDATE\n:{  \n         \n$lte\n: ISODate(\n2000-01-01T00:00:00.000Z\n)\n      },\n      \nItems.SHIPDATE\n:{  \n         \n$gte\n: ISODate(\n1990-01-01T00:00:00.000Z\n)\n      }\n   }\n}\n\n\n\n\nSimilarly the unwind stage:\n\n\n{  \n   \n$unwind\n:\n$Items\n\n}\n\n\n\n\nThe project stage:\n\n\n{  \n   \n$project\n:{  \n      \nORDERDATE\n:1,\n      \nSHIPPRIORITY\n:1,\n      \nItems.EXTENDEDPRICE\n:1,\n      \nl_dis_min_1\n:{  \n         \n$subtract\n:[  \n            1,\n            \n$Items.DISCOUNT\n\n         ]\n      }\n   }\n}\n\n\n\n\nThe group stage:\n\n\n{  \n   \n$group\n:{  \n      \n_id\n:{  \n         \nORDERKEY\n:\n$ORDERKEY\n,\n         \nORDERDATE\n:\n$ORDERDATE\n,\n         \nSHIPPRIORITY\n:\n$SHIPPRIORITY\n\n      },\n      \nrevenue\n:{  \n         \n$sum\n:{  \n            \n$multiply\n:[  \n               \n$Items.EXTENDEDPRICE\n,\n               \n$l_dis_min_1\n\n            ]\n         }\n      }\n   }\n}\n\n\n\n\nFinally the sort stage:\n\n\n{  \n   \n$sort\n:{  \n      \nrevenue\n:1,\n      \nORDERDATE\n:1\n   }\n}\n\n\n\n\nThe complete query in mongoDB is shown below:\n\n\n[  \n   {  \n      \n$match\n:{  \n         \nCustomer.MKTSEGMENT\n:\nAUTOMOBILE\n,\n         \nORDERDATE\n:{  \n            \n$lte\n:  ISODate(\n2000-01-01T00:00:00.000Z\n)\n         },\n         \nItems.SHIPDATE\n:{  \n            \n$gte\n: ISODate(\n1990-01-01T00:00:00.000Z\n)\n         }\n      }\n   },\n   {  \n      \n$unwind\n:\n$Items\n\n   },\n   {  \n      \n$project\n:{  \n         \nORDERDATE\n:1,\n         \nSHIPPRIORITY\n:1,\n         \nItems.EXTENDEDPRICE\n:1,\n         \nl_dis_min_1\n:{  \n            \n$subtract\n:[  \n               1,\n               \n$Items.DISCOUNT\n\n            ]\n         }\n      }\n   },\n   {  \n      \n$group\n:{  \n         \n_id\n:{  \n            \nORDERKEY\n:\n$ORDERKEY\n,\n            \nORDERDATE\n:\n$ORDERDATE\n,\n            \nSHIPPRIORITY\n:\n$SHIPPRIORITY\n\n         },\n         \nrevenue\n:{  \n            \n$sum\n:{  \n               \n$multiply\n:[  \n                  \n$Items.EXTENDEDPRICE\n,\n                  \n$l_dis_min_1\n\n               ]\n            }\n         }\n      }\n   },\n   {  \n      \n$sort\n:{  \n         \nrevenue\n:1,\n         \nORDERDATE\n:1\n      }\n   }\n]\n\n\n\n\nThe complete java code for the api that will return the results of TPCH Q3 is shown below:\n\n\n    @GET\n    @Path(\n/q3\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q3 using this model\n, notes = \nReturns mongoDB document(s)\n, response = Document.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public ArrayList\nDocument\n getQ2Results() {\n\n        AggregateIterable\nDocument\n result;\n\n        try {\n\n            String matchStringQuery = \n{\\\n$match\\\n:{\\\nCustomer.MKTSEGMENT\\\n:\\\nAUTOMOBILE\\\n,\\\nORDERDATE\\\n:{\\\n$lte\\\n:ISODate(\\\n2000-01-01T00:00:00.000Z\\\n) },\\\nItems.SHIPDATE\\\n:{\\\n$gte\\\n: ISODate(\\\n1990-01-01T00:00:00.000Z\\\n)}}}\n;\n\n            String unWindStringQuery = \n{$unwind: \\\n$Items\\\n}\n;\n\n            String projectStringQuery = \n{\\\n$project\\\n:{\\\nORDERDATE\\\n:1,\\\nSHIPPRIORITY\\\n:1,\\\nItems.EXTENDEDPRICE\\\n:1,\\\nl_dis_min_1\\\n:{\\\n$subtract\\\n:[1,\\\n$Items.DISCOUNT\\\n]}}}\n;\n\n            String groupStringQuery = \n{\\\n$group\\\n:{\\\n_id\\\n:{\\\nORDERKEY\\\n:\\\n$ORDERKEY\\\n,\\\nORDERDATE\\\n:\\\n$ORDERDATE\\\n,\\\nSHIPPRIORITY\\\n:\\\n$SHIPPRIORITY\\\n},\\\nrevenue\\\n:{\\\n$sum\\\n:{\\\n$multiply\\\n:[\\\n$Items.EXTENDEDPRICE\\\n,\\\n$l_dis_min_1\\\n]}}}}\n;\n\n            String sortStringQuery = \n{\\\n$sort\\\n:{\\\nrevenue\\\n:1,\\\nORDERDATE\\\n:1}}\n;\n\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n            ArrayList\nBson\n aggregateQuery = new ArrayList\nBson\n();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.denormalized_order.aggregate(aggregateQuery);\n\n            MongoCursor\nDocument\n iterator = result.iterator();\n\n            ArrayList\nDocument\n results = new ArrayList\nDocument\n();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIf you call the above query, you will get the results below:\n\n\n\n\nOrder Priority Checking Query (Q4)\n\n\nThis query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:\n\n\nselect\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate \n= date '[DATE]'\n and o_orderdate \n date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate \n l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;\n\n\n\n\nSimilar to Q1 and Q3, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages we will need are  a \"project\" stage, a \"match\" stage, a \"group\" stage, and a \"sort\" stage. The \"project\" is used at the beginning to compare the lineitem COMMITDATE with the RECEIPTDATE and returned a 0 or 1 based on the result of the comparison. Then this value can be used in the following match stage which is shown below:\n\n\nThe first project stage:\n\n\n{  \n   \n$project\n:{  \n      \nORDERDATE\n:1,\n      \nORDERPRIORITY\n:1,\n      \neq\n:{  \n         \n$cond\n:[  \n            {  \n               \n$lt\n:[  \n                  \n$Items.COMMITDATE\n,\n                  \n$Items.RECEIPTDATE\n\n               ]\n            },\n            0,\n            1\n         ]\n      }\n   }\n}\n\n\n\n\nThe following match stage:\n\n\n{  \n   \n$match\n:{  \n      \nORDERDATE\n:{  \n         \n$gte\n:         ISODate(\n1990-01-01T00:00:00.000         Z\n)\n      },\n      \nORDERDATE\n:{  \n         \n$lt\n:         ISODate(\n2000-01-01T00:00:00.000         Z\n)\n      },\n      \neq\n:{  \n         \n$eq\n:1\n      }\n   }\n}\n\n\n\n\nThe group stage:\n\n\n{  \n   \n$group\n:{  \n      \n_id\n:{  \n         \nORDERPRIORITY\n:\n$ORDERPRIORITY\n\n      },\n      \norder_count\n:{  \n         \n$sum\n:1\n      }\n   }\n}\n\n\n\n\nFinally the sort stage:\n\n\n{  \n   \n$sort\n:{  \n      \nORDERPRIORITY\n:1\n   }\n}\n\n\n\n\nThe complete query in mongoDB is shown below:\n\n\n[  \n   {  \n      \n$project\n:{  \n         \nORDERDATE\n:1,\n         \nORDERPRIORITY\n:1,\n         \neq\n:{  \n            \n$cond\n:[  \n               {  \n                  \n$lt\n:[  \n                     \n$Items.COMMITDATE\n,\n                     \n$Items.RECEIPTDATE\n\n                  ]\n               },\n               0,\n               1\n            ]\n         }\n      }\n   },\n,\n   {  \n      \n$match\n:{  \n         \nORDERDATE\n:{  \n            \n$gte\n: ISODate(\n1990-01-01T00:00:00.000Z\n)\n         },\n         \nORDERDATE\n:{  \n            \n$lt\n: ISODate(\n2000-01-01T00:00:00.000Z\n)\n         },\n         \neq\n:{  \n            \n$eq\n:1\n         }\n      }\n   },\n   {  \n      \n$group\n:{  \n         \n_id\n:{  \n            \nORDERPRIORITY\n:\n$ORDERPRIORITY\n\n         },\n         \norder_count\n:{  \n            \n$sum\n:1\n         }\n      }\n   },\n   {  \n      \n$sort\n:{  \n         \nORDERPRIORITY\n:1\n      }\n   }\n]\n\n\n\n\nThe complete java code for the api that will return the results of TPCH Q4 is shown below:\n\n\n    @GET\n    @Path(\n/q4\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q4 using this model\n, notes = \nReturns mongoDB document(s)\n, response = Document.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public ArrayList\nDocument\n getQ3Results() {\n\n        AggregateIterable\nDocument\n result;\n\n        try {\n\n            String projectStringQuery = \n{\\\n$project\\\n:{\\\nORDERDATE\\\n:1,\\\nORDERPRIORITY\\\n:1,\\\neq\\\n:{\\\n$cond\\\n:[{\\\n$lt\\\n:[\\\n$Items.COMMITDATE\\\n,\\\n$Items.RECEIPTDATE\\\n]},0,1]}}}\n;\n\n            String matchStringQuery = \n{\\\n$match\\\n: {\\\nORDERDATE\\\n: {\\\n$gte\\\n: ISODate(\\\n1990-01-01T00:00:00.000Z\\\n)},\\\nORDERDATE\\\n: {\\\n$lt\\\n: ISODate(\\\n2000-01-01T00:00:00.000Z\\\n)},\\\neq\\\n:{\\\n$eq\\\n:1}}}\n;\n\n            String groupStringQuery = \n{\\\n$group\\\n:{\\\n_id\\\n:{\\\nORDERPRIORITY\\\n:\\\n$ORDERPRIORITY\\\n},\\\norder_count\\\n:{\\\n$sum\\\n:1}}}\n;\n\n            String sortStringQuery = \n{\\\n$sort\\\n:{\\\nORDERPRIORITY\\\n:1}}\n;\n\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n\n            BsonDocument matchBsonQuery = BsonDocument\n                    .parse(matchStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList\nBson\n aggregateQuery = new ArrayList\nBson\n();\n\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.denormalized_order.aggregate(aggregateQuery);\n\n            MongoCursor\nDocument\n iterator = result.iterator();\n\n            ArrayList\nDocument\n results = new ArrayList\nDocument\n();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIf you call the above query, you will get the results below:", 
            "title": "Denormalised Model"
        }, 
        {
            "location": "/MongoDB/Examples/Denormalised Model/#pricing-summary-report-query-q1", 
            "text": "This query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:  select\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate  = date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;  To execute the above query in MongoDB, we will use the pipeline aggregation framework. We will divide the above query to different stages then execute them at once. The stages that we will need are a \"match\" stage, an \"unwind\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that are having the lineitem shipdate less than or equal some date value or the sql query part shown below:  where\n   l_shipdate  = date '1998-12-01' - interval '[DELTA]' day (3)  The equivalent match stage is shown below:  {  \n    $match :{  \n       Items :{  \n          $elemMatch :{  \n             SHIPDATE :{  \n                $lte :ISODate( 2000-01-01T00:00:00.000Z )\n            }\n         }\n      }\n   }\n}  Then since the returned documents will contain an array of lineitem documents, we need to unwind these document so that they can be processed by the other stages. The unwind stage query is shown below:  {  \n    $unwind : $Items \n}  After unwinding the lineitem documents, we can run the project stage which will return only the attributes that we need as per the original sql query. The project stage query is shown below:  {  \n    $project :{  \n       Items.RETURNFLAG :1,\n       Items.LINESTATUS :1,\n       Items.QUANTITY :1,\n       Items.EXTENDEDPRICE :1,\n       Items.DISCOUNT :1,\n       l_dis_min_1 :{  \n          $subtract :[  \n            1,\n             $Items.DISCOUNT \n         ]\n      },\n       l_tax_plus_1 :{  \n          $add :[  \n             $Items.TAX ,\n            1\n         ]\n      }\n   }\n}  The above stage will return the attributes that we need to use in the following stages. We can also perform any required mathematical operations on any of the attributes.  Then we can perform the group stage which will group the results by certain attributes as shown below:  {  \n    $group :{  \n       _id :{  \n          RETURNFLAG : $Items.RETURNFLAG ,\n          LINESTATUS : $Items.LINESTATUS \n      },\n       sum_qty :{  \n          $sum : $Items.QUANTITY \n      },\n       sum_base_price :{  \n          $sum : $Items.EXTENDEDPRICE \n      },\n       sum_disc_price :{  \n          $sum :{  \n             $multiply :[  \n                $Items.EXTENDEDPRICE ,\n                $l_dis_min_1 \n            ]\n         }\n      },\n       sum_charge :{  \n          $sum :{  \n             $multiply :[  \n                $Items.EXTENDEDPRICE ,\n               {  \n                   $multiply :[  \n                      $l_tax_plus_1 ,\n                      $l_dis_min_1 \n                  ]\n               }\n            ]\n         }\n      },\n       avg_price :{  \n          $avg : $Items.EXTENDEDPRICE \n      },\n       avg_disc :{  \n          $avg : $Items.DISCOUNT \n      },\n       count_order :{  \n          $sum :1\n      }\n   }\n}  Finally we sort the results using the sort stage as shown below:  {  \n    $sort :{  \n       Items.RETURNFLAG :1,\n       Items.LINESTATUS :1\n   }\n}  The complete query in mongoDB is shown below:  [  \n   {  \n       $match :{  \n          Items :{  \n             $elemMatch :{  \n                SHIPDATE :{  \n                   $lte : ISODate( 2000-01-01T00:00:00.000Z )\n               }\n            }\n         }\n      }\n   },\n   {  \n       $unwind : $Items \n   },\n   {  \n       $project :{  \n          Items.RETURNFLAG :1,\n          Items.LINESTATUS :1,\n          Items.QUANTITY :1,\n          Items.EXTENDEDPRICE :1,\n          Items.DISCOUNT :1,\n          l_dis_min_1 :{  \n             $subtract :[  \n               1,\n                $Items.DISCOUNT \n            ]\n         },\n          l_tax_plus_1 :{  \n             $add :[  \n                $Items.TAX ,\n               1\n            ]\n         }\n      }\n   },\n   {  \n       $group :{  \n          _id :{  \n             RETURNFLAG : $Items.RETURNFLAG ,\n             LINESTATUS : $Items.LINESTATUS \n         },\n          sum_qty :{  \n             $sum : $Items.QUANTITY \n         },\n          sum_base_price :{  \n             $sum : $Items.EXTENDEDPRICE \n         },\n          sum_disc_price :{  \n             $sum :{  \n                $multiply :[  \n                   $Items.EXTENDEDPRICE ,\n                   $l_dis_min_1 \n               ]\n            }\n         },\n          sum_charge :{  \n             $sum :{  \n                $multiply :[  \n                   $Items.EXTENDEDPRICE ,\n                  {  \n                      $multiply :[  \n                         $l_tax_plus_1 ,\n                         $l_dis_min_1 \n                     ]\n                  }\n               ]\n            }\n         },\n          avg_price :{  \n             $avg : $Items.EXTENDEDPRICE \n         },\n          avg_disc :{  \n             $avg : $Items.DISCOUNT \n         },\n          count_order :{  \n             $sum :1\n         }\n      }\n   },\n   {  \n       $sort :{  \n          Items.RETURNFLAG :1,\n          Items.LINESTATUS :1\n      }\n   }\n]  The complete java code for the api that will return the results of TPCH Q1 is shown below:      @GET\n    @Path( /q1 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q1 using this model , notes =  Returns mongoDB document(s) , response = Document.class, responseContainer =  list )\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public ArrayList Document  getQ1Results() {\n\n        AggregateIterable Document  result;\n\n        try {\n\n            String matchStringQuery =  {\\ $match\\ :{\\ Items\\ :{\\ $elemMatch\\ :{\\ SHIPDATE\\ :{\\ $lte\\ :ISODate(\\ 2000-01-01T00:00:00.000Z\\ )}}}}} ;\n\n            String unWindStringQuery =  {$unwind: \\ $Items\\ } ;\n\n            String projectStringQuery =  {\\ $project\\ :{\\ Items.RETURNFLAG\\ :1,\\ Items.LINESTATUS\\ :1,\\ Items.QUANTITY\\ :1,\\ Items.EXTENDEDPRICE\\ :1,\\ Items.DISCOUNT\\ :1,\\ l_dis_min_1\\ :{\\ $subtract\\ :[1,\\ $Items.DISCOUNT\\ ]},\\ l_tax_plus_1\\ :{\\ $add\\ :[\\ $Items.TAX\\ ,1]}}} ;\n\n            String groupStringQuery =  {\\ $group\\ :{\\ _id\\ :{\\ RETURNFLAG\\ :\\ $Items.RETURNFLAG\\ ,\\ LINESTATUS\\ :\\ $Items.LINESTATUS\\ },\\ sum_qty\\ :{\\ $sum\\ :\\ $Items.QUANTITY\\ },\\ sum_base_price\\ :{\\ $sum\\ :\\ $Items.EXTENDEDPRICE\\ },\\ sum_disc_price\\ :{\\ $sum\\ :{\\ $multiply\\ :[\\ $Items.EXTENDEDPRICE\\ ,\\ $l_dis_min_1\\ ]}},\\ sum_charge\\ :{\\ $sum\\ :{\\ $multiply\\ :[\\ $Items.EXTENDEDPRICE\\ ,{\\ $multiply\\ :[\\ $l_tax_plus_1\\ ,\\ $l_dis_min_1\\ ]}]}},\\ avg_price\\ :{\\ $avg\\ :\\ $Items.EXTENDEDPRICE\\ },\\ avg_disc\\ :{\\ $avg\\ :\\ $Items.DISCOUNT\\ },\\ count_order\\ :{\\ $sum\\ :1}}} ;\n\n            String sortStringQuery =  {\\ $sort\\ :{\\ Items.RETURNFLAG\\ :1,\\ Items.LINESTATUS\\ :1}} ;\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList Bson  aggregateQuery = new ArrayList Bson ();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.denormalized_order.aggregate(aggregateQuery);\n\n            MongoCursor Document  iterator = result.iterator();\n\n            ArrayList Document  results = new ArrayList Document ();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above query, you will get the results below:", 
            "title": "Pricing Summary Report Query (Q1)"
        }, 
        {
            "location": "/MongoDB/Examples/Denormalised Model/#shipping-priority-query-q3", 
            "text": "This query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:  select\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate   date '[DATE]'\n and l_shipdate   date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;  Similar to Q1, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages that we will need are also a \"match\" stage, an \"unwind\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that corresponds to the sql query part shown below:  where\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate   date '[DATE]'\n and l_shipdate   date '[DATE]'  Except that no need to query the join conditions since we have already represented the relationship between the different objects by embedding documents. The match stage query is shown below:  {  \n    $match :{  \n       Customer.MKTSEGMENT : AUTOMOBILE ,\n       ORDERDATE :{  \n          $lte : ISODate( 2000-01-01T00:00:00.000Z )\n      },\n       Items.SHIPDATE :{  \n          $gte : ISODate( 1990-01-01T00:00:00.000Z )\n      }\n   }\n}  Similarly the unwind stage:  {  \n    $unwind : $Items \n}  The project stage:  {  \n    $project :{  \n       ORDERDATE :1,\n       SHIPPRIORITY :1,\n       Items.EXTENDEDPRICE :1,\n       l_dis_min_1 :{  \n          $subtract :[  \n            1,\n             $Items.DISCOUNT \n         ]\n      }\n   }\n}  The group stage:  {  \n    $group :{  \n       _id :{  \n          ORDERKEY : $ORDERKEY ,\n          ORDERDATE : $ORDERDATE ,\n          SHIPPRIORITY : $SHIPPRIORITY \n      },\n       revenue :{  \n          $sum :{  \n             $multiply :[  \n                $Items.EXTENDEDPRICE ,\n                $l_dis_min_1 \n            ]\n         }\n      }\n   }\n}  Finally the sort stage:  {  \n    $sort :{  \n       revenue :1,\n       ORDERDATE :1\n   }\n}  The complete query in mongoDB is shown below:  [  \n   {  \n       $match :{  \n          Customer.MKTSEGMENT : AUTOMOBILE ,\n          ORDERDATE :{  \n             $lte :  ISODate( 2000-01-01T00:00:00.000Z )\n         },\n          Items.SHIPDATE :{  \n             $gte : ISODate( 1990-01-01T00:00:00.000Z )\n         }\n      }\n   },\n   {  \n       $unwind : $Items \n   },\n   {  \n       $project :{  \n          ORDERDATE :1,\n          SHIPPRIORITY :1,\n          Items.EXTENDEDPRICE :1,\n          l_dis_min_1 :{  \n             $subtract :[  \n               1,\n                $Items.DISCOUNT \n            ]\n         }\n      }\n   },\n   {  \n       $group :{  \n          _id :{  \n             ORDERKEY : $ORDERKEY ,\n             ORDERDATE : $ORDERDATE ,\n             SHIPPRIORITY : $SHIPPRIORITY \n         },\n          revenue :{  \n             $sum :{  \n                $multiply :[  \n                   $Items.EXTENDEDPRICE ,\n                   $l_dis_min_1 \n               ]\n            }\n         }\n      }\n   },\n   {  \n       $sort :{  \n          revenue :1,\n          ORDERDATE :1\n      }\n   }\n]  The complete java code for the api that will return the results of TPCH Q3 is shown below:      @GET\n    @Path( /q3 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q3 using this model , notes =  Returns mongoDB document(s) , response = Document.class, responseContainer =  list )\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public ArrayList Document  getQ2Results() {\n\n        AggregateIterable Document  result;\n\n        try {\n\n            String matchStringQuery =  {\\ $match\\ :{\\ Customer.MKTSEGMENT\\ :\\ AUTOMOBILE\\ ,\\ ORDERDATE\\ :{\\ $lte\\ :ISODate(\\ 2000-01-01T00:00:00.000Z\\ ) },\\ Items.SHIPDATE\\ :{\\ $gte\\ : ISODate(\\ 1990-01-01T00:00:00.000Z\\ )}}} ;\n\n            String unWindStringQuery =  {$unwind: \\ $Items\\ } ;\n\n            String projectStringQuery =  {\\ $project\\ :{\\ ORDERDATE\\ :1,\\ SHIPPRIORITY\\ :1,\\ Items.EXTENDEDPRICE\\ :1,\\ l_dis_min_1\\ :{\\ $subtract\\ :[1,\\ $Items.DISCOUNT\\ ]}}} ;\n\n            String groupStringQuery =  {\\ $group\\ :{\\ _id\\ :{\\ ORDERKEY\\ :\\ $ORDERKEY\\ ,\\ ORDERDATE\\ :\\ $ORDERDATE\\ ,\\ SHIPPRIORITY\\ :\\ $SHIPPRIORITY\\ },\\ revenue\\ :{\\ $sum\\ :{\\ $multiply\\ :[\\ $Items.EXTENDEDPRICE\\ ,\\ $l_dis_min_1\\ ]}}}} ;\n\n            String sortStringQuery =  {\\ $sort\\ :{\\ revenue\\ :1,\\ ORDERDATE\\ :1}} ;\n\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n            ArrayList Bson  aggregateQuery = new ArrayList Bson ();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.denormalized_order.aggregate(aggregateQuery);\n\n            MongoCursor Document  iterator = result.iterator();\n\n            ArrayList Document  results = new ArrayList Document ();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above query, you will get the results below:", 
            "title": "Shipping Priority Query (Q3)"
        }, 
        {
            "location": "/MongoDB/Examples/Denormalised Model/#order-priority-checking-query-q4", 
            "text": "This query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:  select\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate  = date '[DATE]'\n and o_orderdate   date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate   l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;  Similar to Q1 and Q3, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages we will need are  a \"project\" stage, a \"match\" stage, a \"group\" stage, and a \"sort\" stage. The \"project\" is used at the beginning to compare the lineitem COMMITDATE with the RECEIPTDATE and returned a 0 or 1 based on the result of the comparison. Then this value can be used in the following match stage which is shown below:  The first project stage:  {  \n    $project :{  \n       ORDERDATE :1,\n       ORDERPRIORITY :1,\n       eq :{  \n          $cond :[  \n            {  \n                $lt :[  \n                   $Items.COMMITDATE ,\n                   $Items.RECEIPTDATE \n               ]\n            },\n            0,\n            1\n         ]\n      }\n   }\n}  The following match stage:  {  \n    $match :{  \n       ORDERDATE :{  \n          $gte :         ISODate( 1990-01-01T00:00:00.000         Z )\n      },\n       ORDERDATE :{  \n          $lt :         ISODate( 2000-01-01T00:00:00.000         Z )\n      },\n       eq :{  \n          $eq :1\n      }\n   }\n}  The group stage:  {  \n    $group :{  \n       _id :{  \n          ORDERPRIORITY : $ORDERPRIORITY \n      },\n       order_count :{  \n          $sum :1\n      }\n   }\n}  Finally the sort stage:  {  \n    $sort :{  \n       ORDERPRIORITY :1\n   }\n}  The complete query in mongoDB is shown below:  [  \n   {  \n       $project :{  \n          ORDERDATE :1,\n          ORDERPRIORITY :1,\n          eq :{  \n             $cond :[  \n               {  \n                   $lt :[  \n                      $Items.COMMITDATE ,\n                      $Items.RECEIPTDATE \n                  ]\n               },\n               0,\n               1\n            ]\n         }\n      }\n   },\n,\n   {  \n       $match :{  \n          ORDERDATE :{  \n             $gte : ISODate( 1990-01-01T00:00:00.000Z )\n         },\n          ORDERDATE :{  \n             $lt : ISODate( 2000-01-01T00:00:00.000Z )\n         },\n          eq :{  \n             $eq :1\n         }\n      }\n   },\n   {  \n       $group :{  \n          _id :{  \n             ORDERPRIORITY : $ORDERPRIORITY \n         },\n          order_count :{  \n             $sum :1\n         }\n      }\n   },\n   {  \n       $sort :{  \n          ORDERPRIORITY :1\n      }\n   }\n]  The complete java code for the api that will return the results of TPCH Q4 is shown below:      @GET\n    @Path( /q4 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q4 using this model , notes =  Returns mongoDB document(s) , response = Document.class, responseContainer =  list )\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public ArrayList Document  getQ3Results() {\n\n        AggregateIterable Document  result;\n\n        try {\n\n            String projectStringQuery =  {\\ $project\\ :{\\ ORDERDATE\\ :1,\\ ORDERPRIORITY\\ :1,\\ eq\\ :{\\ $cond\\ :[{\\ $lt\\ :[\\ $Items.COMMITDATE\\ ,\\ $Items.RECEIPTDATE\\ ]},0,1]}}} ;\n\n            String matchStringQuery =  {\\ $match\\ : {\\ ORDERDATE\\ : {\\ $gte\\ : ISODate(\\ 1990-01-01T00:00:00.000Z\\ )},\\ ORDERDATE\\ : {\\ $lt\\ : ISODate(\\ 2000-01-01T00:00:00.000Z\\ )},\\ eq\\ :{\\ $eq\\ :1}}} ;\n\n            String groupStringQuery =  {\\ $group\\ :{\\ _id\\ :{\\ ORDERPRIORITY\\ :\\ $ORDERPRIORITY\\ },\\ order_count\\ :{\\ $sum\\ :1}}} ;\n\n            String sortStringQuery =  {\\ $sort\\ :{\\ ORDERPRIORITY\\ :1}} ;\n\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n\n            BsonDocument matchBsonQuery = BsonDocument\n                    .parse(matchStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList Bson  aggregateQuery = new ArrayList Bson ();\n\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.denormalized_order.aggregate(aggregateQuery);\n\n            MongoCursor Document  iterator = result.iterator();\n\n            ArrayList Document  results = new ArrayList Document ();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above query, you will get the results below:", 
            "title": "Order Priority Checking Query (Q4)"
        }, 
        {
            "location": "/MongoDB/Examples/Mixed Model/", 
            "text": "In this model, we will try to denormalise some of the objects and normalise the rest. We have embedded the lineitem documents into the order document, however the supplier, customer, and partsupp are separate documents. The nation document is embedded into the supplier and the customer documents and the nation itself embed the region document. The partsupp document embed the part document, but reference the supplier document as shown below:\n\n\n\n\nThe Order document:\n\n\n {\n    \n_id\n: \nObjectID\n,\n    \norderstatus\n: \nString\n,\n    \ntotalprice\n: \nDouble\n,\n    \norderdate\n: \nDate\n,\n    \norderpriority\n: \nString\n,\n    \nclerk\n: \nString\n,\n    \nshippriority\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \ncustomer\n: \nObjectID\nCustomer\n,\n    \nLineItems\n: \nArray\nObject\nlineitem\n\n}\n\n\n\n\nThe customer document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \naddress\n: \nString\n,\n    \nphone\n: \nDouble\n,\n    \nacctbal\n: \nDouble\n,\n    \nmktsegment\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \nnation\n: \nObject\nnation\n\n}\n\n\n\n\nThe lineitem document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nquantity\n: \nDouble\n,\n    \nextende dprice\n: \nDouble\n,\n    \ndiscount\n: \nDouble\n,\n    \ntax\n: \nDouble\n,\n    \nreturnflag\n: \nBoolean\n,\n    \nlinestatus\n: \nString\n,\n    \nshipdate\n: \nDate\n,\n    \ncommitdate\n: \nDate\n,\n    \nreceiptdate\n: \nDate\n,\n    \nshipinstruct\n: \nString\n,\n    \nshipmode\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \npartsupp\n: \nObjectID\npartsupp\n\n}\n\n\n\n\nThe partsupp document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \navailqty\n: \nDouble\n,\n    \nsupplycost\n: \nDouble\n,\n    \ncomment\n: \nString\n,\n    \npart\n: \nObject\npart\n,\n    \nsupplier\n: \nObjectID\nsupplier\n\n}\n\n\n\n\nThe part document:\n\n\n{\n    \n_id\n: \nString\n,\n    \nname\n: \nString\n,\n    \nmfgr\n: \nString\n,\n    \nbrand\n: \nString\n,\n    \ntype\n: \nString\n,\n    \nsize\n: \nDouble\n,\n    \ncontainer\n: \nString\n,\n    \nretailprice\n: \nDouble\n,\n    \ncomment\n: \nString\n\n}\n\n\n\n\nThe supplier document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \naddress\n: \nString\n,\n    \nphone\n: \nDouble\n,\n    \nacctbal\n: \nDouble\n,\n    \ncomment\n: \nString\n,\n    \nnation\n: \nObject\nnation\n\n}\n\n\n\n\nThe nation nation document :\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \nregion\n: \nObject\nregion\n\n}\n\n\n\n\nThe region document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \ncomment\n: \nString\n\n}\n\n\n\n\nAn example is shown below:\n\n\nOrder\n\n\n {\n    \n_id\n: \n7821ef4d-8e8c-46e0-950d-83c245b9bee8\n,\n    \norderstatus\n: \nOpen\n,\n    \ntotalprice\n: \n233\n,\n    \norderdate\n: \n2015-12-21 10:51:25\n,\n    \norderpriority\n: \nHigh\n,\n    \nclerk\n: \nJohn\n,\n    \nshippriority\n: \nHigh\n,\n    \ncomment\n: \nThis is an order\n,\n    \ncustomer\n: \nf4005128-a7d0-11e5-bf7f-feff819cdc9f\n,\n    \nLineItems\n: [{\n        \n_id\n: \n2ddbd282-a7d1-11e5-bf7f-feff819cdc9f\n,\n        \nquantity\n: \n1\n,\n        \nextende dprice\n: \n200\n,\n        \ndiscount\n: \n20\n,\n        \ntax\n: \n2\n,\n        \nreturnflag\n: \nfalse\n,\n        \nlinestatus\n: \navailable\n,\n        \nshipdate\n: \n2015-12-21 10:51:25\n,\n        \ncommitdate\n: \n2015-12-21 10:51:25\n,\n        \nreceiptdate\n: \n2015-12-21 10:51:25\n,\n        \nshipinstruct\n: \nsome text\n,\n        \nshipmode\n: \nDHL\n,\n        \ncomment\n: \nsome text\n,\n        \npartsupp\n: \n62353e7e-a7d1-11e5-bf7f-feff819cdc9f\n\n    }]\n}\n\n\n\n\ncustomer:\n\n\n{\n    \n_id\n: \nf4005128-a7d0-11e5-bf7f-feff819cdc9f\n,\n    \nname\n: \nBilal\n,\n    \naddress\n: \nStreet 1\n,\n    \nphone\n: \n1223456\n,\n    \nacctbal\n: \n212\n,\n    \nmktsegment\n: \nsome text\n,\n    \ncomment\n: \nsome text\n,\n    \nnation\n: {\n        \n_id\n: \n0aa770e6-a7d1-11e5-bf7f-feff819cdc9f\n,\n        \nname\n: \nUS\n,\n        \ncomment\n: \nsome text\n,\n        \nregion\n: {\n            \n_id\n: \n18f228bc-a7d1-11e5-bf7f-feff819cdc9f\n,\n            \nname\n: \nTexas\n,\n            \ncomment\n: \nsome text\n\n        }\n    }\n}\n\n\n\n\npartsupp :\n\n\n{\n    \n_id\n: \n62353e7e-a7d1-11e5-bf7f-feff819cdc9f\n,\n    \navailqty\n: \n20\n,\n    \nsupplycost\n: \n220\n,\n    \ncomment\n: \nsome text\n,\n    \npart\n: {\n        \n_id\n: \n75f28eda-a7d1-11e5-bf7f-feff819cdc9f\n,\n        \nname\n: \nTshirt\n,\n        \nmfgr\n: \nBoss\n,\n        \nbrand\n: \nBoss\n,\n        \ntype\n: \nsport\n,\n        \nsize\n: \n40\n,\n        \ncontainer\n: \nsome text\n,\n        \nretailprice\n: \n230\n,\n        \ncomment\n: \nsome text\n\n    },\n    \nsupplier\n: \n968a0d3a-a7d1-11e5-bf7f-feff819cdc9f\n\n}\n\n\n\n\nsupplier:\n\n\n{\n    \n_id\n: \n968a0d3a-a7d1-11e5-bf7f-feff819cdc9f\n,\n    \nname\n: \nBoss Supplier\n,\n    \naddress\n: \nstreet 2\n,\n    \nphone\n: \n212323\n,\n    \nacctbal\n: \n2933\n,\n    \ncomment\n: \nsome text\n,\n    \nnation\n: {\n        \n_id\n: \n0aa770e6-a7d1-11e5-bf7f-feff819cdc9f\n,\n        \nname\n: \nUS\n,\n        \ncomment\n: \nsome text\n,\n        \nregion\n: {\n            \n_id\n: \n18f228bc-a7d1-11e5-bf7f-feff819cdc9f\n,\n            \nname\n: \nTexas\n,\n            \ncomment\n: \nsome text\n\n        }\n    }\n}\n\n\n\n\nIn the following sections, we will show how to represent three complex TPC-H sql queries in MongoDB using this data model.\n\n\nPricing Summary Report Query (Q1)\n\n\nThis query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:\n\n\nselect\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate \n= date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;\n\n\n\n\nSince the above query needs to be run only against the lineitem table, then we can easily run the corresponding query in MongoDB agains the lineitem collection.\n\n\nTo execute the above query in MongoDB, we will use the pipeline aggregation framework. We will divide the above query to different stages then execute them all at once. The stages that we will need are a \"match\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that are having the lineitem shipdate less than or equal some date value or the sql query part shown below:\n\n\nwhere\n   l_shipdate \n= date '1998-12-01' - interval '[DELTA]' day (3)\n\n\n\n\nThe equivalent match stage is shown below:\n\n\n{  \n   \n$match\n:{  \n      \nItems\n:{  \n         \n$elemMatch\n:{  \n            \nSHIPDATE\n:{  \n               \n$lte\n: ISODate(\n2016-01-01T00:00:00.000Z\n)\n            }\n         }\n      }\n   }\n}\n\n\n\n\nThen since the returned documents will contain an array of lineitem documents, we need to unwind these document so that they can be processed by the other stages. The unwind stage query is shown below:\n\n\n{  \n   \n$unwind\n:\n$Items\n\n}\n\n\n\n\nAfter unwinding the lineitem documents, we can run the project stage which will return only the attributes that we need as per the original sql query. The project stage query is shown below:\n\n\n{  \n   \n$project\n:{  \n      \nItems.RETURNFLAG\n:1,\n      \nItems.LINESTATUS\n:1,\n      \nItems.QUANTITY\n:1,\n      \nItems.EXTENDEDPRICE\n:1,\n      \nItems.DISCOUNT\n:1,\n      \nl_dis_min_1\n:{  \n         \n$subtract\n:[  \n            1,\n            \n$Items.DISCOUNT\n\n         ]\n      },\n      \nl_tax_plus_1\n:{  \n         \n$add\n:[  \n            \n$Items.TAX\n,\n            1\n         ]\n      }\n   }\n}\n\n\n\n\nThe above stage will return the attributes that we need to use in the following stages. We can also perform any required mathematical operations on any of the attributes.  Then we can perform the group stage which will group the results by certain attributes as shown below:\n\n\n{  \n   \n$group\n:{  \n      \n_id\n:{  \n         \nRETURNFLAG\n:\n$Items.RETURNFLAG\n,\n         \nLINESTATUS\n:\n$Items.LINESTATUS\n\n      },\n      \nsum_qty\n:{  \n         \n$sum\n:\n$Items.QUANTITY\n\n      },\n      \nsum_base_price\n:{  \n         \n$sum\n:\n$Items.EXTENDEDPRICE\n\n      },\n      \nsum_disc_price\n:{  \n         \n$sum\n:{  \n            \n$multiply\n:[  \n               \n$Items.EXTENDEDPRICE\n,\n               \n$l_dis_min_1\n\n            ]\n         }\n      },\n      \nsum_charge\n:{  \n         \n$sum\n:{  \n            \n$multiply\n:[  \n               \n$Items.EXTENDEDPRICE\n,\n               {  \n                  \n$multiply\n:[  \n                     \n$l_tax_plus_1\n,\n                     \n$l_dis_min_1\n\n                  ]\n               }\n            ]\n         }\n      },\n      \navg_price\n:{  \n         \n$avg\n:\n$Items.EXTENDEDPRICE\n\n      },\n      \navg_disc\n:{  \n         \n$avg\n:\n$Items.DISCOUNT\n\n      },\n      \ncount_order\n:{  \n         \n$sum\n:1\n      }\n   }\n}\n\n\n\n\nFinally we sort the results using the sort stage as shown below:\n\n\n{  \n   \n$sort\n:{  \n      \nRETURNFLAG\n:1,\n      \nLINESTATUS\n:1\n   }\n}\n\n\n\n\nThe complete query in mongoDB is shown below:\n\n\n[  \n   {  \n      \n$match\n:{  \n         \nItems\n:{  \n            \n$elemMatch\n:{  \n               \nSHIPDATE\n:{  \n                  \n$lte\n: ISODate(\n2016-01-01T00:00:00.000Z\n)\n               }\n            }\n         }\n      }\n   },\n   {  \n      \n$project\n:{  \n         \nItems.RETURNFLAG\n:1,\n         \nItems.LINESTATUS\n:1,\n         \nItems.QUANTITY\n:1,\n         \nItems.EXTENDEDPRICE\n:1,\n         \nItems.DISCOUNT\n:1,\n         \nl_dis_min_1\n:{  \n            \n$subtract\n:[  \n               1,\n               \n$Items.DISCOUNT\n\n            ]\n         },\n         \nl_tax_plus_1\n:{  \n            \n$add\n:[  \n               \n$Items.TAX\n,\n               1\n            ]\n         }\n      }\n   },\n   {  \n      \n$group\n:{  \n         \n_id\n:{  \n            \nRETURNFLAG\n:\n$Items.RETURNFLAG\n,\n            \nLINESTATUS\n:\n$Items.LINESTATUS\n\n         },\n         \nsum_qty\n:{  \n            \n$sum\n:\n$Items.QUANTITY\n\n         },\n         \nsum_base_price\n:{  \n            \n$sum\n:\n$Items.EXTENDEDPRICE\n\n         },\n         \nsum_disc_price\n:{  \n            \n$sum\n:{  \n               \n$multiply\n:[  \n                  \n$Items.EXTENDEDPRICE\n,\n                  \n$l_dis_min_1\n\n               ]\n            }\n         },\n         \nsum_charge\n:{  \n            \n$sum\n:{  \n               \n$multiply\n:[  \n                  \n$Items.EXTENDEDPRICE\n,\n                  {  \n                     \n$multiply\n:[  \n                        \n$l_tax_plus_1\n,\n                        \n$l_dis_min_1\n\n                     ]\n                  }\n               ]\n            }\n         },\n         \navg_price\n:{  \n            \n$avg\n:\n$Items.EXTENDEDPRICE\n\n         },\n         \navg_disc\n:{  \n            \n$avg\n:\n$Items.DISCOUNT\n\n         },\n         \ncount_order\n:{  \n            \n$sum\n:1\n         }\n      }\n   },\n   {  \n      \n$sort\n:{  \n         \nRETURNFLAG\n:1,\n         \nLINESTATUS\n:1\n      }\n   }\n]\n\n\n\n\nThe complete java code for the api that will return the results of TPCH Q1 is shown below:\n\n\n    @GET\n    @Path(\n/q1\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q1 using this model\n, notes = \nReturns mongoDB document(s)\n, response = Document.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public ArrayList\nDocument\n getQ1Results() {\n\n        AggregateIterable\nDocument\n result;\n\n        try {\n\n            String matchStringQuery = \n{\\\n$match\\\n:{\\\nItems\\\n:{\\\n$elemMatch\\\n:{\\\nSHIPDATE\\\n:{\\\n$lte\\\n:ISODate(\\\n2016-01-01T00:00:00.000Z\\\n)}}}}}\n;\n\n            String unWindStringQuery = \n{$unwind: \\\n$Items\\\n}\n;\n\n            String projectStringQuery = \n{\\\n$project\\\n:{\\\nItems.RETURNFLAG\\\n:1,\\\nItems.LINESTATUS\\\n:1,\\\nItems.QUANTITY\\\n:1,\\\nItems.EXTENDEDPRICE\\\n:1,\\\nItems.DISCOUNT\\\n:1,\\\nl_dis_min_1\\\n:{\\\n$subtract\\\n:[1,\\\n$Items.DISCOUNT\\\n]},\\\nl_tax_plus_1\\\n:{\\\n$add\\\n:[\\\n$Items.TAX\\\n,1]}}}\n;\n\n            String groupStringQuery = \n{\\\n$group\\\n:{\\\n_id\\\n:{\\\nRETURNFLAG\\\n:\\\n$Items.RETURNFLAG\\\n,\\\nLINESTATUS\\\n:\\\n$Items.LINESTATUS\\\n},\\\nsum_qty\\\n:{\\\n$sum\\\n:\\\n$Items.QUANTITY\\\n},\\\nsum_base_price\\\n:{\\\n$sum\\\n:\\\n$Items.EXTENDEDPRICE\\\n},\\\nsum_disc_price\\\n:{\\\n$sum\\\n:{\\\n$multiply\\\n:[\\\n$Items.EXTENDEDPRICE\\\n,\\\n$l_dis_min_1\\\n]}},\\\nsum_charge\\\n:{\\\n$sum\\\n:{\\\n$multiply\\\n:[\\\n$Items.EXTENDEDPRICE\\\n,{\\\n$multiply\\\n:[\\\n$l_tax_plus_1\\\n,\\\n$l_dis_min_1\\\n]}]}},\\\navg_price\\\n:{\\\n$avg\\\n:\\\n$Items.EXTENDEDPRICE\\\n},\\\navg_disc\\\n:{\\\n$avg\\\n:\\\n$Items.DISCOUNT\\\n},\\\ncount_order\\\n:{\\\n$sum\\\n:1}}}\n;\n\n            String sortStringQuery = \n{\\\n$sort\\\n:{\\\nItems.RETURNFLAG\\\n:1,\\\nItems.LINESTATUS\\\n:1}}\n;\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n            ArrayList\nBson\n aggregateQuery = new ArrayList\nBson\n();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.mixed_order.aggregate(aggregateQuery);\n\n            MongoCursor\nDocument\n iterator = result.iterator();\n\n            ArrayList\nDocument\n results = new ArrayList\nDocument\n();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIf you call the above query, you will get the results below:\n\n\n\n\nShipping Priority Query (Q3)\n\n\nThis query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:\n\n\nselect\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate \n date '[DATE]'\n and l_shipdate \n date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;\n\n\n\n\nSince mongoDB doesn't support joins and since the aggregation in mongoDB happens only in the collection level, then to run an aggregate query against related documents we should first fetch all the related documents and put them in a temporary collection to be able to run later aggregate query on this newly created collection. We do that by querying first the order collection and fetch all the documents that meet the condition \"o_orderdate \n date '[DATE]'\". Then for each customer objectID, we fetch the corresponding customer document and embed it inside the order document as shown below:\n\n\n  BsonDocument bsonQuery = BsonDocument\n                    .parse(\n{\\\nORDERDATE\\\n:{\\\n$lte\\\n:ISODate(\\\n2016-01-01T00:00:00.000Z\\\n) }}\n);\n\n            this.database.createCollection(\nmixed_q3_new_joined_orders\n);\n\n            final MongoCollection\nDocument\n mixed_q3_new_joined_orders = this.database\n                    .getCollection(\nmixed_q3_new_joined_orders\n);\n\n\n\n            this.mixed_order.find(bsonQuery).forEach(new Block\nDocument\n() {\n                @Override\n                public void apply(final Document order) {\n                    BsonDocument customerBsonQuery = BsonDocument\n                            .parse(\n{\\\nCUSTKEY\\\n:\\\n + order.get(\nCUSTKEY\n)\n                                    + \n\\\n}\n);\n\n                    order.put(\ncustomer\n, mixed_customer\n                            .find(customerBsonQuery).first());\n\n                    mixed_q3_new_joined_orders.insertOne(order);\n                }\n            });\n\n\n\n\nNow we have a new collection called \"mixed_q3_new_joined_orders\" that will have all the documents needed to run the original sql aggregate query. After we run the aggregate query, we can drop the temporary collection.\n\n\nSimilar to Q1, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages that we will need are a \"match\" stage, an \"unwind\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that corresponds to the sql query part shown below:\n\n\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate \n date '[DATE]'\n and l_shipdate \n date '[DATE]'\n\n\n\n\nExcept that no need to query the join conditions since we have already represented the relationship between the different objects by embedding documents. The match stage query is shown below:\n\n\n{  \n   \n$match\n:{  \n      \ncustomer.MKTSEGMENT\n:\nAUTOMOBILE\n,\n      \nItems.SHIPDATE\n:{  \n         \n$gte\n: ISODate(\n1990-01-01T00:00:00.000Z\n)\n      }\n   }\n}\n\n\n\n\nSimilarly the unwind stage:\n\n\n{  \n   \n$unwind\n:\n$Items\n\n}\n\n\n\n\nThe project stage:\n\n\n{  \n   \n$project\n:{  \n      \nORDERDATE\n:1,\n      \nSHIPPRIORITY\n:1,\n      \nItems.EXTENDEDPRICE\n:1,\n      \nl_dis_min_1\n:{  \n         \n$subtract\n:[  \n            1,\n            \n$Items.DISCOUNT\n\n         ]\n      }\n   }\n}\n\n\n\n\nThe group stage:\n\n\n{  \n   \n$group\n:{  \n      \n_id\n:{  \n         \nORDERKEY\n:\n$ORDERKEY\n,\n         \nORDERDATE\n:\n$ORDERDATE\n,\n         \nSHIPPRIORITY\n:\n$SHIPPRIORITY\n\n      },\n      \nrevenue\n:{  \n         \n$sum\n:{  \n            \n$multiply\n:[  \n               \n$Items.EXTENDEDPRICE\n,\n               \n$l_dis_min_1\n\n            ]\n         }\n      }\n   }\n}\n\n\n\n\nFinally the sort stage:\n\n\n{  \n   \n$sort\n:{  \n      \nrevenue\n:1,\n      \nORDERDATE\n:1\n   }\n}\n\n\n\n\nThe complete query in mongoDB is shown below:\n\n\n[  \n   {  \n      \n$match\n:{  \n         \ncustomer.MKTSEGMENT\n:\nAUTOMOBILE\n,\n         \nItems.SHIPDATE\n:{  \n            \n$gte\n:ISODate(\n1990-01-01T00:00:00.000Z\n)\n         }\n      }\n   },\n   {  \n      \n$unwind\n:\n$Items\n\n   },\n   {  \n      \n$project\n:{  \n         \nORDERDATE\n:1,\n         \nSHIPPRIORITY\n:1,\n         \nItems.EXTENDEDPRICE\n:1,\n         \nl_dis_min_1\n:{  \n            \n$subtract\n:[  \n               1,\n               \n$Items.DISCOUNT\n\n            ]\n         }\n      }\n   },\n   {  \n      \n$group\n:{  \n         \n_id\n:{  \n            \nORDERKEY\n:\n$ORDERKEY\n,\n            \nORDERDATE\n:\n$ORDERDATE\n,\n            \nSHIPPRIORITY\n:\n$SHIPPRIORITY\n\n         },\n         \nrevenue\n:{  \n            \n$sum\n:{  \n               \n$multiply\n:[  \n                  \n$Items.EXTENDEDPRICE\n,\n                  \n$l_dis_min_1\n\n               ]\n            }\n         }\n      }\n   },\n   {  \n      \n$sort\n:{  \n         \nrevenue\n:1,\n         \nORDERDATE\n:1\n      }\n   }\n]\n\n\n\n\nThe complete java code for the api that will return the results of TPCH Q3 is shown below:\n\n\n     @GET\n    @Path(\n/q3\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q3 using this model\n, notes = \nReturns mongoDB document(s)\n, response = Document.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public ArrayList\nDocument\n getQ3Results() {\n\n        AggregateIterable\nDocument\n result;\n\n        try {\n\n            BsonDocument bsonQuery = BsonDocument\n                    .parse(\n{\\\nORDERDATE\\\n:{\\\n$lte\\\n:ISODate(\\\n2016-01-01T00:00:00.000Z\\\n) }}\n);\n\n            this.database.createCollection(\nmixed_q3_new_joined_orders\n);\n\n            final MongoCollection\nDocument\n mixed_q3_new_joined_orders = this.database\n                    .getCollection(\nmixed_q3_new_joined_orders\n);\n\n\n\n            this.mixed_order.find(bsonQuery).forEach(new Block\nDocument\n() {\n                @Override\n                public void apply(final Document order) {\n                    BsonDocument customerBsonQuery = BsonDocument\n                            .parse(\n{\\\nCUSTKEY\\\n:\\\n + order.get(\nCUSTKEY\n)\n                                    + \n\\\n}\n);\n\n                    order.put(\ncustomer\n, mixed_customer\n                            .find(customerBsonQuery).first());\n\n                    mixed_q3_new_joined_orders.insertOne(order);\n                }\n            });\n\n            String matchStringQuery = \n{\\\n$match\\\n:{\\\ncustomer.MKTSEGMENT\\\n:\\\nAUTOMOBILE\\\n,\\\nItems.SHIPDATE\\\n:{\\\n$gte\\\n: ISODate(\\\n1990-01-01T00:00:00.000Z\\\n) }}}\n;\n\n            String unWindStringQuery = \n{$unwind: \\\n$Items\\\n}\n;\n\n            String projectStringQuery = \n{\\\n$project\\\n:{\\\nORDERDATE\\\n:1,\\\nSHIPPRIORITY\\\n:1,\\\nItems.EXTENDEDPRICE\\\n:1,\\\nl_dis_min_1\\\n:{\\\n$subtract\\\n:[1,\\\n$Items.DISCOUNT\\\n]}}}\n;\n\n            String groupStringQuery = \n{\\\n$group\\\n:{\\\n_id\\\n:{\\\nORDERKEY\\\n:\\\n$ORDERKEY\\\n,\\\nORDERDATE\\\n:\\\n$ORDERDATE\\\n,\\\nSHIPPRIORITY\\\n:\\\n$SHIPPRIORITY\\\n},\\\nrevenue\\\n:{\\\n$sum\\\n:{\\\n$multiply\\\n:[\\\n$Items.EXTENDEDPRICE\\\n,\\\n$l_dis_min_1\\\n]}}}}\n;\n\n            String sortStringQuery = \n{\\\n$sort\\\n:{\\\nrevenue\\\n:1,\\\nORDERDATE\\\n:1}}\n;\n\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n            ArrayList\nBson\n aggregateQuery = new ArrayList\nBson\n();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = mixed_q3_new_joined_orders.aggregate(aggregateQuery);\n\n            MongoCursor\nDocument\n iterator = result.iterator();\n\n            ArrayList\nDocument\n results = new ArrayList\nDocument\n();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            mixed_q3_new_joined_orders.drop();\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIf you call the above query, you will get the results below:\n\n\n\n\nOrder Priority Checking Query (Q4)\n\n\nThis query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:\n\n\nselect\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate \n= date '[DATE]'\n and o_orderdate \n date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate \n l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;\n\n\n\n\nSimilar to Q1 and Q3, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages we will need are  a \"project\" stage, a \"match\" stage, a \"group\" stage, and a \"sort\" stage. The \"project\" is used at the beginning to compare the lineitem COMMITDATE with the RECEIPTDATE and returned a 0 or 1 based on the result of the comparison. Then this value can be used in the following match stage which is shown below:\n\n\nThe first project stage:\n\n\n{  \n   \n$project\n:{  \n      \nORDERDATE\n:1,\n      \nORDERPRIORITY\n:1,\n      \neq\n:{  \n         \n$cond\n:[  \n            {  \n               \n$lt\n:[  \n                  \n$Items.COMMITDATE\n,\n                  \n$Items.RECEIPTDATE\n\n               ]\n            },\n            0,\n            1\n         ]\n      }\n   }\n}\n\n\n\n\nThe following match stage:\n\n\n{  \n   \n$match\n:{  \n      \nORDERDATE\n:{  \n         \n$gte\n: ISODate(\n1990-01-01T00:00:00.000Z\n)\n      },\n      \nORDERDATE\n:{  \n         \n$lt\n: ISODate(\n2000-01-01T00:00:00.000Z\n)\n      },\n      \neq\n:{  \n         \n$eq\n:1\n      }\n   }\n}\n\n\n\n\nThe group stage:\n\n\n{  \n   \n$group\n:{  \n      \n_id\n:{  \n         \nORDERPRIORITY\n:\n$ORDERPRIORITY\n\n      },\n      \norder_count\n:{  \n         \n$sum\n:1\n      }\n   }\n}\n\n\n\n\nFinally the sort stage:\n\n\n{  \n   \n$sort\n:{  \n      \nORDERPRIORITY\n:1\n   }\n}\n\n\n\n\nThe complete query in mongoDB is shown below:\n\n\n[  \n   {  \n      \n$project\n:{  \n         \nORDERDATE\n:1,\n         \nORDERPRIORITY\n:1,\n         \neq\n:{  \n            \n$cond\n:[  \n               {  \n                  \n$lt\n:[  \n                     \n$Items.COMMITDATE\n,\n                     \n$Items.RECEIPTDATE\n\n                  ]\n               },\n               0,\n               1\n            ]\n         }\n      }\n   },\n   {  \n      \n$match\n:{  \n         \nORDERDATE\n:{  \n            \n$gte\n: ISODate(\n1990-01-01T00:00:00.000Z\n)\n         },\n         \nORDERDATE\n:{  \n            \n$lt\n: ISODate(\n2000-01-01T00:00:00.000Z\n)\n         },\n         \neq\n:{  \n            \n$eq\n:1\n         }\n      }\n   },\n   {  \n      \n$group\n:{  \n         \n_id\n:{  \n            \nORDERPRIORITY\n:\n$ORDERPRIORITY\n\n         },\n         \norder_count\n:{  \n            \n$sum\n:1\n         }\n      }\n   },\n   {  \n      \n$sort\n:{  \n         \nORDERPRIORITY\n:1\n      }\n   }\n]\n\n\n\n\nThe complete java code for the api that will return the results of TPCH Q4 is shown below:\n\n\n    @GET\n    @Path(\n/q4\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q4 using this model\n, notes = \nReturns mongoDB document(s)\n, response = Document.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public ArrayList\nDocument\n getQ4Results() {\n\n        AggregateIterable\nDocument\n result;\n\n        try {\n\n            String projectStringQuery = \n{\\\n$project\\\n:{\\\nORDERDATE\\\n:1,\\\nORDERPRIORITY\\\n:1,\\\neq\\\n:{\\\n$cond\\\n:[{\\\n$lt\\\n:[\\\n$Items.COMMITDATE\\\n,\\\n$Items.RECEIPTDATE\\\n]},0,1]}}}\n;\n\n            String matchStringQuery = \n{\\\n$match\\\n: {\\\nORDERDATE\\\n: {\\\n$gte\\\n: ISODate(\\\n1990-01-01T00:00:00.000Z\\\n)},\\\nORDERDATE\\\n: {\\\n$lt\\\n: ISODate(\\\n2000-01-01T00:00:00.000Z\\\n)},\\\neq\\\n:{\\\n$eq\\\n:1}}}\n;\n\n            String groupStringQuery = \n{\\\n$group\\\n:{\\\n_id\\\n:{\\\nORDERPRIORITY\\\n:\\\n$ORDERPRIORITY\\\n},\\\norder_count\\\n:{\\\n$sum\\\n:1}}}\n;\n\n            String sortStringQuery = \n{\\\n$sort\\\n:{\\\nORDERPRIORITY\\\n:1}}\n;\n\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n            ArrayList\nBson\n aggregateQuery = new ArrayList\nBson\n();\n\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = mixed_order.aggregate(aggregateQuery);\n\n\n            MongoCursor\nDocument\n iterator = result.iterator();\n\n            ArrayList\nDocument\n results = new ArrayList\nDocument\n();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIf you call the above query, you will get the results below:", 
            "title": "Mixed Model"
        }, 
        {
            "location": "/MongoDB/Examples/Mixed Model/#pricing-summary-report-query-q1", 
            "text": "This query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:  select\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate  = date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;  Since the above query needs to be run only against the lineitem table, then we can easily run the corresponding query in MongoDB agains the lineitem collection.  To execute the above query in MongoDB, we will use the pipeline aggregation framework. We will divide the above query to different stages then execute them all at once. The stages that we will need are a \"match\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that are having the lineitem shipdate less than or equal some date value or the sql query part shown below:  where\n   l_shipdate  = date '1998-12-01' - interval '[DELTA]' day (3)  The equivalent match stage is shown below:  {  \n    $match :{  \n       Items :{  \n          $elemMatch :{  \n             SHIPDATE :{  \n                $lte : ISODate( 2016-01-01T00:00:00.000Z )\n            }\n         }\n      }\n   }\n}  Then since the returned documents will contain an array of lineitem documents, we need to unwind these document so that they can be processed by the other stages. The unwind stage query is shown below:  {  \n    $unwind : $Items \n}  After unwinding the lineitem documents, we can run the project stage which will return only the attributes that we need as per the original sql query. The project stage query is shown below:  {  \n    $project :{  \n       Items.RETURNFLAG :1,\n       Items.LINESTATUS :1,\n       Items.QUANTITY :1,\n       Items.EXTENDEDPRICE :1,\n       Items.DISCOUNT :1,\n       l_dis_min_1 :{  \n          $subtract :[  \n            1,\n             $Items.DISCOUNT \n         ]\n      },\n       l_tax_plus_1 :{  \n          $add :[  \n             $Items.TAX ,\n            1\n         ]\n      }\n   }\n}  The above stage will return the attributes that we need to use in the following stages. We can also perform any required mathematical operations on any of the attributes.  Then we can perform the group stage which will group the results by certain attributes as shown below:  {  \n    $group :{  \n       _id :{  \n          RETURNFLAG : $Items.RETURNFLAG ,\n          LINESTATUS : $Items.LINESTATUS \n      },\n       sum_qty :{  \n          $sum : $Items.QUANTITY \n      },\n       sum_base_price :{  \n          $sum : $Items.EXTENDEDPRICE \n      },\n       sum_disc_price :{  \n          $sum :{  \n             $multiply :[  \n                $Items.EXTENDEDPRICE ,\n                $l_dis_min_1 \n            ]\n         }\n      },\n       sum_charge :{  \n          $sum :{  \n             $multiply :[  \n                $Items.EXTENDEDPRICE ,\n               {  \n                   $multiply :[  \n                      $l_tax_plus_1 ,\n                      $l_dis_min_1 \n                  ]\n               }\n            ]\n         }\n      },\n       avg_price :{  \n          $avg : $Items.EXTENDEDPRICE \n      },\n       avg_disc :{  \n          $avg : $Items.DISCOUNT \n      },\n       count_order :{  \n          $sum :1\n      }\n   }\n}  Finally we sort the results using the sort stage as shown below:  {  \n    $sort :{  \n       RETURNFLAG :1,\n       LINESTATUS :1\n   }\n}  The complete query in mongoDB is shown below:  [  \n   {  \n       $match :{  \n          Items :{  \n             $elemMatch :{  \n                SHIPDATE :{  \n                   $lte : ISODate( 2016-01-01T00:00:00.000Z )\n               }\n            }\n         }\n      }\n   },\n   {  \n       $project :{  \n          Items.RETURNFLAG :1,\n          Items.LINESTATUS :1,\n          Items.QUANTITY :1,\n          Items.EXTENDEDPRICE :1,\n          Items.DISCOUNT :1,\n          l_dis_min_1 :{  \n             $subtract :[  \n               1,\n                $Items.DISCOUNT \n            ]\n         },\n          l_tax_plus_1 :{  \n             $add :[  \n                $Items.TAX ,\n               1\n            ]\n         }\n      }\n   },\n   {  \n       $group :{  \n          _id :{  \n             RETURNFLAG : $Items.RETURNFLAG ,\n             LINESTATUS : $Items.LINESTATUS \n         },\n          sum_qty :{  \n             $sum : $Items.QUANTITY \n         },\n          sum_base_price :{  \n             $sum : $Items.EXTENDEDPRICE \n         },\n          sum_disc_price :{  \n             $sum :{  \n                $multiply :[  \n                   $Items.EXTENDEDPRICE ,\n                   $l_dis_min_1 \n               ]\n            }\n         },\n          sum_charge :{  \n             $sum :{  \n                $multiply :[  \n                   $Items.EXTENDEDPRICE ,\n                  {  \n                      $multiply :[  \n                         $l_tax_plus_1 ,\n                         $l_dis_min_1 \n                     ]\n                  }\n               ]\n            }\n         },\n          avg_price :{  \n             $avg : $Items.EXTENDEDPRICE \n         },\n          avg_disc :{  \n             $avg : $Items.DISCOUNT \n         },\n          count_order :{  \n             $sum :1\n         }\n      }\n   },\n   {  \n       $sort :{  \n          RETURNFLAG :1,\n          LINESTATUS :1\n      }\n   }\n]  The complete java code for the api that will return the results of TPCH Q1 is shown below:      @GET\n    @Path( /q1 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q1 using this model , notes =  Returns mongoDB document(s) , response = Document.class, responseContainer =  list )\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public ArrayList Document  getQ1Results() {\n\n        AggregateIterable Document  result;\n\n        try {\n\n            String matchStringQuery =  {\\ $match\\ :{\\ Items\\ :{\\ $elemMatch\\ :{\\ SHIPDATE\\ :{\\ $lte\\ :ISODate(\\ 2016-01-01T00:00:00.000Z\\ )}}}}} ;\n\n            String unWindStringQuery =  {$unwind: \\ $Items\\ } ;\n\n            String projectStringQuery =  {\\ $project\\ :{\\ Items.RETURNFLAG\\ :1,\\ Items.LINESTATUS\\ :1,\\ Items.QUANTITY\\ :1,\\ Items.EXTENDEDPRICE\\ :1,\\ Items.DISCOUNT\\ :1,\\ l_dis_min_1\\ :{\\ $subtract\\ :[1,\\ $Items.DISCOUNT\\ ]},\\ l_tax_plus_1\\ :{\\ $add\\ :[\\ $Items.TAX\\ ,1]}}} ;\n\n            String groupStringQuery =  {\\ $group\\ :{\\ _id\\ :{\\ RETURNFLAG\\ :\\ $Items.RETURNFLAG\\ ,\\ LINESTATUS\\ :\\ $Items.LINESTATUS\\ },\\ sum_qty\\ :{\\ $sum\\ :\\ $Items.QUANTITY\\ },\\ sum_base_price\\ :{\\ $sum\\ :\\ $Items.EXTENDEDPRICE\\ },\\ sum_disc_price\\ :{\\ $sum\\ :{\\ $multiply\\ :[\\ $Items.EXTENDEDPRICE\\ ,\\ $l_dis_min_1\\ ]}},\\ sum_charge\\ :{\\ $sum\\ :{\\ $multiply\\ :[\\ $Items.EXTENDEDPRICE\\ ,{\\ $multiply\\ :[\\ $l_tax_plus_1\\ ,\\ $l_dis_min_1\\ ]}]}},\\ avg_price\\ :{\\ $avg\\ :\\ $Items.EXTENDEDPRICE\\ },\\ avg_disc\\ :{\\ $avg\\ :\\ $Items.DISCOUNT\\ },\\ count_order\\ :{\\ $sum\\ :1}}} ;\n\n            String sortStringQuery =  {\\ $sort\\ :{\\ Items.RETURNFLAG\\ :1,\\ Items.LINESTATUS\\ :1}} ;\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n            ArrayList Bson  aggregateQuery = new ArrayList Bson ();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.mixed_order.aggregate(aggregateQuery);\n\n            MongoCursor Document  iterator = result.iterator();\n\n            ArrayList Document  results = new ArrayList Document ();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above query, you will get the results below:", 
            "title": "Pricing Summary Report Query (Q1)"
        }, 
        {
            "location": "/MongoDB/Examples/Mixed Model/#shipping-priority-query-q3", 
            "text": "This query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:  select\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate   date '[DATE]'\n and l_shipdate   date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;  Since mongoDB doesn't support joins and since the aggregation in mongoDB happens only in the collection level, then to run an aggregate query against related documents we should first fetch all the related documents and put them in a temporary collection to be able to run later aggregate query on this newly created collection. We do that by querying first the order collection and fetch all the documents that meet the condition \"o_orderdate   date '[DATE]'\". Then for each customer objectID, we fetch the corresponding customer document and embed it inside the order document as shown below:    BsonDocument bsonQuery = BsonDocument\n                    .parse( {\\ ORDERDATE\\ :{\\ $lte\\ :ISODate(\\ 2016-01-01T00:00:00.000Z\\ ) }} );\n\n            this.database.createCollection( mixed_q3_new_joined_orders );\n\n            final MongoCollection Document  mixed_q3_new_joined_orders = this.database\n                    .getCollection( mixed_q3_new_joined_orders );\n\n\n\n            this.mixed_order.find(bsonQuery).forEach(new Block Document () {\n                @Override\n                public void apply(final Document order) {\n                    BsonDocument customerBsonQuery = BsonDocument\n                            .parse( {\\ CUSTKEY\\ :\\  + order.get( CUSTKEY )\n                                    +  \\ } );\n\n                    order.put( customer , mixed_customer\n                            .find(customerBsonQuery).first());\n\n                    mixed_q3_new_joined_orders.insertOne(order);\n                }\n            });  Now we have a new collection called \"mixed_q3_new_joined_orders\" that will have all the documents needed to run the original sql aggregate query. After we run the aggregate query, we can drop the temporary collection.  Similar to Q1, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages that we will need are a \"match\" stage, an \"unwind\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that corresponds to the sql query part shown below:  where\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate   date '[DATE]'\n and l_shipdate   date '[DATE]'  Except that no need to query the join conditions since we have already represented the relationship between the different objects by embedding documents. The match stage query is shown below:  {  \n    $match :{  \n       customer.MKTSEGMENT : AUTOMOBILE ,\n       Items.SHIPDATE :{  \n          $gte : ISODate( 1990-01-01T00:00:00.000Z )\n      }\n   }\n}  Similarly the unwind stage:  {  \n    $unwind : $Items \n}  The project stage:  {  \n    $project :{  \n       ORDERDATE :1,\n       SHIPPRIORITY :1,\n       Items.EXTENDEDPRICE :1,\n       l_dis_min_1 :{  \n          $subtract :[  \n            1,\n             $Items.DISCOUNT \n         ]\n      }\n   }\n}  The group stage:  {  \n    $group :{  \n       _id :{  \n          ORDERKEY : $ORDERKEY ,\n          ORDERDATE : $ORDERDATE ,\n          SHIPPRIORITY : $SHIPPRIORITY \n      },\n       revenue :{  \n          $sum :{  \n             $multiply :[  \n                $Items.EXTENDEDPRICE ,\n                $l_dis_min_1 \n            ]\n         }\n      }\n   }\n}  Finally the sort stage:  {  \n    $sort :{  \n       revenue :1,\n       ORDERDATE :1\n   }\n}  The complete query in mongoDB is shown below:  [  \n   {  \n       $match :{  \n          customer.MKTSEGMENT : AUTOMOBILE ,\n          Items.SHIPDATE :{  \n             $gte :ISODate( 1990-01-01T00:00:00.000Z )\n         }\n      }\n   },\n   {  \n       $unwind : $Items \n   },\n   {  \n       $project :{  \n          ORDERDATE :1,\n          SHIPPRIORITY :1,\n          Items.EXTENDEDPRICE :1,\n          l_dis_min_1 :{  \n             $subtract :[  \n               1,\n                $Items.DISCOUNT \n            ]\n         }\n      }\n   },\n   {  \n       $group :{  \n          _id :{  \n             ORDERKEY : $ORDERKEY ,\n             ORDERDATE : $ORDERDATE ,\n             SHIPPRIORITY : $SHIPPRIORITY \n         },\n          revenue :{  \n             $sum :{  \n                $multiply :[  \n                   $Items.EXTENDEDPRICE ,\n                   $l_dis_min_1 \n               ]\n            }\n         }\n      }\n   },\n   {  \n       $sort :{  \n          revenue :1,\n          ORDERDATE :1\n      }\n   }\n]  The complete java code for the api that will return the results of TPCH Q3 is shown below:       @GET\n    @Path( /q3 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q3 using this model , notes =  Returns mongoDB document(s) , response = Document.class, responseContainer =  list )\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public ArrayList Document  getQ3Results() {\n\n        AggregateIterable Document  result;\n\n        try {\n\n            BsonDocument bsonQuery = BsonDocument\n                    .parse( {\\ ORDERDATE\\ :{\\ $lte\\ :ISODate(\\ 2016-01-01T00:00:00.000Z\\ ) }} );\n\n            this.database.createCollection( mixed_q3_new_joined_orders );\n\n            final MongoCollection Document  mixed_q3_new_joined_orders = this.database\n                    .getCollection( mixed_q3_new_joined_orders );\n\n\n\n            this.mixed_order.find(bsonQuery).forEach(new Block Document () {\n                @Override\n                public void apply(final Document order) {\n                    BsonDocument customerBsonQuery = BsonDocument\n                            .parse( {\\ CUSTKEY\\ :\\  + order.get( CUSTKEY )\n                                    +  \\ } );\n\n                    order.put( customer , mixed_customer\n                            .find(customerBsonQuery).first());\n\n                    mixed_q3_new_joined_orders.insertOne(order);\n                }\n            });\n\n            String matchStringQuery =  {\\ $match\\ :{\\ customer.MKTSEGMENT\\ :\\ AUTOMOBILE\\ ,\\ Items.SHIPDATE\\ :{\\ $gte\\ : ISODate(\\ 1990-01-01T00:00:00.000Z\\ ) }}} ;\n\n            String unWindStringQuery =  {$unwind: \\ $Items\\ } ;\n\n            String projectStringQuery =  {\\ $project\\ :{\\ ORDERDATE\\ :1,\\ SHIPPRIORITY\\ :1,\\ Items.EXTENDEDPRICE\\ :1,\\ l_dis_min_1\\ :{\\ $subtract\\ :[1,\\ $Items.DISCOUNT\\ ]}}} ;\n\n            String groupStringQuery =  {\\ $group\\ :{\\ _id\\ :{\\ ORDERKEY\\ :\\ $ORDERKEY\\ ,\\ ORDERDATE\\ :\\ $ORDERDATE\\ ,\\ SHIPPRIORITY\\ :\\ $SHIPPRIORITY\\ },\\ revenue\\ :{\\ $sum\\ :{\\ $multiply\\ :[\\ $Items.EXTENDEDPRICE\\ ,\\ $l_dis_min_1\\ ]}}}} ;\n\n            String sortStringQuery =  {\\ $sort\\ :{\\ revenue\\ :1,\\ ORDERDATE\\ :1}} ;\n\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n            ArrayList Bson  aggregateQuery = new ArrayList Bson ();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = mixed_q3_new_joined_orders.aggregate(aggregateQuery);\n\n            MongoCursor Document  iterator = result.iterator();\n\n            ArrayList Document  results = new ArrayList Document ();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            mixed_q3_new_joined_orders.drop();\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above query, you will get the results below:", 
            "title": "Shipping Priority Query (Q3)"
        }, 
        {
            "location": "/MongoDB/Examples/Mixed Model/#order-priority-checking-query-q4", 
            "text": "This query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:  select\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate  = date '[DATE]'\n and o_orderdate   date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate   l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;  Similar to Q1 and Q3, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages we will need are  a \"project\" stage, a \"match\" stage, a \"group\" stage, and a \"sort\" stage. The \"project\" is used at the beginning to compare the lineitem COMMITDATE with the RECEIPTDATE and returned a 0 or 1 based on the result of the comparison. Then this value can be used in the following match stage which is shown below:  The first project stage:  {  \n    $project :{  \n       ORDERDATE :1,\n       ORDERPRIORITY :1,\n       eq :{  \n          $cond :[  \n            {  \n                $lt :[  \n                   $Items.COMMITDATE ,\n                   $Items.RECEIPTDATE \n               ]\n            },\n            0,\n            1\n         ]\n      }\n   }\n}  The following match stage:  {  \n    $match :{  \n       ORDERDATE :{  \n          $gte : ISODate( 1990-01-01T00:00:00.000Z )\n      },\n       ORDERDATE :{  \n          $lt : ISODate( 2000-01-01T00:00:00.000Z )\n      },\n       eq :{  \n          $eq :1\n      }\n   }\n}  The group stage:  {  \n    $group :{  \n       _id :{  \n          ORDERPRIORITY : $ORDERPRIORITY \n      },\n       order_count :{  \n          $sum :1\n      }\n   }\n}  Finally the sort stage:  {  \n    $sort :{  \n       ORDERPRIORITY :1\n   }\n}  The complete query in mongoDB is shown below:  [  \n   {  \n       $project :{  \n          ORDERDATE :1,\n          ORDERPRIORITY :1,\n          eq :{  \n             $cond :[  \n               {  \n                   $lt :[  \n                      $Items.COMMITDATE ,\n                      $Items.RECEIPTDATE \n                  ]\n               },\n               0,\n               1\n            ]\n         }\n      }\n   },\n   {  \n       $match :{  \n          ORDERDATE :{  \n             $gte : ISODate( 1990-01-01T00:00:00.000Z )\n         },\n          ORDERDATE :{  \n             $lt : ISODate( 2000-01-01T00:00:00.000Z )\n         },\n          eq :{  \n             $eq :1\n         }\n      }\n   },\n   {  \n       $group :{  \n          _id :{  \n             ORDERPRIORITY : $ORDERPRIORITY \n         },\n          order_count :{  \n             $sum :1\n         }\n      }\n   },\n   {  \n       $sort :{  \n          ORDERPRIORITY :1\n      }\n   }\n]  The complete java code for the api that will return the results of TPCH Q4 is shown below:      @GET\n    @Path( /q4 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q4 using this model , notes =  Returns mongoDB document(s) , response = Document.class, responseContainer =  list )\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public ArrayList Document  getQ4Results() {\n\n        AggregateIterable Document  result;\n\n        try {\n\n            String projectStringQuery =  {\\ $project\\ :{\\ ORDERDATE\\ :1,\\ ORDERPRIORITY\\ :1,\\ eq\\ :{\\ $cond\\ :[{\\ $lt\\ :[\\ $Items.COMMITDATE\\ ,\\ $Items.RECEIPTDATE\\ ]},0,1]}}} ;\n\n            String matchStringQuery =  {\\ $match\\ : {\\ ORDERDATE\\ : {\\ $gte\\ : ISODate(\\ 1990-01-01T00:00:00.000Z\\ )},\\ ORDERDATE\\ : {\\ $lt\\ : ISODate(\\ 2000-01-01T00:00:00.000Z\\ )},\\ eq\\ :{\\ $eq\\ :1}}} ;\n\n            String groupStringQuery =  {\\ $group\\ :{\\ _id\\ :{\\ ORDERPRIORITY\\ :\\ $ORDERPRIORITY\\ },\\ order_count\\ :{\\ $sum\\ :1}}} ;\n\n            String sortStringQuery =  {\\ $sort\\ :{\\ ORDERPRIORITY\\ :1}} ;\n\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n            ArrayList Bson  aggregateQuery = new ArrayList Bson ();\n\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = mixed_order.aggregate(aggregateQuery);\n\n\n            MongoCursor Document  iterator = result.iterator();\n\n            ArrayList Document  results = new ArrayList Document ();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above query, you will get the results below:", 
            "title": "Order Priority Checking Query (Q4)"
        }, 
        {
            "location": "/MongoDB/Examples/Normalized Model/", 
            "text": "In this model, we are fully normalising the model as we might do in relational databases. In mongoDB, we can achieve this by using object references. The order document will have references (objectIDs) to the customer and lineitems documents as shown below:\n\n\n\n\nThe Order document:\n\n\n {\n    \n_id\n: \nObjectID\n,\n    \norderstatus\n: \nString\n,\n    \ntotalprice\n: \nDouble\n,\n    \norderdate\n: \nDate\n,\n    \norderpriority\n: \nString\n,\n    \nclerk\n: \nString\n,\n    \nshippriority\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \ncustomer\n: \nObjectID\nCustomer\n,\n    \nLineItems\n: \nArray\nObjectID\nlineitem\n\n}\n\n\n\n\nThe Customer document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \naddress\n: \nString\n,\n    \nphone\n: \nDouble\n,\n    \nacctbal\n: \nDouble\n,\n    \nmktsegment\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \nnation\n: \nObjectID\nnation\n\n}\n\n\n\n\nThe Lineitem document :\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nquantity\n: \nDouble\n,\n    \nextende dprice\n: \nDouble\n,\n    \ndiscount\n: \nDouble\n,\n    \ntax\n: \nDouble\n,\n    \nreturnflag\n: \nBoolean\n,\n    \nlinestatus\n: \nString\n,\n    \nshipdate\n: \nDate\n,\n    \ncommitdate\n: \nDate\n,\n    \nreceiptdate\n: \nDate\n,\n    \nshipinstruct\n: \nString\n,\n    \nshipmode\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \npartsupp\n: \nObjectID\npartsupp\n\n}\n\n\n\n\nThe Partsupp document :\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \navailqty\n: \nDouble\n,\n    \nsupplycost\n: \nDouble\n,\n    \ncomment\n: \nString\n,\n    \npart\n: \nObjectID\npart\n,\n    \nsupplier\n: \nObjectID\nsupplier\n\n}\n\n\n\n\nThe Part document:\n\n\n{\n    \n_id\n: \nString\n,\n    \nname\n: \nString\n,\n    \nmfgr\n: \nString\n,\n    \nbrand\n: \nString\n,\n    \ntype\n: \nString\n,\n    \nsize\n: \nDouble\n,\n    \ncontainer\n: \nString\n,\n    \nretailprice\n: \nDouble\n,\n    \ncomment\n: \nString\n\n}\n\n\n\n\nThe Supplier document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \naddress\n: \nString\n,\n    \nphone\n: \nDouble\n,\n    \nacctbal\n: \nDouble\n,\n    \ncomment\n: \nString\n,\n    \nnation\n: \nObjectID\nnation\n\n}\n\n\n\n\nThe Nation document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \ncomment\n: \nString\n,\n    \nregion\n: \nObjectID\nregion\n\n}\n\n\n\n\nThe Region document:\n\n\n{\n    \n_id\n: \nObjectID\n,\n    \nname\n: \nString\n,\n    \ncomment\n: \nString\n\n}\n\n\n\n\nA sample example for all the documents:\n\n\nOrder:\n\n\n {\n    \n_id\n: \n7821ef4d-8e8c-46e0-950d-83c245b9bee8\n,\n    \norderstatus\n: \nOpen\n,\n    \ntotalprice\n: \n233\n,\n    \norderdate\n: \n2015-12-21 10:51:25\n,\n    \norderpriority\n: \nHigh\n,\n    \nclerk\n: \nJohn\n,\n    \nshippriority\n: \nHigh\n,\n    \ncomment\n: \nThis is an order\n,\n    \ncustomer\n: \nf4005128-a7d0-11e5-bf7f-feff819cdc9f\n,\n    \nLineItems\n: \n[\n2ddbd282-a7d1-11e5-bf7f-feff819cdc9f\n]\n\n}\n\n\n\n\nCustomer:\n\n\n{\n    \n_id\n: \nf4005128-a7d0-11e5-bf7f-feff819cdc9f\n,\n    \nname\n: \nBilal\n,\n    \naddress\n: \nStreet 1\n,\n    \nphone\n: \n1223456\n,\n    \nacctbal\n: \n212\n,\n    \nmktsegment\n: \nsome text\n,\n    \ncomment\n: \nsome text\n,\n    \nnation\n: \n0aa770e6-a7d1-11e5-bf7f-feff819cdc9f\n\n}\n\n\n\n\nlineitem:\n\n\n{\n    \n_id\n: \n2ddbd282-a7d1-11e5-bf7f-feff819cdc9f\n,\n    \nquantity\n: \n1\n,\n    \nextende dprice\n: \n200\n,\n    \ndiscount\n: \n20\n,\n    \ntax\n: \n2\n,\n    \nreturnflag\n: \nfalse\n,\n    \nlinestatus\n: \navailable\n,\n    \nshipdate\n: \n2015-12-21 10:51:25\n,\n    \ncommitdate\n: \n2015-12-21 10:51:25\n,\n    \nreceiptdate\n: \n2015-12-21 10:51:25\n,\n    \nshipinstruct\n: \nsome text\n,\n    \nshipmode\n: \nDHL\n,\n    \ncomment\n: \nsome text\n,\n    \npartsupp\n: \n62353e7e-a7d1-11e5-bf7f-feff819cdc9f\n\n}\n\n\n\n\npartsupp:\n\n\n{\n    \n_id\n: \n62353e7e-a7d1-11e5-bf7f-feff819cdc9f\n,\n    \navailqty\n: \n20\n,\n    \nsupplycost\n: \n220\n,\n    \ncomment\n: \nsome text\n,\n    \npart\n: \n75f28eda-a7d1-11e5-bf7f-feff819cdc9f\n,\n    \nsupplier\n: \n968a0d3a-a7d1-11e5-bf7f-feff819cdc9f\n\n}\n\n\n\n\npart:\n\n\n{\n    \n_id\n: \n75f28eda-a7d1-11e5-bf7f-feff819cdc9f\n,\n    \nname\n: \nTshirt\n,\n    \nmfgr\n: \nBoss\n,\n    \nbrand\n: \nBoss\n,\n    \ntype\n: \nsport\n,\n    \nsize\n: \n40\n,\n    \ncontainer\n: \nsome text\n,\n    \nretailprice\n: \n230\n,\n    \ncomment\n: \nsome text\n\n}\n\n\n\n\nsupplier:\n\n\n{\n    \n_id\n: \n968a0d3a-a7d1-11e5-bf7f-feff819cdc9f\n,\n    \nname\n: \nBoss Supplier\n,\n    \naddress\n: \nstreet 2\n,\n    \nphone\n: \n212323\n,\n    \nacctbal\n: \n2933\n,\n    \ncomment\n: \nsome text\n,\n    \nnation\n: \n0aa770e6-a7d1-11e5-bf7f-feff819cdc9f\n\n}\n\n\n\n\nnation:\n\n\n{\n    \n_id\n: \n0aa770e6-a7d1-11e5-bf7f-feff819cdc9f\n,\n    \nname\n: \nUS\n,\n    \ncomment\n: \nsome text\n,\n    \nregion\n: \n18f228bc-a7d1-11e5-bf7f-feff819cdc9f\n\n}\n\n\n\n\nregion:\n\n\n{\n    \n_id\n: \n18f228bc-a7d1-11e5-bf7f-feff819cdc9f\n,\n    \nname\n: \nTexas\n,\n    \ncomment\n: \nsome text\n\n}\n\n\n\n\nIn the following sections, we will show how to represent three complex TPC-H sql queries in MongoDB using this data model.\n\n\nPricing Summary Report Query (Q1)\n\n\nThis query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:\n\n\nselect\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate \n= date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;\n\n\n\n\nSince the above query needs to be run only against the lineitem table, then we can easily run the corresponding query in MongoDB agains the the lineitem collection.\n\n\nTo execute the above query in MongoDB, we will use the pipeline aggregation framework. We will divide the above query to different stages then execute them all at once. The stages that we will need are a \"match\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that are having the lineitem shipdate less than or equal some date value or the sql query part shown below:\n\n\nwhere\n   l_shipdate \n= date '1998-12-01' - interval '[DELTA]' day (3)\n\n\n\n\nThe equivalent match stage is shown below:\n\n\n{  \n   \n$match\n:{  \n      \nSHIPDATE\n:{  \n         \n$lte\n:ISODate(\n2016-01-01T00:00:00.000Z\n)\n      }\n   }\n}\n\n\n\n\nThen we can run the project stage which will return only the attributes that we need as per the original sql query. The project stage query is shown below:\n\n\n{  \n   \n$project\n:{  \n      \nRETURNFLAG\n:1,\n      \nLINESTATUS\n:1,\n      \nQUANTITY\n:1,\n      \nEXTENDEDPRICE\n:1,\n      \nDISCOUNT\n:1,\n      \nl_dis_min_1\n:{  \n         \n$subtract\n:[  \n            1,\n            \n$DISCOUNT\n\n         ]\n      },\n      \nl_tax_plus_1\n:{  \n         \n$add\n:[  \n            \n$TAX\n,\n            1\n         ]\n      }\n   }\n}\n\n\n\n\nThe above stage will return the attributes that we need to use in the following stages. We can also perform any required mathematical operations on any of the attributes.  Then we can perform the group stage which will group the results by certain attributes as shown below:\n\n\n{  \n   \n$group\n:{  \n      \n_id\n:{  \n         \nRETURNFLAG\n:\n$RETURNFLAG\n,\n         \nLINESTATUS\n:\n$LINESTATUS\n\n      },\n      \nsum_qty\n:{  \n         \n$sum\n:\n$QUANTITY\n\n      },\n      \nsum_base_price\n:{  \n         \n$sum\n:\n$EXTENDEDPRICE\n\n      },\n      \nsum_disc_price\n:{  \n         \n$sum\n:{  \n            \n$multiply\n:[  \n               \n$EXTENDEDPRICE\n,\n               \n$l_dis_min_1\n\n            ]\n         }\n      },\n      \nsum_charge\n:{  \n         \n$sum\n:{  \n            \n$multiply\n:[  \n               \n$EXTENDEDPRICE\n,\n               {  \n                  \n$multiply\n:[  \n                     \n$l_tax_plus_1\n,\n                     \n$l_dis_min_1\n\n                  ]\n               }\n            ]\n         }\n      },\n      \navg_price\n:{  \n         \n$avg\n:\n$EXTENDEDPRICE\n\n      },\n      \navg_disc\n:{  \n         \n$avg\n:\n$DISCOUNT\n\n      },\n      \ncount_order\n:{  \n         \n$sum\n:1\n      }\n   }\n}\n\n\n\n\nFinally we sort the results using the sort stage as shown below:\n\n\n{  \n   \n$sort\n:{  \n      \nRETURNFLAG\n:1,\n      \nLINESTATUS\n:1\n   }\n}\n\n\n\n\nThe complete query in mongoDB is shown below:\n\n\n[  \n   {  \n      \n$match\n:{  \n         \nSHIPDATE\n:{  \n            \n$lte\n: ISODate(\n2016-01-01T00:00:00.000Z\n)\n         }\n      }\n   },\n   {  \n      \n$project\n:{  \n         \nRETURNFLAG\n:1,\n         \nLINESTATUS\n:1,\n         \nQUANTITY\n:1,\n         \nEXTENDEDPRICE\n:1,\n         \nDISCOUNT\n:1,\n         \nl_dis_min_1\n:{  \n            \n$subtract\n:[  \n               1,\n               \n$DISCOUNT\n\n            ]\n         },\n         \nl_tax_plus_1\n:{  \n            \n$add\n:[  \n               \n$TAX\n,\n               1\n            ]\n         }\n      }\n   },\n   {  \n      \n$group\n:{  \n         \n_id\n:{  \n            \nRETURNFLAG\n:\n$RETURNFLAG\n,\n            \nLINESTATUS\n:\n$LINESTATUS\n\n         },\n         \nsum_qty\n:{  \n            \n$sum\n:\n$QUANTITY\n\n         },\n         \nsum_base_price\n:{  \n            \n$sum\n:\n$EXTENDEDPRICE\n\n         },\n         \nsum_disc_price\n:{  \n            \n$sum\n:{  \n               \n$multiply\n:[  \n                  \n$EXTENDEDPRICE\n,\n                  \n$l_dis_min_1\n\n               ]\n            }\n         },\n         \nsum_charge\n:{  \n            \n$sum\n:{  \n               \n$multiply\n:[  \n                  \n$EXTENDEDPRICE\n,\n                  {  \n                     \n$multiply\n:[  \n                        \n$l_tax_plus_1\n,\n                        \n$l_dis_min_1\n\n                     ]\n                  }\n               ]\n            }\n         },\n         \navg_price\n:{  \n            \n$avg\n:\n$EXTENDEDPRICE\n\n         },\n         \navg_disc\n:{  \n            \n$avg\n:\n$DISCOUNT\n\n         },\n         \ncount_order\n:{  \n            \n$sum\n:1\n         }\n      }\n   },\n   {  \n      \n$sort\n:{  \n         \nRETURNFLAG\n:1,\n         \nLINESTATUS\n:1\n      }\n   }\n]\n\n\n\n\nThe complete java code for the api that will return the results of TPCH Q1 is shown below:\n\n\n    @GET\n    @Path(\n/q1\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q1 using this model\n, notes = \nReturns mongoDB document(s)\n, response = Document.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public ArrayList\nDocument\n getQ1Results() {\n\n        AggregateIterable\nDocument\n result;\n\n        try {\n\n            String matchStringQuery = \n{\\\n$match\\\n:{\\\nSHIPDATE\\\n:{\\\n$lte\\\n:ISODate(\\\n2016-01-01T00:00:00.000Z\\\n)}}}\n;\n\n            String projectStringQuery = \n{\\\n$project\\\n:{\\\nRETURNFLAG\\\n:1,\\\nLINESTATUS\\\n:1,\\\nQUANTITY\\\n:1,\\\nEXTENDEDPRICE\\\n:1,\\\nDISCOUNT\\\n:1,\\\nl_dis_min_1\\\n:{\\\n$subtract\\\n:[1,\\\n$DISCOUNT\\\n]},\\\nl_tax_plus_1\\\n:{\\\n$add\\\n:[\\\n$TAX\\\n,1]}}}\n;\n\n            String groupStringQuery = \n{\\\n$group\\\n:{\\\n_id\\\n:{\\\nRETURNFLAG\\\n:\\\n$RETURNFLAG\\\n,\\\nLINESTATUS\\\n:\\\n$LINESTATUS\\\n},\\\nsum_qty\\\n:{\\\n$sum\\\n:\\\n$QUANTITY\\\n},\\\nsum_base_price\\\n:{\\\n$sum\\\n:\\\n$EXTENDEDPRICE\\\n},\\\nsum_disc_price\\\n:{\\\n$sum\\\n:{\\\n$multiply\\\n:[\\\n$EXTENDEDPRICE\\\n,\\\n$l_dis_min_1\\\n]}},\\\nsum_charge\\\n:{\\\n$sum\\\n:{\\\n$multiply\\\n:[\\\n$EXTENDEDPRICE\\\n,{\\\n$multiply\\\n:[\\\n$l_tax_plus_1\\\n,\\\n$l_dis_min_1\\\n]}]}},\\\navg_price\\\n:{\\\n$avg\\\n:\\\n$EXTENDEDPRICE\\\n},\\\navg_disc\\\n:{\\\n$avg\\\n:\\\n$DISCOUNT\\\n},\\\ncount_order\\\n:{\\\n$sum\\\n:1}}}\n;\n\n            String sortStringQuery = \n{\\\n$sort\\\n:{\\\nRETURNFLAG\\\n:1,\\\nLINESTATUS\\\n:1}}\n;\n\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList\nBson\n aggregateQuery = new ArrayList\nBson\n();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.normalized_lineitem.aggregate(aggregateQuery);\n\n            MongoCursor\nDocument\n iterator = result.iterator();\n\n            ArrayList\nDocument\n results = new ArrayList\nDocument\n();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\nIf you call the above query, you will get the results below:\n\n\n\n\nShipping Priority Query (Q3)\n\n\nThis query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:\n\n\nselect\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate \n date '[DATE]'\n and l_shipdate \n date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;\n\n\n\n\nSince mongoDB doesn't support joins and since the aggregation in mongoDB happens only in the collection level, then to run an aggregate query against related documents we should first fetch all the related documents and put them in a temporary collection to be able to run later aggregate query on this newly created collection. We do that by querying first the order collection and fetch all the documents that meet the condition \"o_orderdate \n date '[DATE]'\". Then for each lineitem objectID, we fetch the corresponding lineitem document and embed it inside the order document. We do the same for the customer document as shown below:\n\n\n BsonDocument bsonQuery = BsonDocument\n                    .parse(\n{\\\nORDERDATE\\\n:{\\\n$lte\\\n:ISODate(\\\n2016-01-01T00:00:00.000Z\\\n) }}\n);\n\n            this.database.createCollection(\nnormalized_q3_new_joined_orders\n);\n\n            final MongoCollection\nDocument\n normalized_q3_new_joined_orders = this.database\n                    .getCollection(\nnormalized_q3_new_joined_orders\n);\n\n            this.normalized_order.find(bsonQuery).forEach(\n                    new Block\nDocument\n() {\n                        @Override\n                        public void apply(final Document order) {\n                            BsonDocument customerBsonQuery = BsonDocument\n                                    .parse(\n{\\\nCUSTKEY\\\n:\\\n\n                                            + order.get(\nCUSTKEY\n) + \n\\\n}\n);\n\n                            order.put(\ncustomer\n,\n                                    normalized_customer.find(customerBsonQuery)\n                                            .first());\n\n                            BsonDocument lineitemsBsonQuery = BsonDocument\n                                    .parse(\n{\\\nORDERKEY\\\n:\\\n\n                                            + order\n                                            .get(\nORDERKEY\n) + \n\\\n}\n);\n\n                            order.put(\nlineitems\n, normalized_lineitem\n                                    .find(lineitemsBsonQuery));\n\n                            normalized_q3_new_joined_orders.insertOne(order);\n                        }\n                    });\n\n\n\n\nNow we have a new collection called \"normalized_q3_new_joined_orders\" that will have all the documents needed to run the original sql aggregate query. After we run the aggregate query, we can drop the temporary collection.\n\n\nSimilar to Q1, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages that we will need are a \"match\" stage, an \"unwind\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that corresponds to the sql query part shown below:\n\n\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate \n date '[DATE]'\n and l_shipdate \n date '[DATE]'\n\n\n\n\nExcept that no need to query the join conditions since we have already represented the relationship between the different objects by embedding documents. The match stage query is shown below:\n\n\n{  \n   \n$match\n:{  \n      \ncustomer.MKTSEGMENT\n:\nAUTOMOBILE\n,\n      \nlineitems.SHIPDATE\n:{  \n         \n$gte\n: ISODate(\n1990-01-01T00:00:00.000Z\n)\n      }\n   }\n}\n\n\n\n\nSimilarly the unwind stage:\n\n\n{  \n   \n$unwind\n:\n$lineitems\n\n}\n\n\n\n\nThe project stage:\n\n\n{  \n   \n$project\n:{  \n      \nORDERDATE\n:1,\n      \nSHIPPRIORITY\n:1,\n      \nlineitems.EXTENDEDPRICE\n:1,\n      \nl_dis_min_1\n:{  \n         \n$subtract\n:[  \n            1,\n            \n$lineitems.DISCOUNT\n\n         ]\n      }\n   }\n}\n\n\n\n\nThe group stage:\n\n\n{  \n   \n$group\n:{  \n      \n_id\n:{  \n         \nORDERKEY\n:\n$ORDERKEY\n,\n         \nORDERDATE\n:\n$ORDERDATE\n,\n         \nSHIPPRIORITY\n:\n$SHIPPRIORITY\n\n      },\n      \nrevenue\n:{  \n         \n$sum\n:{  \n            \n$multiply\n:[  \n               \n$lineitems.EXTENDEDPRICE\n,\n               \n$l_dis_min_1\n\n            ]\n         }\n      }\n   }\n}\n\n\n\n\nFinally the sort stage:\n\n\n{  \n   \n$sort\n:{  \n      \nrevenue\n:1,\n      \nORDERDATE\n:1\n   }\n}\n\n\n\n\nThe complete query in mongoDB is shown below:\n\n\n[  \n   {  \n      \n$match\n:{  \n         \ncustomer.MKTSEGMENT\n:\nAUTOMOBILE\n,\n         \nlineitems.SHIPDATE\n:{  \n            \n$gte\n: ISODate(\n1990-01-01T00:00:00.000Z\n)\n         }\n      }\n   },\n   {  \n      \n$unwind\n:\n$lineitems\n\n   },\n   {  \n      \n$project\n:{  \n         \nORDERDATE\n:1,\n         \nSHIPPRIORITY\n:1,\n         \nlineitems.EXTENDEDPRICE\n:1,\n         \nl_dis_min_1\n:{  \n            \n$subtract\n:[  \n               1,\n               \n$lineitems.DISCOUNT\n\n            ]\n         }\n      }\n   },\n   {  \n      \n$group\n:{  \n         \n_id\n:{  \n            \nORDERKEY\n:\n$ORDERKEY\n,\n            \nORDERDATE\n:\n$ORDERDATE\n,\n            \nSHIPPRIORITY\n:\n$SHIPPRIORITY\n\n         },\n         \nrevenue\n:{  \n            \n$sum\n:{  \n               \n$multiply\n:[  \n                  \n$lineitems.EXTENDEDPRICE\n,\n                  \n$l_dis_min_1\n\n               ]\n            }\n         }\n      }\n   },\n   {  \n      \n$sort\n:{  \n         \nrevenue\n:1,\n         \nORDERDATE\n:1\n      }\n   }\n]\n\n\n\n\nThe complete java code for the api that will return the results of TPCH Q3 is shown below:\n\n\n     @GET\n    @Path(\n/q3\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q3 using this model\n, notes = \nReturns mongoDB document(s)\n, response = Document.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public ArrayList\nDocument\n getQ3Results() {\n\n        AggregateIterable\nDocument\n result;\n\n        try {\n\n            BsonDocument bsonQuery = BsonDocument\n                    .parse(\n{\\\nORDERDATE\\\n:{\\\n$lte\\\n:ISODate(\\\n2016-01-01T00:00:00.000Z\\\n) }}\n);\n\n            this.database.createCollection(\nnormalized_q3_new_joined_orders\n);\n\n            final MongoCollection\nDocument\n normalized_q3_new_joined_orders = this.database\n                    .getCollection(\nnormalized_q3_new_joined_orders\n);\n\n            this.normalized_order.find(bsonQuery).forEach(\n                    new Block\nDocument\n() {\n                        @Override\n                        public void apply(final Document order) {\n                            BsonDocument customerBsonQuery = BsonDocument\n                                    .parse(\n{\\\nCUSTKEY\\\n:\\\n\n                                            + order.get(\nCUSTKEY\n) + \n\\\n}\n);\n\n                            order.put(\ncustomer\n,\n                                    normalized_customer.find(customerBsonQuery)\n                                            .first());\n\n                            BsonDocument lineitemsBsonQuery = BsonDocument\n                                    .parse(\n{\\\nORDERKEY\\\n:\\\n\n                                            + order\n                                            .get(\nORDERKEY\n) + \n\\\n}\n);\n\n                            order.put(\nlineitems\n, normalized_lineitem\n                                    .find(lineitemsBsonQuery));\n\n                            normalized_q3_new_joined_orders.insertOne(order);\n                        }\n                    });\n\n            String matchStringQuery = \n{\\\n$match\\\n:{\\\ncustomer.MKTSEGMENT\\\n:\\\nAUTOMOBILE\\\n,\\\nlineitems.SHIPDATE\\\n:{\\\n$gte\\\n: ISODate(\\\n1990-01-01T00:00:00.000Z\\\n) }}}\n;\n\n            String unWindStringQuery = \n{$unwind: \\\n$lineitems\\\n}\n;\n\n            String projectStringQuery = \n{\\\n$project\\\n:{\\\nORDERDATE\\\n:1,\\\nSHIPPRIORITY\\\n:1,\\\nlineitems.EXTENDEDPRICE\\\n:1,\\\nl_dis_min_1\\\n:{\\\n$subtract\\\n:[1,\\\n$lineitems.DISCOUNT\\\n]}}}\n;\n\n            String groupStringQuery = \n{\\\n$group\\\n:{\\\n_id\\\n:{\\\nORDERKEY\\\n:\\\n$ORDERKEY\\\n,\\\nORDERDATE\\\n:\\\n$ORDERDATE\\\n,\\\nSHIPPRIORITY\\\n:\\\n$SHIPPRIORITY\\\n},\\\nrevenue\\\n:{\\\n$sum\\\n:{\\\n$multiply\\\n:[\\\n$lineitems.EXTENDEDPRICE\\\n,\\\n$l_dis_min_1\\\n]}}}}\n;\n\n            String sortStringQuery = \n{\\\n$sort\\\n:{\\\nrevenue\\\n:1,\\\nORDERDATE\\\n:1}}\n;\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList\nBson\n aggregateQuery = new ArrayList\nBson\n();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = normalized_q3_new_joined_orders.aggregate(aggregateQuery);\n\n            MongoCursor\nDocument\n iterator = result.iterator();\n\n            ArrayList\nDocument\n results = new ArrayList\nDocument\n();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            normalized_q3_new_joined_orders.drop();\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIf you call the above query, you will get the results below:\n\n\n\n\nOrder Priority Checking Query (Q4)\n\n\nThis query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:\n\n\nselect\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate \n= date '[DATE]'\n and o_orderdate \n date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate \n l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;\n\n\n\n\nSimilarly since mongoDB doesn't support joins and since the aggregation in mongoDB happens only in the collection level, then to run an aggregate query against related documents we should first fetch all the related documents and put them in a temporary collection to be able to run later aggregate query on this newly created collection. We do that by querying first the order collection and fetch all the documents that meet the condition in the original sql query. Then for each lineitem objectID, we fetch the corresponding lineitem document and embed it inside the order document as shown below:\n\n\n BsonDocument bsonQuery = BsonDocument\n                    .parse(\n{\\\nORDERDATE\\\n: {\\\n$gte\\\n: ISODate(\\\n1990-01-01T00:00:00.000Z\\\n)},\\\nORDERDATE\\\n: {\\\n$lt\\\n: ISODate(\\\n2000-01-01T00:00:00.000Z\\\n)}}\n);\n\n            this.database.createCollection(\nnormalized_q4_new_joined_orders\n);\n\n            final MongoCollection\nDocument\n normalized_q4_new_joined_orders = this.database\n                    .getCollection(\nnormalized_q4_new_joined_orders\n);\n\n            this.normalized_order.find(bsonQuery).forEach(\n                    new Block\nDocument\n() {\n                        @Override\n                        public void apply(final Document order) {\n\n                            BsonDocument lineitemsBsonQuery = BsonDocument\n                                    .parse(\n{\\\nORDERKEY\\\n:\\\n\n                                            + order\n                                            .get(\nORDERKEY\n) + \n\\\n}\n);\n\n                            order.put(\nlineitems\n, normalized_lineitem\n                                    .find(lineitemsBsonQuery));\n\n                            normalized_q4_new_joined_orders.insertOne(order);\n                        }\n                    });\n\n\n\n\nNow we have a new collection called \"normalized_q4_new_joined_orders\" that will have all the documents needed to run the original sql aggregate query. After we run the aggregate query, we can drop the temporary collection.\n\n\nSimilar to Q1 and Q3, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages we will need are  a \"project\" stage, a \"match\" stage, a \"group\" stage, and a \"sort\" stage. The \"project\" is used at the beginning to compare the lineitem COMMITDATE with the RECEIPTDATE and returned a 0 or 1 based on the result of the comparison. Then this value can be used in the following match stage which is shown below:\n\n\nThe first project stage:\n\n\n{  \n   \n$project\n:{  \n      \nORDERDATE\n:1,\n      \nORDERPRIORITY\n:1,\n      \neq\n:{  \n         \n$cond\n:[  \n            {  \n               \n$lt\n:[  \n                  \n$lineitems.COMMITDATE\n,\n                  \n$lineitems.RECEIPTDATE\n\n               ]\n            },\n            0,\n            1\n         ]\n      }\n   }\n}\n\n\n\n\nThe following match stage:\n\n\n{  \n   \n$match\n:{  \n      \neq\n:{  \n         \n$eq\n:1\n      }\n   }\n}\n\n\n\n\nThe group stage:\n\n\n{  \n   \n$group\n:{  \n      \n_id\n:{  \n         \nORDERPRIORITY\n:\n$ORDERPRIORITY\n\n      },\n      \norder_count\n:{  \n         \n$sum\n:1\n      }\n   }\n}\n\n\n\n\nFinally the sort stage:\n\n\n{  \n   \n$sort\n:{  \n      \nORDERPRIORITY\n:1\n   }\n}\n\n\n\n\nThe complete query in mongoDB is shown below:\n\n\n[  \n   {  \n      \n$project\n:{  \n         \nORDERDATE\n:1,\n         \nORDERPRIORITY\n:1,\n         \neq\n:{  \n            \n$cond\n:[  \n               {  \n                  \n$lt\n:[  \n                     \n$lineitems.COMMITDATE\n,\n                     \n$lineitems.RECEIPTDATE\n\n                  ]\n               },\n               0,\n               1\n            ]\n         }\n      }\n   },\n   {  \n      \n$match\n:{  \n         \neq\n:{  \n            \n$eq\n:1\n         }\n      }\n   },\n   {  \n      \n$group\n:{  \n         \n_id\n:{  \n            \nORDERPRIORITY\n:\n$ORDERPRIORITY\n\n         },\n         \norder_count\n:{  \n            \n$sum\n:1\n         }\n      }\n   },\n   {  \n      \n$sort\n:{  \n         \nORDERPRIORITY\n:1\n      }\n   }\n]\n\n\n\n\nThe complete java code for the api that will return the results of TPCH Q4 is shown below:\n\n\n    @GET\n    @Path(\n/q4\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q4 using this model\n, notes = \nReturns mongoDB document(s)\n, response = Document.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public ArrayList\nDocument\n getQ4Results() {\n\n        AggregateIterable\nDocument\n result;\n\n        try {\n\n            BsonDocument bsonQuery = BsonDocument\n                    .parse(\n{\\\nORDERDATE\\\n: {\\\n$gte\\\n: ISODate(\\\n1990-01-01T00:00:00.000Z\\\n)},\\\nORDERDATE\\\n: {\\\n$lt\\\n: ISODate(\\\n2000-01-01T00:00:00.000Z\\\n)}}\n);\n\n            this.database.createCollection(\nnormalized_q4_new_joined_orders\n);\n\n            final MongoCollection\nDocument\n normalized_q4_new_joined_orders = this.database\n                    .getCollection(\nnormalized_q4_new_joined_orders\n);\n\n            this.normalized_order.find(bsonQuery).forEach(\n                    new Block\nDocument\n() {\n                        @Override\n                        public void apply(final Document order) {\n\n                            BsonDocument lineitemsBsonQuery = BsonDocument\n                                    .parse(\n{\\\nORDERKEY\\\n:\\\n\n                                            + order\n                                            .get(\nORDERKEY\n) + \n\\\n}\n);\n\n                            order.put(\nlineitems\n, normalized_lineitem\n                                    .find(lineitemsBsonQuery));\n\n                            normalized_q4_new_joined_orders.insertOne(order);\n                        }\n                    });\n\n            String projectStringQuery = \n{\\\n$project\\\n:{\\\nORDERDATE\\\n:1,\\\nORDERPRIORITY\\\n:1,\\\neq\\\n:{\\\n$cond\\\n:[{\\\n$lt\\\n:[\\\n$lineitems.COMMITDATE\\\n,\\\n$lineitems.RECEIPTDATE\\\n]},0,1]}}}\n;\n\n            String matchStringQuery = \n{\\\n$match\\\n:{\\\neq\\\n:{\\\n$eq\\\n:1}}}\n;\n\n            String groupStringQuery = \n{\\\n$group\\\n:{\\\n_id\\\n:{\\\nORDERPRIORITY\\\n:\\\n$ORDERPRIORITY\\\n},\\\norder_count\\\n:{\\\n$sum\\\n:1}}}\n;\n\n            String sortStringQuery = \n{\\\n$sort\\\n:{\\\nORDERPRIORITY\\\n:1}}\n;\n\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList\nBson\n aggregateQuery = new ArrayList\nBson\n();\n\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = normalized_q4_new_joined_orders.aggregate(aggregateQuery);\n\n            MongoCursor\nDocument\n iterator = result.iterator();\n\n            ArrayList\nDocument\n results = new ArrayList\nDocument\n();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            normalized_q4_new_joined_orders.drop();\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIf you call the above query, you will get the results below:", 
            "title": "Normalized Model"
        }, 
        {
            "location": "/MongoDB/Examples/Normalized Model/#pricing-summary-report-query-q1", 
            "text": "This query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:  select\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate  = date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;  Since the above query needs to be run only against the lineitem table, then we can easily run the corresponding query in MongoDB agains the the lineitem collection.  To execute the above query in MongoDB, we will use the pipeline aggregation framework. We will divide the above query to different stages then execute them all at once. The stages that we will need are a \"match\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that are having the lineitem shipdate less than or equal some date value or the sql query part shown below:  where\n   l_shipdate  = date '1998-12-01' - interval '[DELTA]' day (3)  The equivalent match stage is shown below:  {  \n    $match :{  \n       SHIPDATE :{  \n          $lte :ISODate( 2016-01-01T00:00:00.000Z )\n      }\n   }\n}  Then we can run the project stage which will return only the attributes that we need as per the original sql query. The project stage query is shown below:  {  \n    $project :{  \n       RETURNFLAG :1,\n       LINESTATUS :1,\n       QUANTITY :1,\n       EXTENDEDPRICE :1,\n       DISCOUNT :1,\n       l_dis_min_1 :{  \n          $subtract :[  \n            1,\n             $DISCOUNT \n         ]\n      },\n       l_tax_plus_1 :{  \n          $add :[  \n             $TAX ,\n            1\n         ]\n      }\n   }\n}  The above stage will return the attributes that we need to use in the following stages. We can also perform any required mathematical operations on any of the attributes.  Then we can perform the group stage which will group the results by certain attributes as shown below:  {  \n    $group :{  \n       _id :{  \n          RETURNFLAG : $RETURNFLAG ,\n          LINESTATUS : $LINESTATUS \n      },\n       sum_qty :{  \n          $sum : $QUANTITY \n      },\n       sum_base_price :{  \n          $sum : $EXTENDEDPRICE \n      },\n       sum_disc_price :{  \n          $sum :{  \n             $multiply :[  \n                $EXTENDEDPRICE ,\n                $l_dis_min_1 \n            ]\n         }\n      },\n       sum_charge :{  \n          $sum :{  \n             $multiply :[  \n                $EXTENDEDPRICE ,\n               {  \n                   $multiply :[  \n                      $l_tax_plus_1 ,\n                      $l_dis_min_1 \n                  ]\n               }\n            ]\n         }\n      },\n       avg_price :{  \n          $avg : $EXTENDEDPRICE \n      },\n       avg_disc :{  \n          $avg : $DISCOUNT \n      },\n       count_order :{  \n          $sum :1\n      }\n   }\n}  Finally we sort the results using the sort stage as shown below:  {  \n    $sort :{  \n       RETURNFLAG :1,\n       LINESTATUS :1\n   }\n}  The complete query in mongoDB is shown below:  [  \n   {  \n       $match :{  \n          SHIPDATE :{  \n             $lte : ISODate( 2016-01-01T00:00:00.000Z )\n         }\n      }\n   },\n   {  \n       $project :{  \n          RETURNFLAG :1,\n          LINESTATUS :1,\n          QUANTITY :1,\n          EXTENDEDPRICE :1,\n          DISCOUNT :1,\n          l_dis_min_1 :{  \n             $subtract :[  \n               1,\n                $DISCOUNT \n            ]\n         },\n          l_tax_plus_1 :{  \n             $add :[  \n                $TAX ,\n               1\n            ]\n         }\n      }\n   },\n   {  \n       $group :{  \n          _id :{  \n             RETURNFLAG : $RETURNFLAG ,\n             LINESTATUS : $LINESTATUS \n         },\n          sum_qty :{  \n             $sum : $QUANTITY \n         },\n          sum_base_price :{  \n             $sum : $EXTENDEDPRICE \n         },\n          sum_disc_price :{  \n             $sum :{  \n                $multiply :[  \n                   $EXTENDEDPRICE ,\n                   $l_dis_min_1 \n               ]\n            }\n         },\n          sum_charge :{  \n             $sum :{  \n                $multiply :[  \n                   $EXTENDEDPRICE ,\n                  {  \n                      $multiply :[  \n                         $l_tax_plus_1 ,\n                         $l_dis_min_1 \n                     ]\n                  }\n               ]\n            }\n         },\n          avg_price :{  \n             $avg : $EXTENDEDPRICE \n         },\n          avg_disc :{  \n             $avg : $DISCOUNT \n         },\n          count_order :{  \n             $sum :1\n         }\n      }\n   },\n   {  \n       $sort :{  \n          RETURNFLAG :1,\n          LINESTATUS :1\n      }\n   }\n]  The complete java code for the api that will return the results of TPCH Q1 is shown below:      @GET\n    @Path( /q1 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q1 using this model , notes =  Returns mongoDB document(s) , response = Document.class, responseContainer =  list )\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public ArrayList Document  getQ1Results() {\n\n        AggregateIterable Document  result;\n\n        try {\n\n            String matchStringQuery =  {\\ $match\\ :{\\ SHIPDATE\\ :{\\ $lte\\ :ISODate(\\ 2016-01-01T00:00:00.000Z\\ )}}} ;\n\n            String projectStringQuery =  {\\ $project\\ :{\\ RETURNFLAG\\ :1,\\ LINESTATUS\\ :1,\\ QUANTITY\\ :1,\\ EXTENDEDPRICE\\ :1,\\ DISCOUNT\\ :1,\\ l_dis_min_1\\ :{\\ $subtract\\ :[1,\\ $DISCOUNT\\ ]},\\ l_tax_plus_1\\ :{\\ $add\\ :[\\ $TAX\\ ,1]}}} ;\n\n            String groupStringQuery =  {\\ $group\\ :{\\ _id\\ :{\\ RETURNFLAG\\ :\\ $RETURNFLAG\\ ,\\ LINESTATUS\\ :\\ $LINESTATUS\\ },\\ sum_qty\\ :{\\ $sum\\ :\\ $QUANTITY\\ },\\ sum_base_price\\ :{\\ $sum\\ :\\ $EXTENDEDPRICE\\ },\\ sum_disc_price\\ :{\\ $sum\\ :{\\ $multiply\\ :[\\ $EXTENDEDPRICE\\ ,\\ $l_dis_min_1\\ ]}},\\ sum_charge\\ :{\\ $sum\\ :{\\ $multiply\\ :[\\ $EXTENDEDPRICE\\ ,{\\ $multiply\\ :[\\ $l_tax_plus_1\\ ,\\ $l_dis_min_1\\ ]}]}},\\ avg_price\\ :{\\ $avg\\ :\\ $EXTENDEDPRICE\\ },\\ avg_disc\\ :{\\ $avg\\ :\\ $DISCOUNT\\ },\\ count_order\\ :{\\ $sum\\ :1}}} ;\n\n            String sortStringQuery =  {\\ $sort\\ :{\\ RETURNFLAG\\ :1,\\ LINESTATUS\\ :1}} ;\n\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList Bson  aggregateQuery = new ArrayList Bson ();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = this.normalized_lineitem.aggregate(aggregateQuery);\n\n            MongoCursor Document  iterator = result.iterator();\n\n            ArrayList Document  results = new ArrayList Document ();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above query, you will get the results below:", 
            "title": "Pricing Summary Report Query (Q1)"
        }, 
        {
            "location": "/MongoDB/Examples/Normalized Model/#shipping-priority-query-q3", 
            "text": "This query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:  select\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate   date '[DATE]'\n and l_shipdate   date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;  Since mongoDB doesn't support joins and since the aggregation in mongoDB happens only in the collection level, then to run an aggregate query against related documents we should first fetch all the related documents and put them in a temporary collection to be able to run later aggregate query on this newly created collection. We do that by querying first the order collection and fetch all the documents that meet the condition \"o_orderdate   date '[DATE]'\". Then for each lineitem objectID, we fetch the corresponding lineitem document and embed it inside the order document. We do the same for the customer document as shown below:   BsonDocument bsonQuery = BsonDocument\n                    .parse( {\\ ORDERDATE\\ :{\\ $lte\\ :ISODate(\\ 2016-01-01T00:00:00.000Z\\ ) }} );\n\n            this.database.createCollection( normalized_q3_new_joined_orders );\n\n            final MongoCollection Document  normalized_q3_new_joined_orders = this.database\n                    .getCollection( normalized_q3_new_joined_orders );\n\n            this.normalized_order.find(bsonQuery).forEach(\n                    new Block Document () {\n                        @Override\n                        public void apply(final Document order) {\n                            BsonDocument customerBsonQuery = BsonDocument\n                                    .parse( {\\ CUSTKEY\\ :\\ \n                                            + order.get( CUSTKEY ) +  \\ } );\n\n                            order.put( customer ,\n                                    normalized_customer.find(customerBsonQuery)\n                                            .first());\n\n                            BsonDocument lineitemsBsonQuery = BsonDocument\n                                    .parse( {\\ ORDERKEY\\ :\\ \n                                            + order\n                                            .get( ORDERKEY ) +  \\ } );\n\n                            order.put( lineitems , normalized_lineitem\n                                    .find(lineitemsBsonQuery));\n\n                            normalized_q3_new_joined_orders.insertOne(order);\n                        }\n                    });  Now we have a new collection called \"normalized_q3_new_joined_orders\" that will have all the documents needed to run the original sql aggregate query. After we run the aggregate query, we can drop the temporary collection.  Similar to Q1, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages that we will need are a \"match\" stage, an \"unwind\" stage, a \"project\" stage, a \"group\" stage, and a \"sort\" stage. The \"match\" stage is used to get all the documents that corresponds to the sql query part shown below:  where\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate   date '[DATE]'\n and l_shipdate   date '[DATE]'  Except that no need to query the join conditions since we have already represented the relationship between the different objects by embedding documents. The match stage query is shown below:  {  \n    $match :{  \n       customer.MKTSEGMENT : AUTOMOBILE ,\n       lineitems.SHIPDATE :{  \n          $gte : ISODate( 1990-01-01T00:00:00.000Z )\n      }\n   }\n}  Similarly the unwind stage:  {  \n    $unwind : $lineitems \n}  The project stage:  {  \n    $project :{  \n       ORDERDATE :1,\n       SHIPPRIORITY :1,\n       lineitems.EXTENDEDPRICE :1,\n       l_dis_min_1 :{  \n          $subtract :[  \n            1,\n             $lineitems.DISCOUNT \n         ]\n      }\n   }\n}  The group stage:  {  \n    $group :{  \n       _id :{  \n          ORDERKEY : $ORDERKEY ,\n          ORDERDATE : $ORDERDATE ,\n          SHIPPRIORITY : $SHIPPRIORITY \n      },\n       revenue :{  \n          $sum :{  \n             $multiply :[  \n                $lineitems.EXTENDEDPRICE ,\n                $l_dis_min_1 \n            ]\n         }\n      }\n   }\n}  Finally the sort stage:  {  \n    $sort :{  \n       revenue :1,\n       ORDERDATE :1\n   }\n}  The complete query in mongoDB is shown below:  [  \n   {  \n       $match :{  \n          customer.MKTSEGMENT : AUTOMOBILE ,\n          lineitems.SHIPDATE :{  \n             $gte : ISODate( 1990-01-01T00:00:00.000Z )\n         }\n      }\n   },\n   {  \n       $unwind : $lineitems \n   },\n   {  \n       $project :{  \n          ORDERDATE :1,\n          SHIPPRIORITY :1,\n          lineitems.EXTENDEDPRICE :1,\n          l_dis_min_1 :{  \n             $subtract :[  \n               1,\n                $lineitems.DISCOUNT \n            ]\n         }\n      }\n   },\n   {  \n       $group :{  \n          _id :{  \n             ORDERKEY : $ORDERKEY ,\n             ORDERDATE : $ORDERDATE ,\n             SHIPPRIORITY : $SHIPPRIORITY \n         },\n          revenue :{  \n             $sum :{  \n                $multiply :[  \n                   $lineitems.EXTENDEDPRICE ,\n                   $l_dis_min_1 \n               ]\n            }\n         }\n      }\n   },\n   {  \n       $sort :{  \n          revenue :1,\n          ORDERDATE :1\n      }\n   }\n]  The complete java code for the api that will return the results of TPCH Q3 is shown below:       @GET\n    @Path( /q3 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q3 using this model , notes =  Returns mongoDB document(s) , response = Document.class, responseContainer =  list )\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public ArrayList Document  getQ3Results() {\n\n        AggregateIterable Document  result;\n\n        try {\n\n            BsonDocument bsonQuery = BsonDocument\n                    .parse( {\\ ORDERDATE\\ :{\\ $lte\\ :ISODate(\\ 2016-01-01T00:00:00.000Z\\ ) }} );\n\n            this.database.createCollection( normalized_q3_new_joined_orders );\n\n            final MongoCollection Document  normalized_q3_new_joined_orders = this.database\n                    .getCollection( normalized_q3_new_joined_orders );\n\n            this.normalized_order.find(bsonQuery).forEach(\n                    new Block Document () {\n                        @Override\n                        public void apply(final Document order) {\n                            BsonDocument customerBsonQuery = BsonDocument\n                                    .parse( {\\ CUSTKEY\\ :\\ \n                                            + order.get( CUSTKEY ) +  \\ } );\n\n                            order.put( customer ,\n                                    normalized_customer.find(customerBsonQuery)\n                                            .first());\n\n                            BsonDocument lineitemsBsonQuery = BsonDocument\n                                    .parse( {\\ ORDERKEY\\ :\\ \n                                            + order\n                                            .get( ORDERKEY ) +  \\ } );\n\n                            order.put( lineitems , normalized_lineitem\n                                    .find(lineitemsBsonQuery));\n\n                            normalized_q3_new_joined_orders.insertOne(order);\n                        }\n                    });\n\n            String matchStringQuery =  {\\ $match\\ :{\\ customer.MKTSEGMENT\\ :\\ AUTOMOBILE\\ ,\\ lineitems.SHIPDATE\\ :{\\ $gte\\ : ISODate(\\ 1990-01-01T00:00:00.000Z\\ ) }}} ;\n\n            String unWindStringQuery =  {$unwind: \\ $lineitems\\ } ;\n\n            String projectStringQuery =  {\\ $project\\ :{\\ ORDERDATE\\ :1,\\ SHIPPRIORITY\\ :1,\\ lineitems.EXTENDEDPRICE\\ :1,\\ l_dis_min_1\\ :{\\ $subtract\\ :[1,\\ $lineitems.DISCOUNT\\ ]}}} ;\n\n            String groupStringQuery =  {\\ $group\\ :{\\ _id\\ :{\\ ORDERKEY\\ :\\ $ORDERKEY\\ ,\\ ORDERDATE\\ :\\ $ORDERDATE\\ ,\\ SHIPPRIORITY\\ :\\ $SHIPPRIORITY\\ },\\ revenue\\ :{\\ $sum\\ :{\\ $multiply\\ :[\\ $lineitems.EXTENDEDPRICE\\ ,\\ $l_dis_min_1\\ ]}}}} ;\n\n            String sortStringQuery =  {\\ $sort\\ :{\\ revenue\\ :1,\\ ORDERDATE\\ :1}} ;\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument unWindBsonQuery = BsonDocument\n                    .parse(unWindStringQuery);\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList Bson  aggregateQuery = new ArrayList Bson ();\n\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(unWindBsonQuery);\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = normalized_q3_new_joined_orders.aggregate(aggregateQuery);\n\n            MongoCursor Document  iterator = result.iterator();\n\n            ArrayList Document  results = new ArrayList Document ();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            normalized_q3_new_joined_orders.drop();\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above query, you will get the results below:", 
            "title": "Shipping Priority Query (Q3)"
        }, 
        {
            "location": "/MongoDB/Examples/Normalized Model/#order-priority-checking-query-q4", 
            "text": "This query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:  select\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate  = date '[DATE]'\n and o_orderdate   date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate   l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;  Similarly since mongoDB doesn't support joins and since the aggregation in mongoDB happens only in the collection level, then to run an aggregate query against related documents we should first fetch all the related documents and put them in a temporary collection to be able to run later aggregate query on this newly created collection. We do that by querying first the order collection and fetch all the documents that meet the condition in the original sql query. Then for each lineitem objectID, we fetch the corresponding lineitem document and embed it inside the order document as shown below:   BsonDocument bsonQuery = BsonDocument\n                    .parse( {\\ ORDERDATE\\ : {\\ $gte\\ : ISODate(\\ 1990-01-01T00:00:00.000Z\\ )},\\ ORDERDATE\\ : {\\ $lt\\ : ISODate(\\ 2000-01-01T00:00:00.000Z\\ )}} );\n\n            this.database.createCollection( normalized_q4_new_joined_orders );\n\n            final MongoCollection Document  normalized_q4_new_joined_orders = this.database\n                    .getCollection( normalized_q4_new_joined_orders );\n\n            this.normalized_order.find(bsonQuery).forEach(\n                    new Block Document () {\n                        @Override\n                        public void apply(final Document order) {\n\n                            BsonDocument lineitemsBsonQuery = BsonDocument\n                                    .parse( {\\ ORDERKEY\\ :\\ \n                                            + order\n                                            .get( ORDERKEY ) +  \\ } );\n\n                            order.put( lineitems , normalized_lineitem\n                                    .find(lineitemsBsonQuery));\n\n                            normalized_q4_new_joined_orders.insertOne(order);\n                        }\n                    });  Now we have a new collection called \"normalized_q4_new_joined_orders\" that will have all the documents needed to run the original sql aggregate query. After we run the aggregate query, we can drop the temporary collection.  Similar to Q1 and Q3, we will divide the above query to different stages then execute them all at once using the mongoDB aggregation pipeline framework. The stages we will need are  a \"project\" stage, a \"match\" stage, a \"group\" stage, and a \"sort\" stage. The \"project\" is used at the beginning to compare the lineitem COMMITDATE with the RECEIPTDATE and returned a 0 or 1 based on the result of the comparison. Then this value can be used in the following match stage which is shown below:  The first project stage:  {  \n    $project :{  \n       ORDERDATE :1,\n       ORDERPRIORITY :1,\n       eq :{  \n          $cond :[  \n            {  \n                $lt :[  \n                   $lineitems.COMMITDATE ,\n                   $lineitems.RECEIPTDATE \n               ]\n            },\n            0,\n            1\n         ]\n      }\n   }\n}  The following match stage:  {  \n    $match :{  \n       eq :{  \n          $eq :1\n      }\n   }\n}  The group stage:  {  \n    $group :{  \n       _id :{  \n          ORDERPRIORITY : $ORDERPRIORITY \n      },\n       order_count :{  \n          $sum :1\n      }\n   }\n}  Finally the sort stage:  {  \n    $sort :{  \n       ORDERPRIORITY :1\n   }\n}  The complete query in mongoDB is shown below:  [  \n   {  \n       $project :{  \n          ORDERDATE :1,\n          ORDERPRIORITY :1,\n          eq :{  \n             $cond :[  \n               {  \n                   $lt :[  \n                      $lineitems.COMMITDATE ,\n                      $lineitems.RECEIPTDATE \n                  ]\n               },\n               0,\n               1\n            ]\n         }\n      }\n   },\n   {  \n       $match :{  \n          eq :{  \n             $eq :1\n         }\n      }\n   },\n   {  \n       $group :{  \n          _id :{  \n             ORDERPRIORITY : $ORDERPRIORITY \n         },\n          order_count :{  \n             $sum :1\n         }\n      }\n   },\n   {  \n       $sort :{  \n          ORDERPRIORITY :1\n      }\n   }\n]  The complete java code for the api that will return the results of TPCH Q4 is shown below:      @GET\n    @Path( /q4 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q4 using this model , notes =  Returns mongoDB document(s) , response = Document.class, responseContainer =  list )\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public ArrayList Document  getQ4Results() {\n\n        AggregateIterable Document  result;\n\n        try {\n\n            BsonDocument bsonQuery = BsonDocument\n                    .parse( {\\ ORDERDATE\\ : {\\ $gte\\ : ISODate(\\ 1990-01-01T00:00:00.000Z\\ )},\\ ORDERDATE\\ : {\\ $lt\\ : ISODate(\\ 2000-01-01T00:00:00.000Z\\ )}} );\n\n            this.database.createCollection( normalized_q4_new_joined_orders );\n\n            final MongoCollection Document  normalized_q4_new_joined_orders = this.database\n                    .getCollection( normalized_q4_new_joined_orders );\n\n            this.normalized_order.find(bsonQuery).forEach(\n                    new Block Document () {\n                        @Override\n                        public void apply(final Document order) {\n\n                            BsonDocument lineitemsBsonQuery = BsonDocument\n                                    .parse( {\\ ORDERKEY\\ :\\ \n                                            + order\n                                            .get( ORDERKEY ) +  \\ } );\n\n                            order.put( lineitems , normalized_lineitem\n                                    .find(lineitemsBsonQuery));\n\n                            normalized_q4_new_joined_orders.insertOne(order);\n                        }\n                    });\n\n            String projectStringQuery =  {\\ $project\\ :{\\ ORDERDATE\\ :1,\\ ORDERPRIORITY\\ :1,\\ eq\\ :{\\ $cond\\ :[{\\ $lt\\ :[\\ $lineitems.COMMITDATE\\ ,\\ $lineitems.RECEIPTDATE\\ ]},0,1]}}} ;\n\n            String matchStringQuery =  {\\ $match\\ :{\\ eq\\ :{\\ $eq\\ :1}}} ;\n\n            String groupStringQuery =  {\\ $group\\ :{\\ _id\\ :{\\ ORDERPRIORITY\\ :\\ $ORDERPRIORITY\\ },\\ order_count\\ :{\\ $sum\\ :1}}} ;\n\n            String sortStringQuery =  {\\ $sort\\ :{\\ ORDERPRIORITY\\ :1}} ;\n\n\n            BsonDocument projectBsonQuery = BsonDocument\n                    .parse(projectStringQuery);\n\n            BsonDocument matchBsonQuery = BsonDocument.parse(matchStringQuery);\n\n            BsonDocument groupBsonQuery = BsonDocument.parse(groupStringQuery);\n\n            BsonDocument sortBsonQuery = BsonDocument.parse(sortStringQuery);\n\n\n            ArrayList Bson  aggregateQuery = new ArrayList Bson ();\n\n            aggregateQuery.add(projectBsonQuery);\n            aggregateQuery.add(matchBsonQuery);\n            aggregateQuery.add(groupBsonQuery);\n            aggregateQuery.add(sortBsonQuery);\n\n            result = normalized_q4_new_joined_orders.aggregate(aggregateQuery);\n\n            MongoCursor Document  iterator = result.iterator();\n\n            ArrayList Document  results = new ArrayList Document ();\n            while (iterator.hasNext()) {\n                Document resultDoc = iterator.next();\n                results.add(resultDoc);\n            }\n\n            normalized_q4_new_joined_orders.drop();\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above query, you will get the results below:", 
            "title": "Order Priority Checking Query (Q4)"
        }, 
        {
            "location": "/MongoDB/Examples/TPC-H Queries/", 
            "text": "In this example, I will show how we can model the data of a complete B2C application and how to write complex queries on this data model. I am going to use the data model used by the \nTPC-H benchmark\n which is shown below.\n\n\n\n\nThe idea behind this example is to show how to use MongoDB to design a real B2C application data model and how to write advance SQL queries with joins, grouping and sorting using MongoDB query language. I have chosen three queries from the TPCH benchmark that I think will cover most of the common query capabilities of SQL. The chosen queries are Q1, Q3 and Q4 as will be explained in the following sections.\n\n\nThe data used as input for this example is a small set from the data generated using TPCH DBGEN tool. The data is stored in CSV files that correspond to each object in the TPCH benchmark schema shown above (customer, supplier, part, lineitem, order , etc ...). For more details about how to generate the data, please have a look at \nthis\n blog post. \n\n\nThe complete code of this example can be found in Github along with the steps on how to run it. You can also change the used input data by changing the content of the \ninput files\n stored in the \"data\" folder.\n\n\nTo model this schema using MongoDB, we can either use a denormalised model,a normalised model or a combination of both. To show the impact of data modelling in queries expressiveness, I have modelled the schema using all three models:\n\n\n\n\nA fully denormalised model\n.\n\n\nA fully normalised model\n.\n\n\nA mix of normalised and denormalised models\n. \n\n\n\n\nWe can see from the above three models that we can use multiple ways to model our data in MongoDB. However, the way you model your data will impact your queries. For example, using the fully normalised way, it was difficult to run aggregate queries and we had to create a temporary collection for each query since mongoDB supports only aggregation on the collection level. Using the fully denormalised way provided us with more flexibility to run aggregate queries since we run them against the same collection. However, using the denormalised model might lead to issues when the document size increases to a very large size. In the above data model, it doesn't seem that the document can increase to an unbound size even if the order contain many lineitems. MongoDB supports up to 16MB for a single document which is I think enough for the above use case. Using the mixed approach was ok for some aggregation queries (Q1 and Q4) but we had to go and create a temporary collection to run Q3 due to the relationship between the order and the customer which was referenced and not embedded in the order document.", 
            "title": "TPC H Queries"
        }, 
        {
            "location": "/MongoDB/Query Model/Aggregation and Filtering/", 
            "text": "In many use cases, we will need a way to aggregate values across multiple documents, compute some operations on the aggregated values and then return results.  MongoDB supports three ways to do just that, either through the pipeline aggregation framework, or by using the map reduce data processing paradigm or through some supported uses-specific functions such as distinct, group and count functions that have been already explained in \nprevious section.\n\n\nIn the following sections, I will show briefly how to group and filter your data using the pipeline framework and the map reduce data processing model.\n\n\nAggregation pipeline framework.\n\n\nThe idea behind the aggregation pipeline framework is that the data is processed through multiple stages before a final results is returned. The pipeline works on the collection level as most of MongoDB methods. MongoDB supports many stages to group, match, filter and sort the data such as $project, $match, $group, $sort and $sample stages. For the complete list of all the supported stages, please have a look at \nMongoDB documentation.\n\n\nMongoDB supports the db.collection.aggregate() method that operates on a single collection and can take advantage of the already created indexes to enhance performance.\n\n\nTo show how the aggregation framework could be useful, I will explain how to use it to get the price sum of all orders grouped by the order status and consider only orders with price more than 10. The aggregation query is shown below:\n\n\ndb.orders.aggregate( [\n   { $match: { orderPrice: { $gte: 10 } } } ,\n   { $group: { _id: \n$orderStatus\n, priceSum: { $sum: \n$orderPrice\n } } }\n] )\n\n\n\n\nIn the above example, the first stage is the $match stage which acts as a query filter to return only the documents that match the condition. Then the second stage is the $group stage which will group orders by their order status and the sum of all order price per group. The returned result will be as below:\n\n\n{\n_id: \nCompleted\n,\npriceSum: \n129000\n\n},\n{\n_id: \nOpen\n,\npriceSum: \n20000\n\n},\n{\n_id: \nShipped\n,\npriceSum: \n25360\n\n},\n{\n_id: \nCancalled\n,\npriceSum: \n2300\n\n}\n\n\n\n\nMap-Reduce\n\n\nIn general, using the aggregation pipeline framework is the more preferred way to do aggregation related tasks since it is less complex and more efficient than using map-reduce. However map-reduce provides some sort for flexibility since it is using custom javascript functions to do the map and reduce operations.\n\n\nThe map-reduce is basically a two phases operations, the map operation is used to process each document and emit some key-value records that are consumed by the reduce operation to compute and return some results. There is an optional finalise stage to do some final modification the returned results if needed.\n\n\nMongoDB provides the mapReduce database command that can be executed on the collection level. I will show below how to implement the same example we used previously in the aggregation pipeline framework section using map-reduce:\n\n\ndb.orders.mapReduce(\n                     function(){ emit( this.orderStatus,this.orderPrice)},\n                     function(key,values){ return Array.sum(values)},\n                     { query: {orderPrice: {$gte: 10} },\n                       out: \norders_status_totals\n \n                     }\n                   )\n\n\n\n\nIn the above example, we will first execute the query part to get all products with price greater than or equal to 10. Then these matched documents will be processed by the map operation to get the order status and all order prices with this status. Finally the reduce function will sum all the order prices for each order status and return result like below:\n\n\n{\n_id: \nCompleted\n,\nvalue: \n129000\n\n},\n{\n_id: \nOpen\n,\nvalue: \n20000\n\n},\n{\n_id: \nShipped\n,\nvalue: \n25360\n\n},\n{\n_id: \nCancalled\n,\nvalue: \n2300\n\n}", 
            "title": "Aggregation and Filtering"
        }, 
        {
            "location": "/MongoDB/Query Model/Aggregation and Filtering/#aggregation-pipeline-framework", 
            "text": "The idea behind the aggregation pipeline framework is that the data is processed through multiple stages before a final results is returned. The pipeline works on the collection level as most of MongoDB methods. MongoDB supports many stages to group, match, filter and sort the data such as $project, $match, $group, $sort and $sample stages. For the complete list of all the supported stages, please have a look at  MongoDB documentation.  MongoDB supports the db.collection.aggregate() method that operates on a single collection and can take advantage of the already created indexes to enhance performance.  To show how the aggregation framework could be useful, I will explain how to use it to get the price sum of all orders grouped by the order status and consider only orders with price more than 10. The aggregation query is shown below:  db.orders.aggregate( [\n   { $match: { orderPrice: { $gte: 10 } } } ,\n   { $group: { _id:  $orderStatus , priceSum: { $sum:  $orderPrice  } } }\n] )  In the above example, the first stage is the $match stage which acts as a query filter to return only the documents that match the condition. Then the second stage is the $group stage which will group orders by their order status and the sum of all order price per group. The returned result will be as below:  {\n_id:  Completed ,\npriceSum:  129000 \n},\n{\n_id:  Open ,\npriceSum:  20000 \n},\n{\n_id:  Shipped ,\npriceSum:  25360 \n},\n{\n_id:  Cancalled ,\npriceSum:  2300 \n}", 
            "title": "Aggregation pipeline framework."
        }, 
        {
            "location": "/MongoDB/Query Model/Aggregation and Filtering/#map-reduce", 
            "text": "In general, using the aggregation pipeline framework is the more preferred way to do aggregation related tasks since it is less complex and more efficient than using map-reduce. However map-reduce provides some sort for flexibility since it is using custom javascript functions to do the map and reduce operations.  The map-reduce is basically a two phases operations, the map operation is used to process each document and emit some key-value records that are consumed by the reduce operation to compute and return some results. There is an optional finalise stage to do some final modification the returned results if needed.  MongoDB provides the mapReduce database command that can be executed on the collection level. I will show below how to implement the same example we used previously in the aggregation pipeline framework section using map-reduce:  db.orders.mapReduce(\n                     function(){ emit( this.orderStatus,this.orderPrice)},\n                     function(key,values){ return Array.sum(values)},\n                     { query: {orderPrice: {$gte: 10} },\n                       out:  orders_status_totals  \n                     }\n                   )  In the above example, we will first execute the query part to get all products with price greater than or equal to 10. Then these matched documents will be processed by the map operation to get the order status and all order prices with this status. Finally the reduce function will sum all the order prices for each order status and return result like below:  {\n_id:  Completed ,\nvalue:  129000 \n},\n{\n_id:  Open ,\nvalue:  20000 \n},\n{\n_id:  Shipped ,\nvalue:  25360 \n},\n{\n_id:  Cancalled ,\nvalue:  2300 \n}", 
            "title": "Map-Reduce"
        }, 
        {
            "location": "/MongoDB/Query Model/Full Text Search Support/", 
            "text": "MongoDB supports Full-Text Search using Text Indexes. Text Indexes can be defined on any string or array of strings values in your documents that you want them to be fully searched. By defining a text index on any field, MongoDB will tokenises and stems the field's text content and creates the index accordingly. \n\n\nTo create a text index on any field, you need to specify the \"text\" keyword. You can create only one text index per collection. If you want to index multiple fields of a document, you can create a compound text index. \n\n\nAs for any full text functionality, MongoDB text index assign some score to each document and then you can sort the search results by this score to show the more relevant results. You can also see the search score for each returned document by using the { $meta: \"textScore\" } expression.\n\n\nFor instance, if we want to search products by product name. We can create a text index on the product name field as shown below:\n\n\ndb.products.createIndex({\nproductName\n:\ntext\n})\n\n\n\n\nThen if you want to do a full text search in all the products having the keyword \"shirt\", we can run the below find query:\n\n\ndb.products.find({$text: {$search: \nshirt\n}}, {score: {$meta: \ntextScore\n}}).sort({score:{$meta:\ntextScore\n}})\n\n\n\n\nAs seen above, the query will sort results by their search relevancy and will return the score for each document.\n\n\nIn case you want to do a full-text search in the product names and descriptions, you can create a compound text index as seen below:\n\n\ndb.products.createIndex({\nproductName\n:\ntext\n, \nproductDesc\n:\ntext\n })\n\n\n\n\nThen when you search for a keyword, the product names and descriptions will be fully searched. \n\n\nIn some use cases, you might be interested to do a full-text search for all the fields of a document. Then you can use the Wildcard Indexing supported by MongoDB. An example would be if you want to index all the fields of an email document. Then you can do this as shown below:\n\n\ndb.emails.createIndex({\n$**\n:\ntext\n})\n\n\n\n\nIf you want to do a full text search on a certain phrase, you can either choose to do an OR for all the keywords inside the phrase or to do an AND for the keywords as shown below:\n\n\nBelow the result will be computed by doing an OR operation for all the keywords inside the phrase:\n\n\ndb.products.find({$text: {$search: \nnice branded shirt\n}}, {score: {$meta: \ntextScore\n}}).sort({score:{$meta:\ntextScore\n}})\n\n\n\n\nOr use double quotes in the search phrase then the result will be computed by doing an AND operation for all the keywords inside the phrase:\n\n\ndb.products.find({$text: {$search: \n\\\nnice branded shirt\\\n}}, {score: {$meta: \ntextScore\n}}).sort({score:{$meta:\ntextScore\n}})\n\n\n\n\nFinally, if you want to assign some weight for the text indexed fields in case you are having compound text index. Then you can assign the weights as shown below:\n\n\ndb.products.ceateIndex( {\nproductName\n: \ntext\n, \nproductDesc\n:\ntext\n }, {\nweights\n: { productName: 3, productDesc:1 }} )\n\n\n\n\nThen the score for the documents having the searched keyword in the product name field will be more than the documents having them in the product description field.", 
            "title": "Full Text Search Support"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/", 
            "text": "Indexing is a very important concept in any database to support efficient execution of queries. For instance, MongoDB must perform a full collection scan if we don't create index in one of the fields that we want to query. Indexes acts like the table of contents used to find a certain topic in a very large book. MongoDB indexes are created using the efficient B-tree data structure to store small portion of information used to speed up queries by limiting the number of documents to be searched. These information will be stored using the B tree structure in order useful to perform range queries or sorting. \n\n\nMongoDB supports a variety of index types that are defined on the collection level and can apply to any field or subfield in the document. MongoDB supports the following index types:\n\n\nSingle Field Index\n\n\nMongoDB supports creating indexes in any field or sub-field of a document inside a particular collection. By default, MongoDB creates a unique ascending single field index on the _id field of each document. This index acts like the primary key for a collection and can't be removed. You can create a single field index easily using the createIndex() method as seen below:\n\n\nProduct:\n\n\n{\nsku: \nSomeProductSku\n,\n\nname\n: \nProductName\n,\nsize: \n20\n,\nprice: \n200\n,\naddress:\n{\nstreet: \nStreetName\n\ncity: \nCity\n,\ncountry: \nCountry\n\n} \ntags: [\nStylish\n ,\nTop\n ,\nTShirt\n]\n},\nsold: [\ndate1\n,\ndate2\n,\ndate3\n]\n\n\n\n\nCreate index on the name field:\n\n\ndb.products.createIndex({\nname\n:1})\n\n\n\n\nIn the above example we created an index on the name field of the products collection. If you want to create an index on a sub-field, you can use the dot notation as seen below:\n\n\ndb.products.createIndex({\naddress.street\n:1})\n\n\n\n\nOr you can create an index on the whole sub-document as seen below:\n\n\ndb.products.createIndex({\naddress\n:1})\n\n\n\n\nThen you can run queries like below:\n\n\ndb.products.find({\nname\n: \nProductName\n })\n\n\n\n\ndb.products.find({\naddress.street\n: \nStreetName\n })\n\n\n\n\ndb.products.find({\naddress\n: {street:\nStreetName\n, city:\nCity\n, country: \nCountry\n} })\n\n\n\n\ndb.products.find({\naddress\n: {street:\nStreetName\n, city:\nCity\n, country: \nCountry\n} })\n\n\n\n\nCompound Index\n\n\nMongoDB supports also compound indexes where you can create index to hold references to more than one field inside a document within a particular collection. The order of the fields is important since it makes a difference in the returned results. For example if you create the below compound index on the price and size fields of the products collection:\n\n\ndb.products.createIndex({\nprice\n:1, \nsize\n: 1})\n\n\n\n\nThis means that the index will first sort the products by price then for each price, it will sort by size.  You can then get run a sort query like below:\n\n\ndb.products.sort({\nprice\n:1, \nsize\n: 1})\n\n\n\n\nMongoDB imposes a limit of 31 fields for each compound index.\n\n\nMultikey Indexes\n\n\nMondoDB supports also a multi-key indexes to index any contents stored inside arrays. A multi-key index will be created if you try to create an index in a field with array value.  MongoDB then will create index for each element inside the array. This type of indexes allows you to run queries on array of elements inside a document. An example would be of want to create an index on the tags field of the products collection as seen below:\n\n\ndb.products.createIndex({\ntags\n:1})\n\n\n\n\nIf you want to create a compound index that contains a field of an array type, please note that you can use at most one multi-key index in the compound index. For example, if you want to create a compound index using the tags and sold fields as shown below, you will get an error:\n\n\ndb.products.createIndex({\ntags\n:1,\nsold\n:1})\n\n\n\n\nThe above index won't be created since more than one multi-key index needs to be created.\n\n\nGeospatial Indexes\n\n\nMondoDB supports two type of Geospatial indexes to allow for efficient queries of geospatial data. The first index is called 2d indexes which is used for planner geometry data. The second is called 2 sphere indexes which is used for spherical geometry data.\n\n\nThese indexes stores your data as GeoJSON objects and supports many types such as point, polygon, multipoint and many others. For more information about how to use these types of indexes, please have a look at \nMongoDB documentations\n.\n\n\nText Index\n\n\nMongoDB supports text indexes to allow for queries that performs only search on string contents. You can create a text index easily by using the \"text\" keyword as shown below:\n\n\ndb.products.createIndex({\nname\n: \ntext\n})\n\n\n\n\nLike Multi-Key indexes, you are allowed to use only one text index inside the compound index. For more information about this index, please have a look at \nMongoDB documentations\n.\n\n\nHash Index\n\n\nHash index is used to store the hash value of the real values of the indexed field. If the indexed field is a sub-document, then it will collapse the whole document and compute the hash for all fields inside this sub-document. This index is used when you want to shard a collection by certain hash shard keys. This type of index doesn't support multi-key indexes. You can create a hash index using the \"hashed\" keyword as seen below:\n\n\ndb.products.createIndex( { name: \nhashed\n } )\n\n\n\n\nIndex Properties\n\n\nYou can define some properties for your index as explained below:\n\n\nUnique\n\n\nYou can define a certain index as unique which means that there should be no other document with the same indexed value, otherwise MongoDB will reject this document.  You can make any index Unique as seen below:\n\n\ndb.products.createIndex( { \nname\n: 1 }, { unique: true } )\n\n\n\n\nIf you do this, then any new created document with already existing name will be rejected.\n\n\nPartial\n\n\nThis is a new property added on MongoDB version 3.2 that can be used when creating indexes and specify certain filter so that this index will be applied only to the documents that match this filter. Example is below:\n\n\ndb.products.createIndex( { \nname\n: 1 }, { partialFilterExpression: { size: { $gt: 5 } } )\n\n\n\n\nThe above example will apply the index only on documents having size greater than 5.\n\n\nSparse\n\n\nThis property is like previous property but with only one filter criteria which is that we apply the index only if the indexed field exists. Example is given below:\n\n\ndb.products.createIndex( { \nname\n: 1 }, { sparse: true } )\n\n\n\n\nUsing this property will make the index applies only for documents that are having a name filed. \n\n\nTTL\n\n\nThis property makes the document automatically removed by MongoDB after a pre-defined time.  This index is very useful for certain types of data such as temporary data that shouldn't be persisted in database. Examples are logs or event data. Below is an example of how to use this property:\n\n\ndb.eventlog.createIndex( { \nlastModifiedDate\n: 1 }, { expireAfterSeconds: 3600 } )\n\n\n\n\nIn the above example, we are creating a TTL index on the lastModifiedDate field which will let MongoDB expire the document after 3600 seconds from the value of the lastModifiedDate fields.\n\n\nRemoving indexes\n\n\nTo remove an already created index, you can use the  db.collection.dropIndex() method.  For example if we want to remove the single field index we created on the name field of the products collection, we can run the below :\n\n\ndb.products.dropIndex({\nname\n:1})", 
            "title": "Indexing"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#single-field-index", 
            "text": "MongoDB supports creating indexes in any field or sub-field of a document inside a particular collection. By default, MongoDB creates a unique ascending single field index on the _id field of each document. This index acts like the primary key for a collection and can't be removed. You can create a single field index easily using the createIndex() method as seen below:  Product:  {\nsku:  SomeProductSku , name :  ProductName ,\nsize:  20 ,\nprice:  200 ,\naddress:\n{\nstreet:  StreetName \ncity:  City ,\ncountry:  Country \n} \ntags: [ Stylish  , Top  , TShirt ]\n},\nsold: [ date1 , date2 , date3 ]  Create index on the name field:  db.products.createIndex({ name :1})  In the above example we created an index on the name field of the products collection. If you want to create an index on a sub-field, you can use the dot notation as seen below:  db.products.createIndex({ address.street :1})  Or you can create an index on the whole sub-document as seen below:  db.products.createIndex({ address :1})  Then you can run queries like below:  db.products.find({ name :  ProductName  })  db.products.find({ address.street :  StreetName  })  db.products.find({ address : {street: StreetName , city: City , country:  Country } })  db.products.find({ address : {street: StreetName , city: City , country:  Country } })", 
            "title": "Single Field Index"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#compound-index", 
            "text": "MongoDB supports also compound indexes where you can create index to hold references to more than one field inside a document within a particular collection. The order of the fields is important since it makes a difference in the returned results. For example if you create the below compound index on the price and size fields of the products collection:  db.products.createIndex({ price :1,  size : 1})  This means that the index will first sort the products by price then for each price, it will sort by size.  You can then get run a sort query like below:  db.products.sort({ price :1,  size : 1})  MongoDB imposes a limit of 31 fields for each compound index.", 
            "title": "Compound Index"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#multikey-indexes", 
            "text": "MondoDB supports also a multi-key indexes to index any contents stored inside arrays. A multi-key index will be created if you try to create an index in a field with array value.  MongoDB then will create index for each element inside the array. This type of indexes allows you to run queries on array of elements inside a document. An example would be of want to create an index on the tags field of the products collection as seen below:  db.products.createIndex({ tags :1})  If you want to create a compound index that contains a field of an array type, please note that you can use at most one multi-key index in the compound index. For example, if you want to create a compound index using the tags and sold fields as shown below, you will get an error:  db.products.createIndex({ tags :1, sold :1})  The above index won't be created since more than one multi-key index needs to be created.", 
            "title": "Multikey Indexes"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#geospatial-indexes", 
            "text": "MondoDB supports two type of Geospatial indexes to allow for efficient queries of geospatial data. The first index is called 2d indexes which is used for planner geometry data. The second is called 2 sphere indexes which is used for spherical geometry data.  These indexes stores your data as GeoJSON objects and supports many types such as point, polygon, multipoint and many others. For more information about how to use these types of indexes, please have a look at  MongoDB documentations .", 
            "title": "Geospatial Indexes"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#text-index", 
            "text": "MongoDB supports text indexes to allow for queries that performs only search on string contents. You can create a text index easily by using the \"text\" keyword as shown below:  db.products.createIndex({ name :  text })  Like Multi-Key indexes, you are allowed to use only one text index inside the compound index. For more information about this index, please have a look at  MongoDB documentations .", 
            "title": "Text Index"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#hash-index", 
            "text": "Hash index is used to store the hash value of the real values of the indexed field. If the indexed field is a sub-document, then it will collapse the whole document and compute the hash for all fields inside this sub-document. This index is used when you want to shard a collection by certain hash shard keys. This type of index doesn't support multi-key indexes. You can create a hash index using the \"hashed\" keyword as seen below:  db.products.createIndex( { name:  hashed  } )", 
            "title": "Hash Index"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#index-properties", 
            "text": "You can define some properties for your index as explained below:", 
            "title": "Index Properties"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#unique", 
            "text": "You can define a certain index as unique which means that there should be no other document with the same indexed value, otherwise MongoDB will reject this document.  You can make any index Unique as seen below:  db.products.createIndex( {  name : 1 }, { unique: true } )  If you do this, then any new created document with already existing name will be rejected.", 
            "title": "Unique"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#partial", 
            "text": "This is a new property added on MongoDB version 3.2 that can be used when creating indexes and specify certain filter so that this index will be applied only to the documents that match this filter. Example is below:  db.products.createIndex( {  name : 1 }, { partialFilterExpression: { size: { $gt: 5 } } )  The above example will apply the index only on documents having size greater than 5.", 
            "title": "Partial"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#sparse", 
            "text": "This property is like previous property but with only one filter criteria which is that we apply the index only if the indexed field exists. Example is given below:  db.products.createIndex( {  name : 1 }, { sparse: true } )  Using this property will make the index applies only for documents that are having a name filed.", 
            "title": "Sparse"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#ttl", 
            "text": "This property makes the document automatically removed by MongoDB after a pre-defined time.  This index is very useful for certain types of data such as temporary data that shouldn't be persisted in database. Examples are logs or event data. Below is an example of how to use this property:  db.eventlog.createIndex( {  lastModifiedDate : 1 }, { expireAfterSeconds: 3600 } )  In the above example, we are creating a TTL index on the lastModifiedDate field which will let MongoDB expire the document after 3600 seconds from the value of the lastModifiedDate fields.", 
            "title": "TTL"
        }, 
        {
            "location": "/MongoDB/Query Model/Indexing/#removing-indexes", 
            "text": "To remove an already created index, you can use the  db.collection.dropIndex() method.  For example if we want to remove the single field index we created on the name field of the products collection, we can run the below :  db.products.dropIndex({ name :1})", 
            "title": "Removing indexes"
        }, 
        {
            "location": "/MongoDB/Query Model/Query Options/", 
            "text": "back\n\n\nMongoDB supports one simple method db.collection.find() to search for documents inside a certain collection. This method accepts two inputs, the first input is the selection criteria for you query and the second input is an optional to specify the fields to be returned or what is called the projections. This method will return a cursor that can be used to iterator through all the returned documents. For more information about the cursor, you can have a look at \nMongoDB documentations\n. In the following sections, I will show how you can use MongoDB to run parametrised, range, array, and embedded documents queries. \n\n\nParameterised Queries\n\n\nTo query all the documents inside a particular collection, you need to specify the selection query and define projections if needed as seen in the below example:\n\n\ndb.products.find({\nname\n:\nTshirt\n})\n\n\n\n\nIn the above example we are querying the products collection to find all documents with name \"Tshirt\". If you don't specify any projections, the MongoDB will return all fields. If you want to return only product name, price and size , you can do it as shown below:\n\n\ndb.products.find({\nname\n:\nTshirt\n} , {name:1, price:1, size:1})\n\n\n\n\nMongoDB will return the _id field by default, if you want to exclude the _id field you can change the query to below:\n\n\ndb.products.find({\nname\n:\nTshirt\n} , {name:1, price:1, size:1, _id:0})\n\n\n\n\nYou can also query documents using one of the query selectors as shown in the example below:\n\n\ndb.products.find({\nname\n:{$in: [\nTshirt\n,\nTV\n]}} , {name:1, price:1, size:1, _id:0})\n\n\n\n\nIn the above example, we are querying for all documents with names \"Tshirt\" and \"TV\". For a complete list for all the query selectors supported by MongoDB, please have a look at the \ndocumentations\n.\n\n\nRange Queries\n\n\nDoing a range query in MongoDB is simple. You can just use the range query selectors such as $gt, $gte, $lt, and $lte in the query criteria to do any range queries. For example, to get all products with price in a certain range we do that as shown below:\n\n\ndb.products.find({\nprice\n:{$gt: 400, $lt: 1000}})\n\n\n\n\nQuery Arrays\n\n\nIf you want to query array fields inside a documents, you can simply check if the array values contains our query values as seen in the example below:\n\n\ndb.products.find({\ntags\n:\nElectronics\n})\n\n\n\n\nIf you are querying an array of documents, you can use the $elemMatch query selector as shown below:\n\n\ndb.products.find({\nmanufacture\n: \n                               {\n                                 $elemMatch:\n                                       {\n                                         name: Boss,\n                                         Country: US\n                                       }\n                })\n\n\n\n\nIn the above example, we are search inside the manufactures list which is an array value. By using $elemMatch, we can look for a document inside the array with the exact fields.\n\n\nQuery Embedded Documents\n\n\nTo query inside the embedded documents, you can use the dot notation as shown in the example below:\n\n\ndb.products.find({\ndetails.size\n:\n20\n})\n\n\n\n\nIn the above example we are searching for all products with size equal to 20 which is a subfield inside the product document.\n\n\nYou can also search for the exact sub document as shown below:\n\n\ndb.products.find({\ndetails\n:{size:20 , colour: Red})\n\n\n\n\nThis will give you the exact match of the sub document, so the order of the fields is very important. This means the above query won't return below document:\n\n\n{\n_id: \nSomeID\n,\nname: \nProductName\n,\ndetails: {colour: Red, size:20}\n}\n\n\n\n\nBuilt-in query functions\n\n\nMongoDB supports many built in functions that can be applied during query operations such as skip(), limit(), min(), max(), distinct and count(). For a complete list of all the options, please have a look at \nMongoDB documentations\n. I will show below how to use the most popular options. \n\n\ncount()\n\n\nTo get the count of all the documents in a query result set, you can directly use the count() method as shown below:\n\n\ndb.products.find({cat: \nElectornics\n}).count()\n```\n\nThe above query will return the number of products in the electronics category. \n\n\n##### min()\n\nSimilarly, you can use the min() method to get the lower index bound of the returned result set as shown below:\n\n\n\n\n\ndb.products.find({cat: \"Electornics\"}).min({ price: 1.00 })\n\n\n\nIn the above query I am returning the product with minimum price in the electronics category. You need to define an index bound, as shown above we have used 1.00 as the index minimum bound.\n\n\n##### max()\n\nYou can also use the max() method to get the upper index bound of the returned result set as shown below:\n\n````\ndb.products.find({cat: \nElectornics\n}).max({ price: 2000 })\n\n\n\n\nIn the above query I am returning the product with maximum price of 2000 in the electronics category. \n\n\nskip()\n\n\nThis method is used if you want to skip some documents from the returned results set as seen below:\n\n\ndb.products.find({cat: \nElectornics\n}).skip(5)\n```\n\nIn the above example, we are skipping the first 5 documents and then returning all other documents.\n\n\n##### limit()\n\nTo return only a limited number for documents from the returned results set, a limit() method is supported as seen below:\n\n\n\n\n\n\ndb.products.find({cat: \"Electornics\"}).limit(5)\n\n\n\nIn the above example, we are returning only 5 documents from the results set.\n\n\n\n##### distinct()\n\nMongoDB supports also the distinct function to get all the unique values of a certain field across multiple documents in a collection. An example is given below:\n\n\n\n\n\ndb.products.distinct(\"size\")\n````\n\n\nThe above query will get the unique sizes across all products.", 
            "title": "Query Options"
        }, 
        {
            "location": "/MongoDB/Query Model/Query Options/#back", 
            "text": "MongoDB supports one simple method db.collection.find() to search for documents inside a certain collection. This method accepts two inputs, the first input is the selection criteria for you query and the second input is an optional to specify the fields to be returned or what is called the projections. This method will return a cursor that can be used to iterator through all the returned documents. For more information about the cursor, you can have a look at  MongoDB documentations . In the following sections, I will show how you can use MongoDB to run parametrised, range, array, and embedded documents queries.", 
            "title": "back"
        }, 
        {
            "location": "/MongoDB/Query Model/Query Options/#parameterised-queries", 
            "text": "To query all the documents inside a particular collection, you need to specify the selection query and define projections if needed as seen in the below example:  db.products.find({ name : Tshirt })  In the above example we are querying the products collection to find all documents with name \"Tshirt\". If you don't specify any projections, the MongoDB will return all fields. If you want to return only product name, price and size , you can do it as shown below:  db.products.find({ name : Tshirt } , {name:1, price:1, size:1})  MongoDB will return the _id field by default, if you want to exclude the _id field you can change the query to below:  db.products.find({ name : Tshirt } , {name:1, price:1, size:1, _id:0})  You can also query documents using one of the query selectors as shown in the example below:  db.products.find({ name :{$in: [ Tshirt , TV ]}} , {name:1, price:1, size:1, _id:0})  In the above example, we are querying for all documents with names \"Tshirt\" and \"TV\". For a complete list for all the query selectors supported by MongoDB, please have a look at the  documentations .", 
            "title": "Parameterised Queries"
        }, 
        {
            "location": "/MongoDB/Query Model/Query Options/#range-queries", 
            "text": "Doing a range query in MongoDB is simple. You can just use the range query selectors such as $gt, $gte, $lt, and $lte in the query criteria to do any range queries. For example, to get all products with price in a certain range we do that as shown below:  db.products.find({ price :{$gt: 400, $lt: 1000}})", 
            "title": "Range Queries"
        }, 
        {
            "location": "/MongoDB/Query Model/Query Options/#query-arrays", 
            "text": "If you want to query array fields inside a documents, you can simply check if the array values contains our query values as seen in the example below:  db.products.find({ tags : Electronics })  If you are querying an array of documents, you can use the $elemMatch query selector as shown below:  db.products.find({ manufacture : \n                               {\n                                 $elemMatch:\n                                       {\n                                         name: Boss,\n                                         Country: US\n                                       }\n                })  In the above example, we are search inside the manufactures list which is an array value. By using $elemMatch, we can look for a document inside the array with the exact fields.", 
            "title": "Query Arrays"
        }, 
        {
            "location": "/MongoDB/Query Model/Query Options/#query-embedded-documents", 
            "text": "To query inside the embedded documents, you can use the dot notation as shown in the example below:  db.products.find({ details.size : 20 })  In the above example we are searching for all products with size equal to 20 which is a subfield inside the product document.  You can also search for the exact sub document as shown below:  db.products.find({ details :{size:20 , colour: Red})  This will give you the exact match of the sub document, so the order of the fields is very important. This means the above query won't return below document:  {\n_id:  SomeID ,\nname:  ProductName ,\ndetails: {colour: Red, size:20}\n}", 
            "title": "Query Embedded Documents"
        }, 
        {
            "location": "/MongoDB/Query Model/Query Options/#built-in-query-functions", 
            "text": "MongoDB supports many built in functions that can be applied during query operations such as skip(), limit(), min(), max(), distinct and count(). For a complete list of all the options, please have a look at  MongoDB documentations . I will show below how to use the most popular options.", 
            "title": "Built-in query functions"
        }, 
        {
            "location": "/MongoDB/Query Model/Query Options/#count", 
            "text": "To get the count of all the documents in a query result set, you can directly use the count() method as shown below:  db.products.find({cat:  Electornics }).count()\n```\n\nThe above query will return the number of products in the electronics category. \n\n\n##### min()\n\nSimilarly, you can use the min() method to get the lower index bound of the returned result set as shown below:  db.products.find({cat: \"Electornics\"}).min({ price: 1.00 })  \nIn the above query I am returning the product with minimum price in the electronics category. You need to define an index bound, as shown above we have used 1.00 as the index minimum bound.\n\n\n##### max()\n\nYou can also use the max() method to get the upper index bound of the returned result set as shown below:\n\n````\ndb.products.find({cat:  Electornics }).max({ price: 2000 })  In the above query I am returning the product with maximum price of 2000 in the electronics category.", 
            "title": "count()"
        }, 
        {
            "location": "/MongoDB/Query Model/Query Options/#skip", 
            "text": "This method is used if you want to skip some documents from the returned results set as seen below:  db.products.find({cat:  Electornics }).skip(5)\n```\n\nIn the above example, we are skipping the first 5 documents and then returning all other documents.\n\n\n##### limit()\n\nTo return only a limited number for documents from the returned results set, a limit() method is supported as seen below:  db.products.find({cat: \"Electornics\"}).limit(5)  \nIn the above example, we are returning only 5 documents from the results set.\n\n\n\n##### distinct()\n\nMongoDB supports also the distinct function to get all the unique values of a certain field across multiple documents in a collection. An example is given below:  db.products.distinct(\"size\")\n````  The above query will get the unique sizes across all products.", 
            "title": "skip()"
        }, 
        {
            "location": "/MongoDB/Query Model/Regular Expressions Support/", 
            "text": "MongoDB supports regular expressions for pattern matching strings in query operations. MongoDB supports a huge number of regular expressions. You can use the $regex keyword to specifiy regular expression matching patterns as shown below:\n\n\ndb.products.find( { sku: { $regex: /^ABC/i } } )\n\n\n\n\nFor a complete list of all the regular expressions options that you can use, please have a look at \nMongoDB documentations\n.", 
            "title": "Regular Expressions Support"
        }, 
        {
            "location": "/MongoDB/Query Model/Sorting/", 
            "text": "MongoDB provide very useful method to sort the returned results of a query operation. The method sort() can be used by defining the fields to be used for sorting. An example is shown below:\n\n\ndb.products.find().sort({price: 1})\n\n\n\n\nYou can sort the collection documents using a certain field either ascending or descending by defining this field with 1 or -1 respectively. In the above example, we are sorting the products collection documents ascendingly by price. \n\n\nIn general, you should create index in the field you are planning to use for sorting. Otherwise MongoDB is going to sort the documents in memory which requires your result set to be less than 32 MB. So usually if you want to sort without indexes, you should use the sort method with the limit function as shown below:\n\n\ndb.products.find().sort({price: 1}).limit(20)", 
            "title": "Sorting"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/", 
            "text": "In this section, I will talk about the strengths and weaknesses of MongoDB and when using it makes sense.\n\n\nStrengths\n\n\nBelow are the main strengths of MongoDB:\n\n\nEasy to scale out\n\n\nMongoDB supports scaling out your reads and write throughputs using the sharded cluster.  The sharded cluster supports auto sharding and auto balancing which makes it easy to horizontally scale your application by adding thousands of nodes and be able to response to millions of operations per second over a very large data set. \n\n\nFlexible document Data model\n\n\nThe especial document data model of MongoDB allows for more flexible designs since there is no need to convert or map your application objects to database object like we do in relational databases (no impedance mismatch). The objects are easily mapped to BSON documents which is more natural. This makes MongoDB more developer friendly and since MongoDB is schema less, it is easier to change your data structure on the fly by just adding any kind of fields at any time (no ALTER TABLE).\n\n\nReplication and High Availability\n\n\nMongoDB supports a configuration called a replica set which allows for for data redundancy, fault tolerance and disaster recovery. Additionally, MongoDB supports automatic failover process that takes place when the primary node in a replica set isn't accessible for some reasons. This makes MonoDB easily configurable to be a high available database.\n\n\nBetter performance when used correctly\n\n\nIf your data fits the document data model, MongoDB can show a very big improvement in performance. MongoDB encourages denormalising your data into a single document and then it can provide great performance. However it is not always possible to model your data in a denormalised way especially if your data has a relational nature. If your data doesn't fit the document data model,  you can end up with slower performance. But then you have chosen the wrong database. \n\n\nRich query capabilities\n\n\nMongoDB support deep query capabilities such as full-tex search, regular expressions, dynamic queries on documents, aggregation (using map-reduce or the aggregation pipeline framework), and sorting. The aggregation and the queries are performed in the collection-level which might be a problem if you want to run global queries across the whole database since joins aren't supported. \n\n\nWeaknesses\n\n\nBelow I will talk about the main weaknesses of MongoDB:\n\n\nNot recommended for relational data\n\n\nAs mentioned before, MongoDB isn't suitable for relational data since joins aren't supported. If the data is so much related and doesn't fit into the document data model, then you will have to normalise the data model as you do in relational databases and this will result in many follow up queries that reduce the performance or introduce code complexity. It is always not recommended to use MongoDB for use cases where you want to model related data and later run complex join-intensive queries. Modelling related data can also lead to so much duplication of the data that can result in documents which increase to unbound limit. When the documents are so large, the performance will degrade and the complexity will increase. Finally, the aggregation and the query in MongoDB is done in the collection level which makes running complex aggregate queries not an easy task.\n\n\nNo internal transaction support\n\n\nMongoDB supports transaction on the document level. However transactions are not automatically supported across different documents. Supporting transactions in MongoDB requires manual intervention to verify, commit or rollback using methods such as the Two Phases Commits pattern. \n\n\nSummary\n\n\nMongoDB supports very interesting features such as its unique document data model, scalability, availability and rich query support. This makes it suitable for many use cases such as logging and content management. It is always recommend to use MongoDB for application where the data can fit easily in the document data model. MongoDB isn't a replacement for relational databases, and doesn't support well applications having data with a relational nature such as social data. Besides, MongoDB is not recommended for applications that requires high transaction support such as the critical financial applications. MongoDB excels on applications that requires efficient horizontal scaling, high availability and flexible data model that makes developer's life more easy and at the same time having a data suitable for the document data model.", 
            "title": "Strengths and Weaknesses"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#strengths", 
            "text": "Below are the main strengths of MongoDB:", 
            "title": "Strengths"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#easy-to-scale-out", 
            "text": "MongoDB supports scaling out your reads and write throughputs using the sharded cluster.  The sharded cluster supports auto sharding and auto balancing which makes it easy to horizontally scale your application by adding thousands of nodes and be able to response to millions of operations per second over a very large data set.", 
            "title": "Easy to scale out"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#flexible-document-data-model", 
            "text": "The especial document data model of MongoDB allows for more flexible designs since there is no need to convert or map your application objects to database object like we do in relational databases (no impedance mismatch). The objects are easily mapped to BSON documents which is more natural. This makes MongoDB more developer friendly and since MongoDB is schema less, it is easier to change your data structure on the fly by just adding any kind of fields at any time (no ALTER TABLE).", 
            "title": "Flexible document Data model"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#replication-and-high-availability", 
            "text": "MongoDB supports a configuration called a replica set which allows for for data redundancy, fault tolerance and disaster recovery. Additionally, MongoDB supports automatic failover process that takes place when the primary node in a replica set isn't accessible for some reasons. This makes MonoDB easily configurable to be a high available database.", 
            "title": "Replication and High Availability"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#better-performance-when-used-correctly", 
            "text": "If your data fits the document data model, MongoDB can show a very big improvement in performance. MongoDB encourages denormalising your data into a single document and then it can provide great performance. However it is not always possible to model your data in a denormalised way especially if your data has a relational nature. If your data doesn't fit the document data model,  you can end up with slower performance. But then you have chosen the wrong database.", 
            "title": "Better performance when used correctly"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#rich-query-capabilities", 
            "text": "MongoDB support deep query capabilities such as full-tex search, regular expressions, dynamic queries on documents, aggregation (using map-reduce or the aggregation pipeline framework), and sorting. The aggregation and the queries are performed in the collection-level which might be a problem if you want to run global queries across the whole database since joins aren't supported.", 
            "title": "Rich query capabilities"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#weaknesses", 
            "text": "Below I will talk about the main weaknesses of MongoDB:", 
            "title": "Weaknesses"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#not-recommended-for-relational-data", 
            "text": "As mentioned before, MongoDB isn't suitable for relational data since joins aren't supported. If the data is so much related and doesn't fit into the document data model, then you will have to normalise the data model as you do in relational databases and this will result in many follow up queries that reduce the performance or introduce code complexity. It is always not recommended to use MongoDB for use cases where you want to model related data and later run complex join-intensive queries. Modelling related data can also lead to so much duplication of the data that can result in documents which increase to unbound limit. When the documents are so large, the performance will degrade and the complexity will increase. Finally, the aggregation and the query in MongoDB is done in the collection level which makes running complex aggregate queries not an easy task.", 
            "title": "Not recommended for relational data"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#no-internal-transaction-support", 
            "text": "MongoDB supports transaction on the document level. However transactions are not automatically supported across different documents. Supporting transactions in MongoDB requires manual intervention to verify, commit or rollback using methods such as the Two Phases Commits pattern.", 
            "title": "No internal transaction support"
        }, 
        {
            "location": "/MongoDB/Results/Strengths and Weaknesses/#summary", 
            "text": "MongoDB supports very interesting features such as its unique document data model, scalability, availability and rich query support. This makes it suitable for many use cases such as logging and content management. It is always recommend to use MongoDB for application where the data can fit easily in the document data model. MongoDB isn't a replacement for relational databases, and doesn't support well applications having data with a relational nature such as social data. Besides, MongoDB is not recommended for applications that requires high transaction support such as the critical financial applications. MongoDB excels on applications that requires efficient horizontal scaling, high availability and flexible data model that makes developer's life more easy and at the same time having a data suitable for the document data model.", 
            "title": "Summary"
        }, 
        {
            "location": "/MongoDB/Results/Summary/", 
            "text": "MongoDB is an open source database that uses the document model which stores the data as documents and collections to provide more flexibility in terms of data modeling. MongoDB provides high performance, availability and scalability by using features such as sharding, replication, embedded documents, indexing, aggregation pipeline and automatic failover.\n\n\nUse cases\n\n\nMongoDB is based on a flexible document model and uses the rich BSON format representation. Additionally it provides high performance, horizontal scalability, high availability and fault tolerance. This makes it suitable for many use cases such as event logging, content management, blogging platforms, real time analytics, or web and ecommerce applications. \n\n\nBasic Concepts\n\n\nThe main concepts used in MongoDB:\n\n\nDocuments: MongoDB stores the data as documents which contains key-value pairs and have a structure similar to a map. They grow in size up to 16MB and each document should have an _id field which will be indexed automatically. The _id field is generated automatically if not specified and is used as a unique identifier for the document. The data inside the document is stored in a json-like format called BSON.\n\n\nCollections: Are used to group multiple documents and are analog to tables in RDBMS. Collections can contain documents with different schema, however it is always encouraged that each collection stores documents with similar schema.\n\n\nBSON: MongoDB uses a BSON format to represent its documents. BSON is a binary encoded format that extends the well-known JSON model to provide more data types support. BSON also is a lightweight efficient format. It is also very fast and traversable.\n\n\nInstalling\n\n\nCan be installed in most of the operating systems by building from the source using the  distribution binaries. In the linux-based systems, you can also install it using package managers such as Yum. In Windows system, you can use the installation file (a file with .msi extension) to install it by following simple installation wizard. In Mac OS, it is possible to install it using Homebrew package manager.\n\n\nQuery language\n\n\nMongoDB provides normal CRUD operations to interact easily with your stored data. These operations can run only at the collection level. To query the data, the db.products.find() can be used. To insert the data, many commands can be used such as db.collection.insertOne(), db.collection.insertMany(), and db.collection.insert(). To update the data, multiple commands are also supported such as db.collection.updateOne(), db.collection.updateMany(), db.collection.update(). Finally, to remove the data, commands such as db.collection.deleteOne(), db.collection.deleteMany() and db.collection.remove() can be used.\n\n\nTransaction support\n\n\nOn the level of a single document, MongoDB guarantees that the write operation is atomic. This means that if you try to update, insert or delete a single document in MongoDB, the operation will be applied in the form of \"all or nothing\". However if you try to do a write operations for multiple documents, the operations will be applied atomically for each single document alone but the operations as whole may interleave and are not atomic. Hence the transaction is possible using MongoDB if you model your data in a single document and not supported internally if you plan to do it across multiple documents.  There are workaround methods that can be used to support transactions across multiple documents such as the Two Phases Commits pattern. Isolation can be supported across multiple documents using the $isolated option. Finally, concurrency control (allowing multiple applications to access documents concurrently without causing inconsistency or conflicts) is supported by creating a unique index on one of the document field so that any concurrent insert or update for this document won't result in a duplicate document. \n\n\nData Import and Export\n\n\nMongoDB supports the db.collection.bulkWrite() method that can be used to execute a batch of write operations such as insert, update and delete at the same time. Additionally, MongoDB supports two useful tools (mongoexport and mongoimport) that can be used to migrate data between different mongodb instances. By using mongoexport or mongoimport, you can import or export data in a CSV or BSON format. \n\n\nData Layout\n\n\nMongoDB is schema-less and doesn't enforce documents structure. Hence data modelling is more flexible. Coming up with good data model design needs to consider the below criteria:\n\n\n\n\nWhat are your data access patterns?\n\n\nHow do you query and update your data?\n\n\nHow is your data look like?\n\n\nWhat are your application requirements?\n\n\nWhat are the relationships between your data?\n\n\n\n\nData can be modeled based on the data relationships using the normalized model or the denormalized model. The normalized model is done by defining separate documents and using references to model the relationships between them. The denormalized model is done by embedding documents inside each other to model relationships between them. The normalized model provides more data modeling flexibility and prevent data duplication but requires many follow-up queries that reduce the performance.  The denormalized model provides better read perfromance and support for transactions, however the document size can increase to unbound limit and impact performance due to data duplication.\n\n\nRelational data\n\n\nAll relationships can be either modeled using the normalized or the denormalized models. However it is recommended to model the one-to-one relationships using the denormalized model by embedding documents to provide better perfomance. The one-to-many relationships can be modeled using the denormalized model if this wont result in document growing to an unbound size which can impact perfromance. Finally, the many-to-many relationships are recommended to be modeled using the normalized model using references to prevent extensive data redundancy that can result in very large documents.\n\n\nNormalisation/Denormalisation\n\n\nMongoDB encourage denormalising the data by using the rich document structure it supports. Denormalising data in MongoDB can reduce development complexity and speed time to market as well as increasing performance. In the other hand, MongoDB supports normalised or denormalised models when representing relationships between your data as have been already explained in previous sections.\n\n\nReferential Integrity\n\n\nThere is no referential integrity enforced by MongoDB\n\n\nNested Data\n\n\nMongoDB supports very well nested structures by allowing documents to embed single or multiple documents. This feature allows for great flexibility during data modelling. \n\n\nBuilt-in query functions\n\n\nMongoDB supports many built in functions that can be applied during query operations such as skip(), limit(), min(), max(), distinct and count()\n\n\nQuery\n\n\nMongoDB supports one simple method db.collection.find() to search for documents inside a certain collection. This method accepts two inputs, the first input is the selection criteria for you query and the second input is an optional to specify the fields to be returned or what is called the projections. This method will return a cursor that can be used to iterator through all the returned documents. MongoDB supports parametrised and range using many query selectors such as $in, $eq, $gt, $lt, $gte, $lte, $ne and $nin. It is also supported to query arrays using the $elemMatch keyword. Finally, it is possible to query embedded documents using the dot notation.\n\n\nFull Text Search\n\n\nMongoDB supports Full-Text Search using Text Indexes. Text Indexes can be defined on any string or array of strings values in your documents that you want them to be fully searched. By defining a text index on any field, MongoDB will tokenises and stems the field's text content and creates the index accordingly. You can create only one text index per collection but it is possible to create compound Text index or even Text index the whole document. \n\n\nRegular Expressions\n\n\nMongoDB supports regular expressions for pattern matching strings in query operations using the $regex keyword.\n\n\nIndexing\n\n\nMongoDB indexes are created using the efficient B-tree data structure to store small portion of information used to speed up queries by limiting the number of documents to be searched. These information will be stored using the B tree structure in order useful to perform range queries or sorting. MongoDB supports a variety of index types that are defined on the collection level and can apply to any field or subfield in the document such single index, compound index, Multikey index, Geospatial index, Text index, and Hash index. The index can have many properties such as Unique, Partial, Sparse and TTL.\n\n\nAggregation\n\n\nMongoDB supports three ways to aggregate the data, either through the pipeline aggregation framework, or by using the map reduce data processing paradigm or through some supported uses-specific functions such as distinct, group and count functions. In the aggregation pipeline framework, the data is processed through multiple stages before a final results is returned. Many stages are supported such as $project, $match, $group, $sort and $sample stages. The aggregation is done using the db.collection.aggregate() method that operates on a single collection and can take advantage of the already created indexes to enhance performance. In general, using the aggregation pipeline framework is the more preferred way to do aggregation related tasks since it is less complex and more efficient than using map-reduce. However map-reduce provides some sort for flexibility since it is using custom javascript functions to do the map and reduce operations.\n\n\nSorting\n\n\nMongoDB supports a sort() method that allows for sorting the data by some sorting fields either ascendingly or descendingly. It is recommended to create an index on the fields that will be used for sorting.\n\n\nDocument Validation Feature\n\n\nUsed to enforce some validation rules on the documents structure. These rules will be checked when an insert or update is performed against the collection. If the validation rules have been violated, then an error or warning will be raised depending on the validationAction option. \n\n\nGridFS Feature\n\n\nUsed to store and manage large files that are beyond the normal 16MB document size. GridFS simply store large binary files by splitting them into smaller parts called \"chunks\" and then store them into MongoDB. Then when you want to retrieve the file, GridFS will combine all the chunks and return the file for you. You can also perform range queries on the file chunks to retrieve only a certain part of the file.\n\n\nConfiguration\n\n\nMongoDB uses a YAMLconfiguration file for each mongod or mongos instance that contains all the settings and command options that will be used by this instance. If you want to change a configuration option in the configuration file of a mongod or mongos instances, you will need to restart the instance to pick up the new changes.\n\n\nScalability\n\n\nMongoDB supports sharding by using a sharded cluster which is composed of three components: the query routers, the configuration servers and the shards.  The query routers ate mongos instances which are used by the client applications to interact with the sharded data. Client applications can't access the shards directly and can only submit read and write requests to mongos instance. Mongos instance is the one responsible for routing these reads and writes requests to the respective shards.  The config servers holds the metadata of the sharded cluster such as the shards locations of each sharded data. Config servers contain very important information for the sharded cluster to work and if the servers are down for any reason, the sharded cluster becomes inoperable. For this reason, it is a good idea to replicate the data in the config server using a replica set configuration which allows the sharded cluster to have more than 3 config servers and up to 50 servers to ensure availability. The shards are a replica set or a single server to hold part of the sharded data. Usually, each shard is a replica set which provides high availability and data redundancy that can be used in case of disaster recovery.  MongoDB partition and distribute the data evenly based on the sharded key. To partition the data using the sharded key, MongoDB either use range based partitioning or hash based partitioning. In the range based sharding, MongoDB divides the data into ranges based on the sharded key values which provides efficient range queries but uneven distribution of the data. In the other hand, the hash based partitioning divides the data based on the value of a hash function which provides random even distribution of the data but not efficient range queries.\n\n\nPersistency\n\n\nMongoDB supports two persistent storage engines: the WiredTiger storage engine or the MMAPv1 storage engine.  Usually, persistency is supported by flushing the data from the memory to disk periodically (default is each 60 seconds). In addition, a mechanism called journalling is also used to provide more durable solution in the event of failure. MonogoDB uses journal files to store the in-memory changes and can be used later to recover the data when the server crashes before flushing data to disk files. The write operations in MongoDB are durable when journalling is enabled. Otherwise, write operations could be lost between checkpoints or flushing data to disk. So the write operation is considered durable when it has been written to the journal files in case of a single server mongod. In case of a replica set, the write operation is durable if it has been written to the journal files of the majority voting nodes.\n\n\nBackup\n\n\nMongoDB provides a tool called mongodump that can read data from the database and creates a backup BSON files. To restore the created backup files, you can use the mongorestore tool to populate the MongoDB database with the backup BSON files. After restoring the data using mongorestore, the mongod instance must rebuild all indexes. Mongodump tool can impact the mongod instance performance, so it is usually recommended to run this tool against a secondary member in a replica set. \n\n\nSecurity\n\n\nMongoDB offers many security features such as Authentication, Role-Based access control, and communication encryption. Authentication used to verify the identity of clients who are trying to connect to the database. Role-Based Access Control (RBAC) used to control the access to certain resource by granting a set of permissions to the user. A resource is either a database, a collection, set of collection or a cluster. Finally, MongoDB supports the use of TLS/SSL protocols for encrypting the connections to mongod or mongos instances.\n\n\nUpgrading\n\n\nTo upgrade a MongoDB instance, you should start by shutting the server instance first. After that you can either manually upgrading by downloading the new 3.2 binaries and then just replace them with the old 3.0 binaries or you can use the package managers such as apt, dnf, yum, or brew.\n\n\nAvailability\n\n\nMongoDB supports data redundancy through replication. MongoDB supports a configuration called a replica set which is a group of mongod instances that use the same data set. Each replica set has only one a primary node and the rest are all secondaries.  MongoDB supports also a special type of nodes called arbiters which are used only in case that there are even number of nodes in the replica set and to be used during the voting process. The primary node is the node responsible for all write operations and records all of its operations to an operation log called oplog. The oplog is then used by secondary nodes to replicate the primary data set. The primary node is also the one responsible for read operations, the application can chose a different read concern if it needs to read data from secondary nodes but it is not recommended since the data in secondary nodes might be stale. If you want to scale reads, you can use sharding and not replication. Replication in MongoDB is mainly supported for data redundancy and availability. Secondaries first do an initial sync when the secondary member doesn't have data such as when the member is new. During the initial sync, the secondary node will clone all the databases and build all indexes on all collections. After that the secondary nodes will just do a simple sync operation to replicate the primary node data using the oplog asynchronously. MongoDB also supports that you configure a secondary node so that it can't be promoted into a primary node in case of a failure by changing it is priority to zero. If you want to use some secondary nodes for specific tasks such as reporting or backup, you can configure the nodes to be hidden by setting its priority to zero to prevent a hidden member to become primary. You can also configure a secondary node to act as a delayed replica that contains an earlier version of the data set and can be used later for rollback or running historical snapshots. Finally, MongoDB supports an automatic failover process that takes place when the primary node in a replica set isn't accessible for some reasons. If the other replica set members aren't able to communicate with the primary server for more than 10 seconds, an eligible secondary member will automatically be promoted to act as a primary node. The selection of the secondary node that will replace the failed primary node is based on a majority voting of the replica set members.", 
            "title": "Summary"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#use-cases", 
            "text": "MongoDB is based on a flexible document model and uses the rich BSON format representation. Additionally it provides high performance, horizontal scalability, high availability and fault tolerance. This makes it suitable for many use cases such as event logging, content management, blogging platforms, real time analytics, or web and ecommerce applications.", 
            "title": "Use cases"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#basic-concepts", 
            "text": "The main concepts used in MongoDB:  Documents: MongoDB stores the data as documents which contains key-value pairs and have a structure similar to a map. They grow in size up to 16MB and each document should have an _id field which will be indexed automatically. The _id field is generated automatically if not specified and is used as a unique identifier for the document. The data inside the document is stored in a json-like format called BSON.  Collections: Are used to group multiple documents and are analog to tables in RDBMS. Collections can contain documents with different schema, however it is always encouraged that each collection stores documents with similar schema.  BSON: MongoDB uses a BSON format to represent its documents. BSON is a binary encoded format that extends the well-known JSON model to provide more data types support. BSON also is a lightweight efficient format. It is also very fast and traversable.", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#installing", 
            "text": "Can be installed in most of the operating systems by building from the source using the  distribution binaries. In the linux-based systems, you can also install it using package managers such as Yum. In Windows system, you can use the installation file (a file with .msi extension) to install it by following simple installation wizard. In Mac OS, it is possible to install it using Homebrew package manager.", 
            "title": "Installing"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#query-language", 
            "text": "MongoDB provides normal CRUD operations to interact easily with your stored data. These operations can run only at the collection level. To query the data, the db.products.find() can be used. To insert the data, many commands can be used such as db.collection.insertOne(), db.collection.insertMany(), and db.collection.insert(). To update the data, multiple commands are also supported such as db.collection.updateOne(), db.collection.updateMany(), db.collection.update(). Finally, to remove the data, commands such as db.collection.deleteOne(), db.collection.deleteMany() and db.collection.remove() can be used.", 
            "title": "Query language"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#transaction-support", 
            "text": "On the level of a single document, MongoDB guarantees that the write operation is atomic. This means that if you try to update, insert or delete a single document in MongoDB, the operation will be applied in the form of \"all or nothing\". However if you try to do a write operations for multiple documents, the operations will be applied atomically for each single document alone but the operations as whole may interleave and are not atomic. Hence the transaction is possible using MongoDB if you model your data in a single document and not supported internally if you plan to do it across multiple documents.  There are workaround methods that can be used to support transactions across multiple documents such as the Two Phases Commits pattern. Isolation can be supported across multiple documents using the $isolated option. Finally, concurrency control (allowing multiple applications to access documents concurrently without causing inconsistency or conflicts) is supported by creating a unique index on one of the document field so that any concurrent insert or update for this document won't result in a duplicate document.", 
            "title": "Transaction support"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#data-import-and-export", 
            "text": "MongoDB supports the db.collection.bulkWrite() method that can be used to execute a batch of write operations such as insert, update and delete at the same time. Additionally, MongoDB supports two useful tools (mongoexport and mongoimport) that can be used to migrate data between different mongodb instances. By using mongoexport or mongoimport, you can import or export data in a CSV or BSON format.", 
            "title": "Data Import and Export"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#data-layout", 
            "text": "MongoDB is schema-less and doesn't enforce documents structure. Hence data modelling is more flexible. Coming up with good data model design needs to consider the below criteria:   What are your data access patterns?  How do you query and update your data?  How is your data look like?  What are your application requirements?  What are the relationships between your data?   Data can be modeled based on the data relationships using the normalized model or the denormalized model. The normalized model is done by defining separate documents and using references to model the relationships between them. The denormalized model is done by embedding documents inside each other to model relationships between them. The normalized model provides more data modeling flexibility and prevent data duplication but requires many follow-up queries that reduce the performance.  The denormalized model provides better read perfromance and support for transactions, however the document size can increase to unbound limit and impact performance due to data duplication.", 
            "title": "Data Layout"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#relational-data", 
            "text": "All relationships can be either modeled using the normalized or the denormalized models. However it is recommended to model the one-to-one relationships using the denormalized model by embedding documents to provide better perfomance. The one-to-many relationships can be modeled using the denormalized model if this wont result in document growing to an unbound size which can impact perfromance. Finally, the many-to-many relationships are recommended to be modeled using the normalized model using references to prevent extensive data redundancy that can result in very large documents.", 
            "title": "Relational data"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#normalisationdenormalisation", 
            "text": "MongoDB encourage denormalising the data by using the rich document structure it supports. Denormalising data in MongoDB can reduce development complexity and speed time to market as well as increasing performance. In the other hand, MongoDB supports normalised or denormalised models when representing relationships between your data as have been already explained in previous sections.", 
            "title": "Normalisation/Denormalisation"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#referential-integrity", 
            "text": "There is no referential integrity enforced by MongoDB", 
            "title": "Referential Integrity"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#nested-data", 
            "text": "MongoDB supports very well nested structures by allowing documents to embed single or multiple documents. This feature allows for great flexibility during data modelling.", 
            "title": "Nested Data"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#built-in-query-functions", 
            "text": "MongoDB supports many built in functions that can be applied during query operations such as skip(), limit(), min(), max(), distinct and count()", 
            "title": "Built-in query functions"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#query", 
            "text": "MongoDB supports one simple method db.collection.find() to search for documents inside a certain collection. This method accepts two inputs, the first input is the selection criteria for you query and the second input is an optional to specify the fields to be returned or what is called the projections. This method will return a cursor that can be used to iterator through all the returned documents. MongoDB supports parametrised and range using many query selectors such as $in, $eq, $gt, $lt, $gte, $lte, $ne and $nin. It is also supported to query arrays using the $elemMatch keyword. Finally, it is possible to query embedded documents using the dot notation.", 
            "title": "Query"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#full-text-search", 
            "text": "MongoDB supports Full-Text Search using Text Indexes. Text Indexes can be defined on any string or array of strings values in your documents that you want them to be fully searched. By defining a text index on any field, MongoDB will tokenises and stems the field's text content and creates the index accordingly. You can create only one text index per collection but it is possible to create compound Text index or even Text index the whole document.", 
            "title": "Full Text Search"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#regular-expressions", 
            "text": "MongoDB supports regular expressions for pattern matching strings in query operations using the $regex keyword.", 
            "title": "Regular Expressions"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#indexing", 
            "text": "MongoDB indexes are created using the efficient B-tree data structure to store small portion of information used to speed up queries by limiting the number of documents to be searched. These information will be stored using the B tree structure in order useful to perform range queries or sorting. MongoDB supports a variety of index types that are defined on the collection level and can apply to any field or subfield in the document such single index, compound index, Multikey index, Geospatial index, Text index, and Hash index. The index can have many properties such as Unique, Partial, Sparse and TTL.", 
            "title": "Indexing"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#aggregation", 
            "text": "MongoDB supports three ways to aggregate the data, either through the pipeline aggregation framework, or by using the map reduce data processing paradigm or through some supported uses-specific functions such as distinct, group and count functions. In the aggregation pipeline framework, the data is processed through multiple stages before a final results is returned. Many stages are supported such as $project, $match, $group, $sort and $sample stages. The aggregation is done using the db.collection.aggregate() method that operates on a single collection and can take advantage of the already created indexes to enhance performance. In general, using the aggregation pipeline framework is the more preferred way to do aggregation related tasks since it is less complex and more efficient than using map-reduce. However map-reduce provides some sort for flexibility since it is using custom javascript functions to do the map and reduce operations.", 
            "title": "Aggregation"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#sorting", 
            "text": "MongoDB supports a sort() method that allows for sorting the data by some sorting fields either ascendingly or descendingly. It is recommended to create an index on the fields that will be used for sorting.", 
            "title": "Sorting"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#document-validation-feature", 
            "text": "Used to enforce some validation rules on the documents structure. These rules will be checked when an insert or update is performed against the collection. If the validation rules have been violated, then an error or warning will be raised depending on the validationAction option.", 
            "title": "Document Validation Feature"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#gridfs-feature", 
            "text": "Used to store and manage large files that are beyond the normal 16MB document size. GridFS simply store large binary files by splitting them into smaller parts called \"chunks\" and then store them into MongoDB. Then when you want to retrieve the file, GridFS will combine all the chunks and return the file for you. You can also perform range queries on the file chunks to retrieve only a certain part of the file.", 
            "title": "GridFS Feature"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#configuration", 
            "text": "MongoDB uses a YAMLconfiguration file for each mongod or mongos instance that contains all the settings and command options that will be used by this instance. If you want to change a configuration option in the configuration file of a mongod or mongos instances, you will need to restart the instance to pick up the new changes.", 
            "title": "Configuration"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#scalability", 
            "text": "MongoDB supports sharding by using a sharded cluster which is composed of three components: the query routers, the configuration servers and the shards.  The query routers ate mongos instances which are used by the client applications to interact with the sharded data. Client applications can't access the shards directly and can only submit read and write requests to mongos instance. Mongos instance is the one responsible for routing these reads and writes requests to the respective shards.  The config servers holds the metadata of the sharded cluster such as the shards locations of each sharded data. Config servers contain very important information for the sharded cluster to work and if the servers are down for any reason, the sharded cluster becomes inoperable. For this reason, it is a good idea to replicate the data in the config server using a replica set configuration which allows the sharded cluster to have more than 3 config servers and up to 50 servers to ensure availability. The shards are a replica set or a single server to hold part of the sharded data. Usually, each shard is a replica set which provides high availability and data redundancy that can be used in case of disaster recovery.  MongoDB partition and distribute the data evenly based on the sharded key. To partition the data using the sharded key, MongoDB either use range based partitioning or hash based partitioning. In the range based sharding, MongoDB divides the data into ranges based on the sharded key values which provides efficient range queries but uneven distribution of the data. In the other hand, the hash based partitioning divides the data based on the value of a hash function which provides random even distribution of the data but not efficient range queries.", 
            "title": "Scalability"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#persistency", 
            "text": "MongoDB supports two persistent storage engines: the WiredTiger storage engine or the MMAPv1 storage engine.  Usually, persistency is supported by flushing the data from the memory to disk periodically (default is each 60 seconds). In addition, a mechanism called journalling is also used to provide more durable solution in the event of failure. MonogoDB uses journal files to store the in-memory changes and can be used later to recover the data when the server crashes before flushing data to disk files. The write operations in MongoDB are durable when journalling is enabled. Otherwise, write operations could be lost between checkpoints or flushing data to disk. So the write operation is considered durable when it has been written to the journal files in case of a single server mongod. In case of a replica set, the write operation is durable if it has been written to the journal files of the majority voting nodes.", 
            "title": "Persistency"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#backup", 
            "text": "MongoDB provides a tool called mongodump that can read data from the database and creates a backup BSON files. To restore the created backup files, you can use the mongorestore tool to populate the MongoDB database with the backup BSON files. After restoring the data using mongorestore, the mongod instance must rebuild all indexes. Mongodump tool can impact the mongod instance performance, so it is usually recommended to run this tool against a secondary member in a replica set.", 
            "title": "Backup"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#security", 
            "text": "MongoDB offers many security features such as Authentication, Role-Based access control, and communication encryption. Authentication used to verify the identity of clients who are trying to connect to the database. Role-Based Access Control (RBAC) used to control the access to certain resource by granting a set of permissions to the user. A resource is either a database, a collection, set of collection or a cluster. Finally, MongoDB supports the use of TLS/SSL protocols for encrypting the connections to mongod or mongos instances.", 
            "title": "Security"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#upgrading", 
            "text": "To upgrade a MongoDB instance, you should start by shutting the server instance first. After that you can either manually upgrading by downloading the new 3.2 binaries and then just replace them with the old 3.0 binaries or you can use the package managers such as apt, dnf, yum, or brew.", 
            "title": "Upgrading"
        }, 
        {
            "location": "/MongoDB/Results/Summary/#availability", 
            "text": "MongoDB supports data redundancy through replication. MongoDB supports a configuration called a replica set which is a group of mongod instances that use the same data set. Each replica set has only one a primary node and the rest are all secondaries.  MongoDB supports also a special type of nodes called arbiters which are used only in case that there are even number of nodes in the replica set and to be used during the voting process. The primary node is the node responsible for all write operations and records all of its operations to an operation log called oplog. The oplog is then used by secondary nodes to replicate the primary data set. The primary node is also the one responsible for read operations, the application can chose a different read concern if it needs to read data from secondary nodes but it is not recommended since the data in secondary nodes might be stale. If you want to scale reads, you can use sharding and not replication. Replication in MongoDB is mainly supported for data redundancy and availability. Secondaries first do an initial sync when the secondary member doesn't have data such as when the member is new. During the initial sync, the secondary node will clone all the databases and build all indexes on all collections. After that the secondary nodes will just do a simple sync operation to replicate the primary node data using the oplog asynchronously. MongoDB also supports that you configure a secondary node so that it can't be promoted into a primary node in case of a failure by changing it is priority to zero. If you want to use some secondary nodes for specific tasks such as reporting or backup, you can configure the nodes to be hidden by setting its priority to zero to prevent a hidden member to become primary. You can also configure a secondary node to act as a delayed replica that contains an earlier version of the data set and can be used later for rollback or running historical snapshots. Finally, MongoDB supports an automatic failover process that takes place when the primary node in a replica set isn't accessible for some reasons. If the other replica set members aren't able to communicate with the primary server for more than 10 seconds, an eligible secondary member will automatically be promoted to act as a primary node. The selection of the secondary node that will replace the failed primary node is based on a majority voting of the replica set members.", 
            "title": "Availability"
        }, 
        {
            "location": "/MongoDB/Special Features/Document Validation/", 
            "text": "Starting from MongoDB version 3.2, MongoDB supports a new feature called document validation. This is a useful feature that can be used to enforce some validation rules on the documents structure inside a particular collection. These rules will be checked when an insert or update is performed against the collection. If the validation rules have been violated, then an error or warning will be raised depending on the validationAction option. You can set the validation rules either when you create the collection using the db.createCollection() method with the validator option or with the collMod command with the validator option if the collection is already created. An example how to create a collection with some validation rules is shown below:\n\n\ndb.createCollection( \ncustomers\n,\n   { validator: { $or:\n      [\n         { phone: { $type: \ndouble\n } },\n         { email: { $regex: /@yahoo\\.com$/ } }\n      ]\n   }\n} )\n\n\n\n\nMongoDB validates the documents during updates or inserts. If you add a validation rules to an already existing collection, the existing documents will be validated only when modified. You can set the validationLevel option to either strict or moderate. If you chose strict, then all documents will be validated during inserts or updates. However, if you use the moderate option, then the documents will be validated during updates and inserts only if the documents fulfil the validation criteria.  If you want to disable validation, you can set validationLevel to off.", 
            "title": "Document Validation"
        }, 
        {
            "location": "/MongoDB/Special Features/GridFS/", 
            "text": "MongoDB supports a powerful feature used to store and manage large files called GridFS. This feature can be used in many use cases such as content management system where you need to store and manage big binary data with metadata information. Other use case is in health-care related application where you want to show all patient information in one place. Part of the patient information can be a large binary data such as the x-ray, and MRI images which can be difficult to store in a normal documents in MongoDB because of the 16 MB BSON size limit. Beside storing large files, GridFS can be used when your filesystem limits the number of files in a single directory or when you want later to retrieve only part of your files instead of loading the whole file into memory.\n\n\nGridFS simply store large binary files by splitting them into smaller parts called \"chunks\" and then store them into MongoDB. Then when you want to retrieve the file, GridFS will combine all the chunks and return the file for you. \n\n\nWhen you store the binary files by using GridFS, it will break the files into chunks and store it in the fs.chunks collection. The chunk size is 255K. GridFS uses another collection called fs.files to store the metadata information of the file. \n\n\nYou can use GridFS to store and retrieved files by either using the \"mongofiles\" shell command or by any of the client drivers that support GridFS.\n\n\nYou can also perform range queries on the file chunks to retrieve only a certain part of the file and GridFS uses unique and compound index on the \"files_id\" and \"n\" fields of the chunks collection to allow for efficient retrieval of files.\n\n\nFor more information about GridFS, please have a look at \nMongoDB documentation.", 
            "title": "GridFS"
        }, 
        {
            "location": "/Neo4j/Neo4j_Main/", 
            "text": "back\n\n\nGetting Started\n\n\nBasic Features\n\n\nData Modeling\n\n\nSearching Data\n\n\nAdministration and Maintenance\n\n\nExample\n\n\nResults", 
            "title": "Neo4j Main"
        }, 
        {
            "location": "/Neo4j/Neo4j_Main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Neo4j_Main/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/Neo4j/Neo4j_Main/#basic-features", 
            "text": "", 
            "title": "Basic Features"
        }, 
        {
            "location": "/Neo4j/Neo4j_Main/#data-modeling", 
            "text": "", 
            "title": "Data Modeling"
        }, 
        {
            "location": "/Neo4j/Neo4j_Main/#searching-data", 
            "text": "", 
            "title": "Searching Data"
        }, 
        {
            "location": "/Neo4j/Neo4j_Main/#administration-and-maintenance", 
            "text": "", 
            "title": "Administration and Maintenance"
        }, 
        {
            "location": "/Neo4j/Neo4j_Main/#example", 
            "text": "", 
            "title": "Example"
        }, 
        {
            "location": "/Neo4j/Neo4j_Main/#results", 
            "text": "", 
            "title": "Results"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/", 
            "text": "back\n\n\nConfiguration\n\n\nScalability\n\n\nPersistency\n\n\nBackup\n\n\nSecurity\n\n\nUpgrading\n\n\nAvailability\n\n\nRefernces", 
            "title": "Admin main"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/#scalability", 
            "text": "", 
            "title": "Scalability"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/#persistency", 
            "text": "", 
            "title": "Persistency"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/#backup", 
            "text": "", 
            "title": "Backup"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/#security", 
            "text": "", 
            "title": "Security"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/#upgrading", 
            "text": "", 
            "title": "Upgrading"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/#availability", 
            "text": "", 
            "title": "Availability"
        }, 
        {
            "location": "/Neo4j/Adminstration/admin_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Adminstration/availability/", 
            "text": "back\n\n\nHigh availability features are supported only in the Neo4j enterprise edition which provides the ability to use a master-slave configuration architecture that can be used for availability and horizontal scaling. More than one database server slave can be configured to be the exact replica of the master database server and can be used to make the system continue working in case of hardware failure. The slaves can be also used to handle some of the read load which allows for read scaling.\n\n\nUsing the high availability option is easy, you need just to create the database instance from the HighlyAvailableGraphDatabaseFactory and not from the GraphDatabaseFactory. This mean that there will be no need to change any of the application code if you want to switch to the high available multi machine architecture. Then when your Neo4j server is running in the HA mode, it would expected a single master and zero or more slaves. The new master-slave architecture allows for writes to be served not only from the master instance but also in any of the slaves. The slaves will synchronise the write with the master to maintain consistency. Then after the master receives the new write from the slave, it will propagate the change to all other slaves. This means that in this configuration, the write is eventually consisted and the other slaves can't see the write immediately. \n\n\n\n\nThe instances in a HA cluster will monitor each other for availability and monitor if any new instance has joined or any existing instance has left the cluster. They elect a new master automatically when the old one leaves the cluster for any reason. An initial file called ha.initial having a list of urls for the instances inside the cluster, should be provided to any new instance that needs to join the cluster.\n\n\nTo configure the server to use the HA mode, you need to specify the below property in the configuration file conf/neo4j-server.properties:\n\n\norg.neo4j.server.database.mode=HA \n\n\n\n\nThere are many other configuration properties that you can specify while configuring the server to work in the HA mode. For more details about these properties, please see the \ndocumentations\n.\n\n\nWhen you do a transaction write, the master will propagate the changes to all the slave after the commit is success. This is done optimistically which means that the transaction will still be successful even if the propagation of the changes to some of the slave fail for some reason. So there is some risk that the transaction will be lost if the changes fail to be pushed to one of the slaves when the master goes down. This means that Transactions are atomic, consistent and durable but transaction will be eventually propagated to all available slaves. \n\n\nDuring master failover, a new master will be automatically elected and during this time the writes will be blocked until the new master is available. It is also possible to use arbiter instances which are instances that have only one role which is to participate in election in case there is no odd number of nodes and an instance is needed to break the tie.", 
            "title": "Availability"
        }, 
        {
            "location": "/Neo4j/Adminstration/availability/#back", 
            "text": "High availability features are supported only in the Neo4j enterprise edition which provides the ability to use a master-slave configuration architecture that can be used for availability and horizontal scaling. More than one database server slave can be configured to be the exact replica of the master database server and can be used to make the system continue working in case of hardware failure. The slaves can be also used to handle some of the read load which allows for read scaling.  Using the high availability option is easy, you need just to create the database instance from the HighlyAvailableGraphDatabaseFactory and not from the GraphDatabaseFactory. This mean that there will be no need to change any of the application code if you want to switch to the high available multi machine architecture. Then when your Neo4j server is running in the HA mode, it would expected a single master and zero or more slaves. The new master-slave architecture allows for writes to be served not only from the master instance but also in any of the slaves. The slaves will synchronise the write with the master to maintain consistency. Then after the master receives the new write from the slave, it will propagate the change to all other slaves. This means that in this configuration, the write is eventually consisted and the other slaves can't see the write immediately.    The instances in a HA cluster will monitor each other for availability and monitor if any new instance has joined or any existing instance has left the cluster. They elect a new master automatically when the old one leaves the cluster for any reason. An initial file called ha.initial having a list of urls for the instances inside the cluster, should be provided to any new instance that needs to join the cluster.  To configure the server to use the HA mode, you need to specify the below property in the configuration file conf/neo4j-server.properties:  org.neo4j.server.database.mode=HA   There are many other configuration properties that you can specify while configuring the server to work in the HA mode. For more details about these properties, please see the  documentations .  When you do a transaction write, the master will propagate the changes to all the slave after the commit is success. This is done optimistically which means that the transaction will still be successful even if the propagation of the changes to some of the slave fail for some reason. So there is some risk that the transaction will be lost if the changes fail to be pushed to one of the slaves when the master goes down. This means that Transactions are atomic, consistent and durable but transaction will be eventually propagated to all available slaves.   During master failover, a new master will be automatically elected and during this time the writes will be blocked until the new master is available. It is also possible to use arbiter instances which are instances that have only one role which is to participate in election in case there is no odd number of nodes and an instance is needed to break the tie.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/backup/", 
            "text": "back\n\n\nNeo4j provides an online backup tool called neo4j-backup that will be enabled by default and will store a local copy of the database. To enable it in case it wasn't not enabled, you can set the configuration parameter online_backup_enabled to true. By default the backup tool will listen to the loopback local host and the port 6362 but you can listen to other database instance by setting the online_backup_server configuration parameter. The backup tool will take an incremental backup after taking the first backup.  For more configuration options related to the backup tool, please check the \ndocumentations\n.\n\n\nTo perform a manual backup, you can use the below command:\n\n\nA full backup:\n\n\n$ mkdir /mnt/backup/neo4j-backup\n$ ./bin/neo4j-backup -host 192.168.1.34 -to /mnt/backup/neo4j-backup\n\n\n\n\nAn incremental backup:\n\n\n./bin/neo4j-backup -host 192.168.1.34 -to /mnt/backup/neo4j-backup\n\n\n\n\nThe backups can then act as fully functional databases in case of a failure. To restore the database in case of failure, you can just replace your database folder with the backed up version. If you are using the master-slave cluster configuration, you need first to shut down all the database instances in the cluster. Then replace the database folder with the backed up database. Finally start a node and issue an update transaction which will propagate the new database to all instances in the cluster.", 
            "title": "Backup"
        }, 
        {
            "location": "/Neo4j/Adminstration/backup/#back", 
            "text": "Neo4j provides an online backup tool called neo4j-backup that will be enabled by default and will store a local copy of the database. To enable it in case it wasn't not enabled, you can set the configuration parameter online_backup_enabled to true. By default the backup tool will listen to the loopback local host and the port 6362 but you can listen to other database instance by setting the online_backup_server configuration parameter. The backup tool will take an incremental backup after taking the first backup.  For more configuration options related to the backup tool, please check the  documentations .  To perform a manual backup, you can use the below command:  A full backup:  $ mkdir /mnt/backup/neo4j-backup\n$ ./bin/neo4j-backup -host 192.168.1.34 -to /mnt/backup/neo4j-backup  An incremental backup:  ./bin/neo4j-backup -host 192.168.1.34 -to /mnt/backup/neo4j-backup  The backups can then act as fully functional databases in case of a failure. To restore the database in case of failure, you can just replace your database folder with the backed up version. If you are using the master-slave cluster configuration, you need first to shut down all the database instances in the cluster. Then replace the database folder with the backed up database. Finally start a node and issue an update transaction which will propagate the new database to all instances in the cluster.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/configurations/", 
            "text": "back\n\n\nA Neo4j database server instance can be started without providing a configuration file and then the default configuration options will be considered. If you want to change the server configuration options, you need to change the settings inside the configuration file conf/neo4j-server.properties. In this section, I will talk about some of the main configuration options that you might want to change. \n\n\nIf you want to change the location of the database file that will contain your database changes, you can change the below configuration parameter:\n\n\norg.neo4j.server.database.location=data/graph.db\n\n\n\n\nThe URI path of the HTTP REST API can be changed using the below configuration option:\n\n\norg.neo4j.server.webadmin.data.uri=/db/data/\n\n\n\n\nAlso you can change the transaction timeout, which is the time used by the Neo4j instance to wait for the transaction to commit. After the timeout, the server will perform a roll back for the transaction. This configuration option can be changed as shown below:\n\n\norg.neo4j.server.transaction.timeout=60\n\n\n\n\nThe logging configurations can be changed using the below configuration parameters:\n\n\norg.neo4j.server.http.log.enabled=true\norg.neo4j.server.http.log.config=conf/neo4j-http-logging.xml\n\n\n\n\nThe first option enable or disable logging and the second option specify the logging file. \n\n\nThere are many other configuration options that can be used to configure performance, security and other operating features. For a full list of all the configuration options, please check out the \ndocumentations\n.", 
            "title": "Configurations"
        }, 
        {
            "location": "/Neo4j/Adminstration/configurations/#back", 
            "text": "A Neo4j database server instance can be started without providing a configuration file and then the default configuration options will be considered. If you want to change the server configuration options, you need to change the settings inside the configuration file conf/neo4j-server.properties. In this section, I will talk about some of the main configuration options that you might want to change.   If you want to change the location of the database file that will contain your database changes, you can change the below configuration parameter:  org.neo4j.server.database.location=data/graph.db  The URI path of the HTTP REST API can be changed using the below configuration option:  org.neo4j.server.webadmin.data.uri=/db/data/  Also you can change the transaction timeout, which is the time used by the Neo4j instance to wait for the transaction to commit. After the timeout, the server will perform a roll back for the transaction. This configuration option can be changed as shown below:  org.neo4j.server.transaction.timeout=60  The logging configurations can be changed using the below configuration parameters:  org.neo4j.server.http.log.enabled=true\norg.neo4j.server.http.log.config=conf/neo4j-http-logging.xml  The first option enable or disable logging and the second option specify the logging file.   There are many other configuration options that can be used to configure performance, security and other operating features. For a full list of all the configuration options, please check out the  documentations .", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/data migration/", 
            "text": "back\n\n\nData migration can be done in Neo4j by using the CSV import tool which has been already explained in the \nNeo4j bulk support section\n.", 
            "title": "Data migration"
        }, 
        {
            "location": "/Neo4j/Adminstration/data migration/#back", 
            "text": "Data migration can be done in Neo4j by using the CSV import tool which has been already explained in the  Neo4j bulk support section .", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/persistance/", 
            "text": "back\n\n\nNeo4j is durable and all written data is persisted on disk. As we already know by now, all data writes in Neo4j are handled as transactions and whenever a transaction is marked as successful and committed, it will be written directly to disk.", 
            "title": "Persistance"
        }, 
        {
            "location": "/Neo4j/Adminstration/persistance/#back", 
            "text": "Neo4j is durable and all written data is persisted on disk. As we already know by now, all data writes in Neo4j are handled as transactions and whenever a transaction is marked as successful and committed, it will be written directly to disk.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/refernces/", 
            "text": "back\n\n\n1- http://neo4j.com/docs/\n\n\n2- Graph Databases 2nd Edition. By Ian Robinson, Jim Webber, and Emil Eifr\u00e9m. Publisher: O'Reilly Media.", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Adminstration/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/refernces/#1-httpneo4jcomdocs", 
            "text": "", 
            "title": "1- http://neo4j.com/docs/"
        }, 
        {
            "location": "/Neo4j/Adminstration/refernces/#2-graph-databases-2nd-edition-by-ian-robinson-jim-webber-and-emil-eifrem-publisher-oreilly-media", 
            "text": "", 
            "title": "2- Graph Databases 2nd Edition. By Ian Robinson, Jim Webber, and Emil Eifr\u00e9m. Publisher: O'Reilly Media."
        }, 
        {
            "location": "/Neo4j/Adminstration/scalability/", 
            "text": "back\n\n\nIn this section, I will discuss how Neo4j support scalability in terms of throughput and Capacity.\n\n\nCapacity\n\n\nNeo4j has some upper bound limit for the graph size and can support single graphs having tens of billions of nodes, relationships and properties. The current Neo4j version supports up to around 34 billions of nodes and relationships and around 274 billions of properties. This is quite enough for large graphs of Facebook similar network graph size.\n\n\nThroughput\n\n\nAs explained in the \navailability section\n, Neo4j supports HA master-slave clusters that can linearly scale reads where slaves can share the read load.  As for the write load, only the master instance in the cluster can handle it. Other slave instances can still receive the write requests from clients but then these requests will be forwarded to the master node. This means that Neo4j doesn't scale the write load very well and in case of exceptionally high write loads, only vertical scaling of the master instance is possible. Although it is possible to build some sharding logic in the client application to distribute the data across number of servers, however the sharding logic still not natively supported by Neo4j. This is because sharding the graph is a near impossible or NP hard problem. In general, sharding the data in the client side depends on the graph structure. If the graph has clear boundaries, then it can be sharded easily otherwise it can be really difficult. The future goal of Neo4j is to support sharding across multiple machines without the application-level intervention, however this might take time.", 
            "title": "Scalability"
        }, 
        {
            "location": "/Neo4j/Adminstration/scalability/#back", 
            "text": "In this section, I will discuss how Neo4j support scalability in terms of throughput and Capacity.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/scalability/#capacity", 
            "text": "Neo4j has some upper bound limit for the graph size and can support single graphs having tens of billions of nodes, relationships and properties. The current Neo4j version supports up to around 34 billions of nodes and relationships and around 274 billions of properties. This is quite enough for large graphs of Facebook similar network graph size.", 
            "title": "Capacity"
        }, 
        {
            "location": "/Neo4j/Adminstration/scalability/#throughput", 
            "text": "As explained in the  availability section , Neo4j supports HA master-slave clusters that can linearly scale reads where slaves can share the read load.  As for the write load, only the master instance in the cluster can handle it. Other slave instances can still receive the write requests from clients but then these requests will be forwarded to the master node. This means that Neo4j doesn't scale the write load very well and in case of exceptionally high write loads, only vertical scaling of the master instance is possible. Although it is possible to build some sharding logic in the client application to distribute the data across number of servers, however the sharding logic still not natively supported by Neo4j. This is because sharding the graph is a near impossible or NP hard problem. In general, sharding the data in the client side depends on the graph structure. If the graph has clear boundaries, then it can be sharded easily otherwise it can be really difficult. The future goal of Neo4j is to support sharding across multiple machines without the application-level intervention, however this might take time.", 
            "title": "Throughput"
        }, 
        {
            "location": "/Neo4j/Adminstration/security/", 
            "text": "back\n\n\nNeo4j supports different security options on the data level as well as on the server level. In this section, I will talk about the security options available in Neo4j.\n\n\nData Security\n\n\nNeo4j support all the security methods available in the Java programming language and the JVM to encrypt the data before storing for protection from any unauthorised access. Security isn't handled automatically and needs to be implemented in the upper layers. \n\n\nServer Security\n\n\nThe server instance replies only to requests coming from the localhost and port 7474 to protect the server from malicious requests. To change this setting, you can set the configuration parameter below:\n\n\norg.neo4j.server.webserver.address=0.0.0.0\n// Or \norg.neo4j.server.webserver.port=7474\n\n\n\n\nIn addition, Neo4j requires clients to supply some authentication credentials when trying to call the REST API. The data related to authentication and authorisation is usually stored in the data/dbms/auth folder. It is even possible to disable the authentication and authorisation if it is required in some use cases as seen below:\n\n\ndbms.security.auth_enabled=false\n\n\n\n\nIt is also possible to use SSL secured communication over HTTPS. To provide the key and certificate that will be used during the SSL communication, you need to use the settings below:\n\n\ndbms.security.tls_certificate_file=ssl/snakeoil.cert\n\ndbms.security.tls_key_file=ssl/snakeoil.key\n\n\n\n\nTo use more advanced security options such as using custom security rules or other security proxy server, please read the \ndocumentations\n.", 
            "title": "Security"
        }, 
        {
            "location": "/Neo4j/Adminstration/security/#back", 
            "text": "Neo4j supports different security options on the data level as well as on the server level. In this section, I will talk about the security options available in Neo4j.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/security/#data-security", 
            "text": "Neo4j support all the security methods available in the Java programming language and the JVM to encrypt the data before storing for protection from any unauthorised access. Security isn't handled automatically and needs to be implemented in the upper layers.", 
            "title": "Data Security"
        }, 
        {
            "location": "/Neo4j/Adminstration/security/#server-security", 
            "text": "The server instance replies only to requests coming from the localhost and port 7474 to protect the server from malicious requests. To change this setting, you can set the configuration parameter below:  org.neo4j.server.webserver.address=0.0.0.0\n// Or \norg.neo4j.server.webserver.port=7474  In addition, Neo4j requires clients to supply some authentication credentials when trying to call the REST API. The data related to authentication and authorisation is usually stored in the data/dbms/auth folder. It is even possible to disable the authentication and authorisation if it is required in some use cases as seen below:  dbms.security.auth_enabled=false  It is also possible to use SSL secured communication over HTTPS. To provide the key and certificate that will be used during the SSL communication, you need to use the settings below:  dbms.security.tls_certificate_file=ssl/snakeoil.cert\n\ndbms.security.tls_key_file=ssl/snakeoil.key  To use more advanced security options such as using custom security rules or other security proxy server, please read the  documentations .", 
            "title": "Server Security"
        }, 
        {
            "location": "/Neo4j/Adminstration/upgrade/", 
            "text": "back\n\n\nNeo4j supports automatic upgrade for upgrades between patch releases and within the same minor version. However, upgrading a major release is manual. In this section, I will show how to do manual and automatic upgrade.\n\n\nAutomatic Upgrade\n\n\nFollow the steps below:\n\n\n1- Shut down the older Neo4j version.\n2- Install the new version and configure it to use the same old data store directory.\n3- Take a copy of the database.\n4- Start Neo4j.\n\n\nManual Upgrade\n\n\nFollow the steps below:\n\n\n1- Shut down the older Neo4j version.\n2- Install the new version and configure it to use the same old data store directory.\n3- Configure the configuration parameter allow_store_upgrade=true \n4- Start Neo4j.\n5- Configure the configuration parameter allow_store_upgrade=false", 
            "title": "Upgrade"
        }, 
        {
            "location": "/Neo4j/Adminstration/upgrade/#back", 
            "text": "Neo4j supports automatic upgrade for upgrades between patch releases and within the same minor version. However, upgrading a major release is manual. In this section, I will show how to do manual and automatic upgrade.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Adminstration/upgrade/#automatic-upgrade", 
            "text": "Follow the steps below:  1- Shut down the older Neo4j version.\n2- Install the new version and configure it to use the same old data store directory.\n3- Take a copy of the database.\n4- Start Neo4j.", 
            "title": "Automatic Upgrade"
        }, 
        {
            "location": "/Neo4j/Adminstration/upgrade/#manual-upgrade", 
            "text": "Follow the steps below:  1- Shut down the older Neo4j version.\n2- Install the new version and configure it to use the same old data store directory.\n3- Configure the configuration parameter allow_store_upgrade=true \n4- Start Neo4j.\n5- Configure the configuration parameter allow_store_upgrade=false", 
            "title": "Manual Upgrade"
        }, 
        {
            "location": "/Neo4j/Basic Features/Pipline support/", 
            "text": "back\n\n\nIn case you have a huge data set that you want to insert into Neo4j at once such as when you want to migrate from another datasource, Cypher language supports loading data as parameters. These parameters can be either scalar values, maps, lists or lists of maps. The UNWIND clause can be used to help expanding the imported data and inserting it one by one to create the graph structure. You can also import easily a JSON data pulled from some API as seen below:\n\n\nThe products JSON data:\n\n\n{\n  \nproducts\n : [ {\n    \nname\n : \nT-Shirt\n,\n    \nsize\n : 20,\n    \nprice\n: 200 \n    },\n    {\n    \nname\n : \nLaptop\n,\n    \nsize\n : 15,\n    \nprice\n: 1200 \n  } ]\n}\n\n\n\n\nThen imported as:\n\n\nUNWIND {products} as product\nMERGE (p:Product {name:product.name}) ON CREATE SET p.created = timestamp()\n\n\n\n\nAnother option to import huge set of data is by importing the data from csv files. Cypher supports the LOAD CSV clause that will parse a local or remote file into sequence of rows, then you can use Cypher clauses and operations to create the desired nodes and relationships. Usually it would be better if you load the nodes and relationships from separate CSV files. The example below shows how to load a CSV file contains a set of products with the format below:\n\n\nid,name,size,price\n1,T-Shirt,20,200\n2,Laptop,15,1200\n3,Iphone,5,600\n4,Samsung 5s,4,400\n\n\n\n\nLoad the data as shown below:\n\n\nLOAD CSV WITH HEADERS FROM \nhttp://{url}/products.csv\n AS line\nMERGE (p:Product { id:line.id })\nON CREATE SET p.name=line.name,p.size=line.size,p.price=line.price;\n\n\n\n\nThen to create relationships, we can load the orders and create the relationship between each order and product node during loading as seen below:\n\n\nFirst load the orders:\n\n\nLOAD CSV WITH HEADERS FROM \nhttp://{url}/orders.csv\n AS line\nMERGE (o:Order { id:line.id })\nON CREATE SET o.status=line.status,o.total=line.total;\n\n\n\n\nThen load the OrdersProducts table\n\n\nLOAD CSV WITH HEADERS FROM \nhttp://{url}/OrdersProducts.csv\n AS line\nMATCH (p:Product { id:line.productId })\nMATCH (o:Order { id:line.orderId })\nCREATE (o)-[:Contains]-\n(p);", 
            "title": "Pipline support"
        }, 
        {
            "location": "/Neo4j/Basic Features/Pipline support/#back", 
            "text": "In case you have a huge data set that you want to insert into Neo4j at once such as when you want to migrate from another datasource, Cypher language supports loading data as parameters. These parameters can be either scalar values, maps, lists or lists of maps. The UNWIND clause can be used to help expanding the imported data and inserting it one by one to create the graph structure. You can also import easily a JSON data pulled from some API as seen below:  The products JSON data:  {\n   products  : [ {\n     name  :  T-Shirt ,\n     size  : 20,\n     price : 200 \n    },\n    {\n     name  :  Laptop ,\n     size  : 15,\n     price : 1200 \n  } ]\n}  Then imported as:  UNWIND {products} as product\nMERGE (p:Product {name:product.name}) ON CREATE SET p.created = timestamp()  Another option to import huge set of data is by importing the data from csv files. Cypher supports the LOAD CSV clause that will parse a local or remote file into sequence of rows, then you can use Cypher clauses and operations to create the desired nodes and relationships. Usually it would be better if you load the nodes and relationships from separate CSV files. The example below shows how to load a CSV file contains a set of products with the format below:  id,name,size,price\n1,T-Shirt,20,200\n2,Laptop,15,1200\n3,Iphone,5,600\n4,Samsung 5s,4,400  Load the data as shown below:  LOAD CSV WITH HEADERS FROM  http://{url}/products.csv  AS line\nMERGE (p:Product { id:line.id })\nON CREATE SET p.name=line.name,p.size=line.size,p.price=line.price;  Then to create relationships, we can load the orders and create the relationship between each order and product node during loading as seen below:  First load the orders:  LOAD CSV WITH HEADERS FROM  http://{url}/orders.csv  AS line\nMERGE (o:Order { id:line.id })\nON CREATE SET o.status=line.status,o.total=line.total;  Then load the OrdersProducts table  LOAD CSV WITH HEADERS FROM  http://{url}/OrdersProducts.csv  AS line\nMATCH (p:Product { id:line.productId })\nMATCH (o:Order { id:line.orderId })\nCREATE (o)-[:Contains]- (p);", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Basic Features/basic_features_main/", 
            "text": "back\n\n\nQuery Language\n\n\nTransaction Support\n\n\nData Import and Export\n\n\nRefernces", 
            "title": "Basic features main"
        }, 
        {
            "location": "/Neo4j/Basic Features/basic_features_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Basic Features/basic_features_main/#query-language", 
            "text": "", 
            "title": "Query Language"
        }, 
        {
            "location": "/Neo4j/Basic Features/basic_features_main/#transaction-support", 
            "text": "", 
            "title": "Transaction Support"
        }, 
        {
            "location": "/Neo4j/Basic Features/basic_features_main/#data-import-and-export", 
            "text": "", 
            "title": "Data Import and Export"
        }, 
        {
            "location": "/Neo4j/Basic Features/basic_features_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Basic Features/commands/", 
            "text": "back\n\n\nNeo4j uses a declarative query language called Cypher to interact with the stored data. Cypher is a powerful simple language which is designed to be efficient and human readable. It is inspired by SQL where it uses similar clauses such as WHERE and ORDER BY and its pattern matching expressions are inspired by SPARQL. In the following sections, I will talk in more details about Cypher main concepts such as the Cypher general clauses and how to read/write data using Cypher.\n\n\nCypher Concepts\n\n\nThe node general syntax is shown below:\n\n\n()\n(t-shirt)\n(:Product)\n(t-shirt:Product)\n(t-shirt:Product {name: \nt-shirt\n})\n(t-shirt:Product {name: \nt-shirt\n, size: 20})\n\n\n\n\nNodes are identified using \"()\", above I have shown 6 different ways for representing a node. We can either use general form \"()\" which means a general nameless node. Or we can represent a node with some identifier like \"(t-shirt)\", or with identifier and a label \"(t-shirt:Product)\", or identifier, a label and some property \"(t-shirt:Product {name: \"t-shirt\"})\".\n\n\nThe relationship syntax is somehow similar to the node syntax as shown below:\n\n\n--\n\n-[bought_by_John]-\n\n-[:Bought_By]-\n\n-[bought_by_John:Bought_By]-\n\n-[bought_by_John:Bought_By {date: \n01.01.2016\n}]-\n\n\n\n\n\nAs seen above , we can represent a general directed relationship using \"--\n\" or \"\n--\" based on the relationship direction or we can represent a relationship either by using an identifier \"-[bought_by_John]-\n\", a relationship type only \"-[:Bought_By]-\n\", an identifier and a relationship type  \"-[bought_by_John:Bought_By]-\n\", or an identifier, relationship type and some property \"-[bought_by_John:Bought_By {date: \"01.01.2016\"}]-\n\".\n\n\nAnother important concept is the pattern concept. Nodes are connected with each other using relationships forming different patterns that are used by Neo4j to answer different queries. In general, nodes and relationships alone encode little information, however patterns of nodes and relationships can encode very complex queries.  Cypher is based mainly on patterns. Using patterns, Cypher can find different pattern structures on the graph and answer queries efficiently. An example of a pattern is shown below:\n\n\n(:Product) -[:Bought_BY]-\n (:Customer) -[:Lives_In]-\n (:Country)\n\n\n\n\nIn the above pattern, Neo4j can find all products that are bought by customers living in a specific country.\n\n\nThe last concept is the concept of clauses. Cypher statements contains multiple clauses to create, match patterns, filter, paginate, or sort results. In the below sections, I will talk about Cypher general clauses then I will talk about Cypher clauses used for reading and writing data. \n\n\nGeneral Clauses\n\n\nIn this section I will talk about Cypher general clauses such as RETURN, ORDER BY, LIMIT, SKIP, WITH, UNWIND and UNION.\n\n\nRETURN clause is used to define what to return in a query. It acts like the SELECT clause in SQL. The return results specified by RETURN can be either a node, a relationship or specific properties for a node or a relationship. Example is shown below:\n\n\nMATCH (product:Product)\nRETURN product.price\n\n\n\n\nWe can use expressions with the RETURN clause as shown below:\n\n\nMATCH (product:Product)\nRETURN (product.price \n 20) \n\n\n\n\nThe above query will return true or false based on the price of the product.\n\n\nThe DISTINCT keyword is used to remove duplicate from the results as shown in the below example:\n\n\nMATCH (product:Product)\nRETURN DISTINCT product.name \n\n\n\n\nORDER BY clause is used for sorting the results by a certain property in a similar way like SQL as seen in the below example:\n\n\nMATCH (product:Product)\nRETURN product.name\nORDER BY product.price\n\n\n\n\nLIMIT and SKIP is used for paginate results as show below:\n\n\nFirst page:\n\n\nMATCH (product:Product)\nRETURN product.name\nORDER BY product.price\nLIMIT 3\nSKIP 0\n\n\n\n\nSecond Page:\n\n\nMATCH (product:Product)\nRETURN product.name\nORDER BY product.price\nLIMIT 3\nSKIP 3\n\n\n\n\nThe WITH clause is used to pipe the results into multiple stages as seen in the query below:\n\n\nMATCH (product:Product)\nWITH product.name,product.price\nWHERE product.price \n 20\nRETURN product.name\n\n\n\n\nUNWIND is used if you want to expand a collection into a sequence of rows, example is shown below:\n\n\nUNWIND[1,2,3] AS x\nRETURN x\n\n\n\n\nUNION is used in a similar way like SQL to combine the results from two or more queries as shown below:\n\n\nMATCH (n:Customer) \nRETURN n.customerName AS name\nUNION ALL \nMATCH (n:Supplier)\nRETURN n.supplierName AS name\n\n\n\n\nReading Data\n\n\nThe main Cypher clauses used for reading data are MATCH and WHERE. MATCH clause is used to search the graph to find certain matching patterns. Usually MATCH clause is used with the WHERE clause to filter the results. An example is shown below:\n\n\nMATCH (o:Order)-[:Submitted_BY]-\n (:Customer) -[:Lives_In]-\n (c:Country)\nwhere o.status = 'Paid'\nRETURN c.name, count(o.orderID) AS orderCount\nORDER BY orderCount\n\n\n\n\nThe WHERE clause can be used either to find nodes with a particular label, relationships with certain relationship type, or to search for nodes with specific property value. The WHERE clause supports the use of boolean operators such as OR,AND and XOR.  Similarly it can be used with string matching patterns operators such as START WITH, ENDS WITH and CONTAINS. Regular expressions is also supported as will be explained in \nlater section\n.\n\n\nWriting Data\n\n\nIn this section, I will explain how to use Cypher clauses to create, update and delete data stored in Neo4j.\n\n\nTo create a node or a relationship, the CREATE clause can be used. Example shown below to create a product node:\n\n\nCREATE (product1:Product { name : 'T-Shirt', size : '20', price: '200' })\n\n\n\n\nCreating a relationship is shown below:\n\n\nCREATE (product1:Product)-[r:Bought_BY { date : '01.01.2016'}]-\n(customer1:Customer))\n\n\n\n\nIf you want to create a pattern but also ensures not to create a duplicate, you can use MERGE instead of CREATE. MERGE will first check if the pattern exists, and if so it won't create a new one. Otherwise it will create the pattern. So it is like a combination of MATCH and CREATE. MERGE can be used also with ON CREATE and ON MATCH options. For example when creating a new order, you can use either ON CREATE or ON MATCH to set the creation timestamp. If you use ON MATCH, it will take the time of the matching. However if you use ON CREATE, it will take the creation timestamp. Example is shown below:\n\n\nMERGE (order1:Order)-[r:Contains]-\n (product1:Product)\nON CREATE SET order1.created = timestamp()\nON MATCH SET product1.lastseen = timestamp()\nRETURN product1.name, order1.created, product1.lastseen\n\n\n\n\nTo update the node label or its properties or the relationship type or its properties, you can use SET clause. An example is given below:\n\n\nMATCH (n { name: 'T-Shirt' })\nSET n.price = '300'\nRETURN n\n\n\n\n\nTo remove a property of a node or a relationship, the REMOVE clause can be used as shown below:\n\n\nMATCH (n { name: 'T-Shirt' })\nREMOVE n.extraField\nRETURN n\n\n\n\n\nTo delete a relationship or a node, the DELETE clause can be used as shown below:\n\n\nMATCH (product1:Product)\nDELETE product1\n\n\n\n\nCypher also supports FOREACH clause to be used when multiple elements needs to be updated. For instance, if you want to update all products that are having orders you can use the below query:\n\n\nMATCH p =(p:Product)-[*]-\n(o:Order)\nFOREACH (n IN nodes(p)| SET o.status = 'Archived' )", 
            "title": "Commands"
        }, 
        {
            "location": "/Neo4j/Basic Features/commands/#back", 
            "text": "Neo4j uses a declarative query language called Cypher to interact with the stored data. Cypher is a powerful simple language which is designed to be efficient and human readable. It is inspired by SQL where it uses similar clauses such as WHERE and ORDER BY and its pattern matching expressions are inspired by SPARQL. In the following sections, I will talk in more details about Cypher main concepts such as the Cypher general clauses and how to read/write data using Cypher.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Basic Features/commands/#cypher-concepts", 
            "text": "The node general syntax is shown below:  ()\n(t-shirt)\n(:Product)\n(t-shirt:Product)\n(t-shirt:Product {name:  t-shirt })\n(t-shirt:Product {name:  t-shirt , size: 20})  Nodes are identified using \"()\", above I have shown 6 different ways for representing a node. We can either use general form \"()\" which means a general nameless node. Or we can represent a node with some identifier like \"(t-shirt)\", or with identifier and a label \"(t-shirt:Product)\", or identifier, a label and some property \"(t-shirt:Product {name: \"t-shirt\"})\".  The relationship syntax is somehow similar to the node syntax as shown below:  -- \n-[bought_by_John]- \n-[:Bought_By]- \n-[bought_by_John:Bought_By]- \n-[bought_by_John:Bought_By {date:  01.01.2016 }]-   As seen above , we can represent a general directed relationship using \"-- \" or \" --\" based on the relationship direction or we can represent a relationship either by using an identifier \"-[bought_by_John]- \", a relationship type only \"-[:Bought_By]- \", an identifier and a relationship type  \"-[bought_by_John:Bought_By]- \", or an identifier, relationship type and some property \"-[bought_by_John:Bought_By {date: \"01.01.2016\"}]- \".  Another important concept is the pattern concept. Nodes are connected with each other using relationships forming different patterns that are used by Neo4j to answer different queries. In general, nodes and relationships alone encode little information, however patterns of nodes and relationships can encode very complex queries.  Cypher is based mainly on patterns. Using patterns, Cypher can find different pattern structures on the graph and answer queries efficiently. An example of a pattern is shown below:  (:Product) -[:Bought_BY]-  (:Customer) -[:Lives_In]-  (:Country)  In the above pattern, Neo4j can find all products that are bought by customers living in a specific country.  The last concept is the concept of clauses. Cypher statements contains multiple clauses to create, match patterns, filter, paginate, or sort results. In the below sections, I will talk about Cypher general clauses then I will talk about Cypher clauses used for reading and writing data.", 
            "title": "Cypher Concepts"
        }, 
        {
            "location": "/Neo4j/Basic Features/commands/#general-clauses", 
            "text": "In this section I will talk about Cypher general clauses such as RETURN, ORDER BY, LIMIT, SKIP, WITH, UNWIND and UNION.  RETURN clause is used to define what to return in a query. It acts like the SELECT clause in SQL. The return results specified by RETURN can be either a node, a relationship or specific properties for a node or a relationship. Example is shown below:  MATCH (product:Product)\nRETURN product.price  We can use expressions with the RETURN clause as shown below:  MATCH (product:Product)\nRETURN (product.price   20)   The above query will return true or false based on the price of the product.  The DISTINCT keyword is used to remove duplicate from the results as shown in the below example:  MATCH (product:Product)\nRETURN DISTINCT product.name   ORDER BY clause is used for sorting the results by a certain property in a similar way like SQL as seen in the below example:  MATCH (product:Product)\nRETURN product.name\nORDER BY product.price  LIMIT and SKIP is used for paginate results as show below:  First page:  MATCH (product:Product)\nRETURN product.name\nORDER BY product.price\nLIMIT 3\nSKIP 0  Second Page:  MATCH (product:Product)\nRETURN product.name\nORDER BY product.price\nLIMIT 3\nSKIP 3  The WITH clause is used to pipe the results into multiple stages as seen in the query below:  MATCH (product:Product)\nWITH product.name,product.price\nWHERE product.price   20\nRETURN product.name  UNWIND is used if you want to expand a collection into a sequence of rows, example is shown below:  UNWIND[1,2,3] AS x\nRETURN x  UNION is used in a similar way like SQL to combine the results from two or more queries as shown below:  MATCH (n:Customer) \nRETURN n.customerName AS name\nUNION ALL \nMATCH (n:Supplier)\nRETURN n.supplierName AS name", 
            "title": "General Clauses"
        }, 
        {
            "location": "/Neo4j/Basic Features/commands/#reading-data", 
            "text": "The main Cypher clauses used for reading data are MATCH and WHERE. MATCH clause is used to search the graph to find certain matching patterns. Usually MATCH clause is used with the WHERE clause to filter the results. An example is shown below:  MATCH (o:Order)-[:Submitted_BY]-  (:Customer) -[:Lives_In]-  (c:Country)\nwhere o.status = 'Paid'\nRETURN c.name, count(o.orderID) AS orderCount\nORDER BY orderCount  The WHERE clause can be used either to find nodes with a particular label, relationships with certain relationship type, or to search for nodes with specific property value. The WHERE clause supports the use of boolean operators such as OR,AND and XOR.  Similarly it can be used with string matching patterns operators such as START WITH, ENDS WITH and CONTAINS. Regular expressions is also supported as will be explained in  later section .", 
            "title": "Reading Data"
        }, 
        {
            "location": "/Neo4j/Basic Features/commands/#writing-data", 
            "text": "In this section, I will explain how to use Cypher clauses to create, update and delete data stored in Neo4j.  To create a node or a relationship, the CREATE clause can be used. Example shown below to create a product node:  CREATE (product1:Product { name : 'T-Shirt', size : '20', price: '200' })  Creating a relationship is shown below:  CREATE (product1:Product)-[r:Bought_BY { date : '01.01.2016'}]- (customer1:Customer))  If you want to create a pattern but also ensures not to create a duplicate, you can use MERGE instead of CREATE. MERGE will first check if the pattern exists, and if so it won't create a new one. Otherwise it will create the pattern. So it is like a combination of MATCH and CREATE. MERGE can be used also with ON CREATE and ON MATCH options. For example when creating a new order, you can use either ON CREATE or ON MATCH to set the creation timestamp. If you use ON MATCH, it will take the time of the matching. However if you use ON CREATE, it will take the creation timestamp. Example is shown below:  MERGE (order1:Order)-[r:Contains]-  (product1:Product)\nON CREATE SET order1.created = timestamp()\nON MATCH SET product1.lastseen = timestamp()\nRETURN product1.name, order1.created, product1.lastseen  To update the node label or its properties or the relationship type or its properties, you can use SET clause. An example is given below:  MATCH (n { name: 'T-Shirt' })\nSET n.price = '300'\nRETURN n  To remove a property of a node or a relationship, the REMOVE clause can be used as shown below:  MATCH (n { name: 'T-Shirt' })\nREMOVE n.extraField\nRETURN n  To delete a relationship or a node, the DELETE clause can be used as shown below:  MATCH (product1:Product)\nDELETE product1  Cypher also supports FOREACH clause to be used when multiple elements needs to be updated. For instance, if you want to update all products that are having orders you can use the below query:  MATCH p =(p:Product)-[*]- (o:Order)\nFOREACH (n IN nodes(p)| SET o.status = 'Archived' )", 
            "title": "Writing Data"
        }, 
        {
            "location": "/Neo4j/Basic Features/refernces/", 
            "text": "back\n\n\n1- http://neo4j.com/docs\n\n\n2- Learning Neo4j by Rik Van Bruggen", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Basic Features/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Basic Features/refernces/#1-httpneo4jcomdocs", 
            "text": "", 
            "title": "1- http://neo4j.com/docs"
        }, 
        {
            "location": "/Neo4j/Basic Features/refernces/#2-learning-neo4j-by-rik-van-bruggen", 
            "text": "", 
            "title": "2- Learning Neo4j by Rik Van Bruggen"
        }, 
        {
            "location": "/Neo4j/Basic Features/transaction_support/", 
            "text": "back\n\n\nNeo4j is compliant with the ACID properties and provides full transaction support. All operations in Neo4j that changes the graph will be run on a transaction. This means that an update query will be either fully executed or not at all. If you want to execute multiple update statement in one transaction, the following steps needs to be executed:\n\n\n1- First open a transaction.\n\n\n2- Run multiple update statements.\n\n\n3- Commit all of them if no problem occurs.\n\n\n4- Rollback if a problem occurs.\n\n\nUsually to execute transaction, we use a try-catch block where we commit at the end of the try block or rollback inside the catch block. Transactions in Neo4j are having read-committed isolation level which means that we can read only the committed changes.\n\n\nDefault write locks are used in Neo4j whenever we create,update or delete a nodes or a relationships. The locks are added to the transaction and released whenever the transaction completes. However explicit write locks for nodes and relationships can also be enabled to provide higher serialisation isolation level that could be useful for some use cases.\n\n\nFor example, to start a transaction using Cypher HTTP endpoint we issue a POST HTTP request as below:\n\n\nPOST http://localhost:7474/db/data/transaction\nAccept: application/json; charset=UTF-8\nContent-Type: application/json\n\n\n\n\nAnd the body will have the transaction statements:\n\n\n{\n  \nstatements\n : [ {\n    \nstatement\n : \nCREATE (n {props}) RETURN n\n,\n    \nparameters\n : {\n      \nprops\n : {\n        \nid\n : \nOrderID\n,\n        \nstatus\n: \nSubmitted\n,\n        lineitems: [\nproductID\n]\n      }\n    }\n  }, \n  {\n    \nstatement\n : \nMATCH (n { props }) SET n.quantity = n.quantity - 1 RETURN n\n,\n    \nparameters\n : {\n      \nprops\n : {\n        lineitemID: 'productID'\n      }\n    }\n  }]\n}\n\n\n\n\nYou will get a response as shown below:\n\n\n201: Created\nContent-Type: application/json\nLocation: http://localhost:7474/db/data/transaction/1\n\n\n\n\nThen to commit the transaction, you can issue an HTTP request as shown below:\n\n\nPOST http://localhost:7474/db/data/transaction/1\nAccept: application/json; charset=UTF-8\nContent-Type: application/json\n\n\n\n\nAnd the response will be similar to shown below:\n\n\n200: OK\nContent-Type: application/json\n\n\n\n\nTo rollback the transaction you can issue a DELETE request as shown below:\n\n\nDELETE http://localhost:7474/db/data/transaction/1\nAccept: application/json; charset=UTF-8\n\n\n\n\nResponse:\n\n\n200: OK\nContent-Type: application/json; charset=UTF-8", 
            "title": "Transaction support"
        }, 
        {
            "location": "/Neo4j/Basic Features/transaction_support/#back", 
            "text": "Neo4j is compliant with the ACID properties and provides full transaction support. All operations in Neo4j that changes the graph will be run on a transaction. This means that an update query will be either fully executed or not at all. If you want to execute multiple update statement in one transaction, the following steps needs to be executed:  1- First open a transaction.  2- Run multiple update statements.  3- Commit all of them if no problem occurs.  4- Rollback if a problem occurs.  Usually to execute transaction, we use a try-catch block where we commit at the end of the try block or rollback inside the catch block. Transactions in Neo4j are having read-committed isolation level which means that we can read only the committed changes.  Default write locks are used in Neo4j whenever we create,update or delete a nodes or a relationships. The locks are added to the transaction and released whenever the transaction completes. However explicit write locks for nodes and relationships can also be enabled to provide higher serialisation isolation level that could be useful for some use cases.  For example, to start a transaction using Cypher HTTP endpoint we issue a POST HTTP request as below:  POST http://localhost:7474/db/data/transaction\nAccept: application/json; charset=UTF-8\nContent-Type: application/json  And the body will have the transaction statements:  {\n   statements  : [ {\n     statement  :  CREATE (n {props}) RETURN n ,\n     parameters  : {\n       props  : {\n         id  :  OrderID ,\n         status :  Submitted ,\n        lineitems: [ productID ]\n      }\n    }\n  }, \n  {\n     statement  :  MATCH (n { props }) SET n.quantity = n.quantity - 1 RETURN n ,\n     parameters  : {\n       props  : {\n        lineitemID: 'productID'\n      }\n    }\n  }]\n}  You will get a response as shown below:  201: Created\nContent-Type: application/json\nLocation: http://localhost:7474/db/data/transaction/1  Then to commit the transaction, you can issue an HTTP request as shown below:  POST http://localhost:7474/db/data/transaction/1\nAccept: application/json; charset=UTF-8\nContent-Type: application/json  And the response will be similar to shown below:  200: OK\nContent-Type: application/json  To rollback the transaction you can issue a DELETE request as shown below:  DELETE http://localhost:7474/db/data/transaction/1\nAccept: application/json; charset=UTF-8  Response:  200: OK\nContent-Type: application/json; charset=UTF-8", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_layout/", 
            "text": "back\n\n\nNeo4j is based on the property graph model which is similar to an entity relationship diagram. The property graph model is composed of a set of nodes that are connected by relationships. As seen below, the nodes acts as entities that have one or more attributes stored as properties. They can be tagged with one or more labels to distinguish them from other nodes and speed up queries. Relationships can have properties as well and can be tagged with only one relationship type. Relationships must have a start and end node and will be deleted if one of the attached nodes are removed. \n\n\n \n\n\nAs seen from the property graph model of Neo4j, we need to understand our data model and transform it into nodes with labels having relationships between them. Once we designed our data model in the graph model, we can answer our queries easily using the Cypher language. \n\n\nThe first step to design your data model according to the graph model is to describe how exactly your data is related to each other. For example, if you want to transform your B2C data model to the graph model, you would think of a simple statements to describe your model. Let's assume we have only three entities: customers, orders, and products. Then a simple description of the model will be:\n\n\n\"We have some customers who can create some orders and each order can contain some products\"\n\n\nIn order to design the graph model out of the above statement, first we need to extract the nodes and tag them with appropriate labels.\n\n\n\n\nAs seen from the statement above, we can extract three entities: customer, order, and product. They can be easily tagged with \":Customer\", \":Order\", and \":Product\" labels. \n\n\nAfter extracting the nodes and their labels, we should now extract the relationships between them if any. As can be seen from the statement describing the data, we have three different relationships. Each customer creates an order, the relationship can have a type called OWNS_AN or any similar name. Then the order contains one or more products, so the relationship can be of type CONTAINS. So we can extract two relationship types OWNS_AN and CONTAINS. \n\n\nAfter defining the node labels and the relationship types, we can easily build the graph model. For instance let's assume we have a simple data as described by the below two statements:\n\n\n\"A customer called John has created an order containing two products (T-Shirt and a laptop)\"\n\"A customer called Robert has created an order containing one product (Smart TV)\"\n\n\nFrom the data above, we know that we need to create two nodes with label :Customer, two nodes with label :Order and three nodes with label :Product.\n\n\n\n\nAfter creating the nodes and tag them with their corresponding labels, we need to create the relationships between them. From the statements above, we need to create one relationship between john node and order1 with type OWNS_AN, one relationship between Robert node and order2 with type OWNS_AN, two relationships between order1 node and the T-Shirt and Laptop nodes with type CONTAINS and finally a relationship between order2 and the Smart-TV node with type CONTAINS as shown below:\n\n\n\n\nNow after we have successfully created the graph model, we can easily use Cypher language to run simple or complex queries. \n\n\nTo migrate your data model from relational to graph model, you need first to create the labels which corresponds to the table names. Then each row in the relational data model will correspond to a node in the graph model that is tagged with a label corresponds to the table name. Then the columns in the relational tables can be stored as properties for each node ignoring any columns having null values. Table primary keys can be enforced using a unique constraints. Finally table foreign keys will be replaced by relationships representing what the relationship means to other tables and the end node will be the node corresponding to the row in the other table.", 
            "title": "Data layout"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_layout/#back", 
            "text": "Neo4j is based on the property graph model which is similar to an entity relationship diagram. The property graph model is composed of a set of nodes that are connected by relationships. As seen below, the nodes acts as entities that have one or more attributes stored as properties. They can be tagged with one or more labels to distinguish them from other nodes and speed up queries. Relationships can have properties as well and can be tagged with only one relationship type. Relationships must have a start and end node and will be deleted if one of the attached nodes are removed.      As seen from the property graph model of Neo4j, we need to understand our data model and transform it into nodes with labels having relationships between them. Once we designed our data model in the graph model, we can answer our queries easily using the Cypher language.   The first step to design your data model according to the graph model is to describe how exactly your data is related to each other. For example, if you want to transform your B2C data model to the graph model, you would think of a simple statements to describe your model. Let's assume we have only three entities: customers, orders, and products. Then a simple description of the model will be:  \"We have some customers who can create some orders and each order can contain some products\"  In order to design the graph model out of the above statement, first we need to extract the nodes and tag them with appropriate labels.   As seen from the statement above, we can extract three entities: customer, order, and product. They can be easily tagged with \":Customer\", \":Order\", and \":Product\" labels.   After extracting the nodes and their labels, we should now extract the relationships between them if any. As can be seen from the statement describing the data, we have three different relationships. Each customer creates an order, the relationship can have a type called OWNS_AN or any similar name. Then the order contains one or more products, so the relationship can be of type CONTAINS. So we can extract two relationship types OWNS_AN and CONTAINS.   After defining the node labels and the relationship types, we can easily build the graph model. For instance let's assume we have a simple data as described by the below two statements:  \"A customer called John has created an order containing two products (T-Shirt and a laptop)\"\n\"A customer called Robert has created an order containing one product (Smart TV)\"  From the data above, we know that we need to create two nodes with label :Customer, two nodes with label :Order and three nodes with label :Product.   After creating the nodes and tag them with their corresponding labels, we need to create the relationships between them. From the statements above, we need to create one relationship between john node and order1 with type OWNS_AN, one relationship between Robert node and order2 with type OWNS_AN, two relationships between order1 node and the T-Shirt and Laptop nodes with type CONTAINS and finally a relationship between order2 and the Smart-TV node with type CONTAINS as shown below:   Now after we have successfully created the graph model, we can easily use Cypher language to run simple or complex queries.   To migrate your data model from relational to graph model, you need first to create the labels which corresponds to the table names. Then each row in the relational data model will correspond to a node in the graph model that is tagged with a label corresponds to the table name. Then the columns in the relational tables can be stored as properties for each node ignoring any columns having null values. Table primary keys can be enforced using a unique constraints. Finally table foreign keys will be replaced by relationships representing what the relationship means to other tables and the end node will be the node corresponding to the row in the other table.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_modeling_main/", 
            "text": "back\n\n\nData Layout\n\n\nRelational Data\n\n\nReferential Integrity \n\n\nNormalization\n\n\nNested Data\n\n\nRefernces", 
            "title": "Data modeling main"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_modeling_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_modeling_main/#data-layout", 
            "text": "", 
            "title": "Data Layout"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_modeling_main/#relational-data", 
            "text": "", 
            "title": "Relational Data"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_modeling_main/#referential-integrity", 
            "text": "", 
            "title": "Referential Integrity"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_modeling_main/#normalization", 
            "text": "", 
            "title": "Normalization"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_modeling_main/#nested-data", 
            "text": "", 
            "title": "Nested Data"
        }, 
        {
            "location": "/Neo4j/Data Modeling/data_modeling_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Data Modeling/nested_structures/", 
            "text": "back\n\n\nIn Neo4j, the concept of nested data isn't applicable since storing nodes inside other nodes isn't support and it makes more sense to store related data as separate nodes having separate properties and then connect them using relationships.", 
            "title": "Nested structures"
        }, 
        {
            "location": "/Neo4j/Data Modeling/nested_structures/#back", 
            "text": "In Neo4j, the concept of nested data isn't applicable since storing nodes inside other nodes isn't support and it makes more sense to store related data as separate nodes having separate properties and then connect them using relationships.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Data Modeling/normalisation/", 
            "text": "back\n\n\nUsing the graph data model, you can model your data as normalised as you want without any problems. Actually using the graph data model, you can keep your data as it is in the relational model: small, fully normalised and yet represent richly connected entities.", 
            "title": "Normalisation"
        }, 
        {
            "location": "/Neo4j/Data Modeling/normalisation/#back", 
            "text": "Using the graph data model, you can model your data as normalised as you want without any problems. Actually using the graph data model, you can keep your data as it is in the relational model: small, fully normalised and yet represent richly connected entities.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Data Modeling/referential_integerity/", 
            "text": "back\n\n\nReferential integrity isn't enforced in the same way as in relational databases where if one row is deleted, all its related data is also removed. In Neo4j, if we delete a node, all its relationships will be also removed since we can't have a relationship without a start and end node attached to it. However the connected node at the end of the relationship won't be deleted. In other words, data integrity in Neo4j means that there shouldn't be any relationship without start or end node and there shouldn't be any properties that aren't attached to a node or a relationship. \n\n\nIn addition, Neo4j enforces data integrity through the use of some constraints such as the property unique constraint that ensures the uniqueness of a certain node. For example if you want to ensure that there will be no node with label \":Product\" with the same productID, then you can create a constraint as shown below:\n\n\nCREATE CONSTRAINT ON (p:Product) ASSERT p.ID IS UNIQUE\n\n\n\n\nAnother constraint is that you can ensure that all the nodes with specific label should always have a certain property to enforce a certain schema. To create such a constraint, you can run the below:\n\n\nCREATE CONSTRAINT ON (p:Product) ASSERT exists(p.ID)\n\n\n\n\nIn the above, we create a constraint to ensure that all nodes with label Product should always have an ID property.\n\n\nFor relationships, you can create a constraints only to ensure the existence of a certain property as shown below:\n\n\nCREATE (p:Product)-[r:Part_OF]-\n(c:Category) ASSERT exists(r.since)\n\n\n\n\nIn the above example, we are creating a constraint to make sure that there will be no PART_OF relationship between products and categories without specifying the property \"since when this product is part of this category\".", 
            "title": "Referential integerity"
        }, 
        {
            "location": "/Neo4j/Data Modeling/referential_integerity/#back", 
            "text": "Referential integrity isn't enforced in the same way as in relational databases where if one row is deleted, all its related data is also removed. In Neo4j, if we delete a node, all its relationships will be also removed since we can't have a relationship without a start and end node attached to it. However the connected node at the end of the relationship won't be deleted. In other words, data integrity in Neo4j means that there shouldn't be any relationship without start or end node and there shouldn't be any properties that aren't attached to a node or a relationship.   In addition, Neo4j enforces data integrity through the use of some constraints such as the property unique constraint that ensures the uniqueness of a certain node. For example if you want to ensure that there will be no node with label \":Product\" with the same productID, then you can create a constraint as shown below:  CREATE CONSTRAINT ON (p:Product) ASSERT p.ID IS UNIQUE  Another constraint is that you can ensure that all the nodes with specific label should always have a certain property to enforce a certain schema. To create such a constraint, you can run the below:  CREATE CONSTRAINT ON (p:Product) ASSERT exists(p.ID)  In the above, we create a constraint to ensure that all nodes with label Product should always have an ID property.  For relationships, you can create a constraints only to ensure the existence of a certain property as shown below:  CREATE (p:Product)-[r:Part_OF]- (c:Category) ASSERT exists(r.since)  In the above example, we are creating a constraint to make sure that there will be no PART_OF relationship between products and categories without specifying the property \"since when this product is part of this category\".", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Data Modeling/refernces/", 
            "text": "back\n\n\n1- http://neo4j.com/docs/\n\n\n2- Learning Neo4j by Rik Van Bruggen", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Data Modeling/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Data Modeling/refernces/#1-httpneo4jcomdocs", 
            "text": "", 
            "title": "1- http://neo4j.com/docs/"
        }, 
        {
            "location": "/Neo4j/Data Modeling/refernces/#2-learning-neo4j-by-rik-van-bruggen", 
            "text": "", 
            "title": "2- Learning Neo4j by Rik Van Bruggen"
        }, 
        {
            "location": "/Neo4j/Data Modeling/relational_data/", 
            "text": "back\n\n\nIn Neo4j all relationships regardless of whether they are one-to-one, one-to-many or many-to-many are represented in the same way through one relationship. Although relationships in Neo4j are directed but they can be traversed in both direction which means that you just need to create one relationship between different nodes regardless of the direction. It makes sense to create more than one relationship between two different nodes if they are of a different type or meaning since later you can use these different relationships to answer different queries. For instance, if you want to have a many-to-many relationship between products and their categories, you can just create one relationship either a relationship called PART_OF which started from the nodes with label \":Product\" and ends with nodes with label \":Category\". Or you create a relationship called \"HAVING\" which starts from the nodes with label \":Category\" and ends with nodes with label \":Product\". However you need to create only one relationship and not both because they have the same meaning and will answer the same queries. Assuming we choose to create the relationship called PART_OF which starts from the \":Product\" nodes and ends at the \":Category\" nodes. Then we can answer both below queries using this relationship:\n\n\n\"What are all the products under category A ?\"\n\"What are the categories that contains product A ?\"\n\n\nTo answer the first query, we can run a Cypher query as shown below:\n\n\nMATCH (p:Product)-[r:PART_OF]-\n(c:Category {name: \nA\n})\nRETURN  p;\n\n\n\n\nAnd to answer the second query, we can run a Cypher query as shown below:\n\n\nMATCH (p:Product {name: \nA\n})-[r:PART_OF]-\n(c:Category)\nRETURN  c;\n\n\n\n\nAs seen above, the same relationship can be used to traverse in both direction.\n\n\nTo represent one-to-many or one-to-one relationships, there is no special care. We need just one relationship between the nodes as we did with the many-to-many relationship above and then we can easily run Cypher queries as we want.", 
            "title": "Relational data"
        }, 
        {
            "location": "/Neo4j/Data Modeling/relational_data/#back", 
            "text": "In Neo4j all relationships regardless of whether they are one-to-one, one-to-many or many-to-many are represented in the same way through one relationship. Although relationships in Neo4j are directed but they can be traversed in both direction which means that you just need to create one relationship between different nodes regardless of the direction. It makes sense to create more than one relationship between two different nodes if they are of a different type or meaning since later you can use these different relationships to answer different queries. For instance, if you want to have a many-to-many relationship between products and their categories, you can just create one relationship either a relationship called PART_OF which started from the nodes with label \":Product\" and ends with nodes with label \":Category\". Or you create a relationship called \"HAVING\" which starts from the nodes with label \":Category\" and ends with nodes with label \":Product\". However you need to create only one relationship and not both because they have the same meaning and will answer the same queries. Assuming we choose to create the relationship called PART_OF which starts from the \":Product\" nodes and ends at the \":Category\" nodes. Then we can answer both below queries using this relationship:  \"What are all the products under category A ?\"\n\"What are the categories that contains product A ?\"  To answer the first query, we can run a Cypher query as shown below:  MATCH (p:Product)-[r:PART_OF]- (c:Category {name:  A })\nRETURN  p;  And to answer the second query, we can run a Cypher query as shown below:  MATCH (p:Product {name:  A })-[r:PART_OF]- (c:Category)\nRETURN  c;  As seen above, the same relationship can be used to traverse in both direction.  To represent one-to-many or one-to-one relationships, there is no special care. We need just one relationship between the nodes as we did with the many-to-many relationship above and then we can easily run Cypher queries as we want.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Examples/example_main/", 
            "text": "back\n\n\nTPC-H Queries\n\n\nReferences", 
            "title": "Example main"
        }, 
        {
            "location": "/Neo4j/Examples/example_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Examples/example_main/#tpc-h-queries", 
            "text": "", 
            "title": "TPC-H Queries"
        }, 
        {
            "location": "/Neo4j/Examples/example_main/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/Neo4j/Examples/references/", 
            "text": "1- http://www.tpc.org/tpch", 
            "title": "References"
        }, 
        {
            "location": "/Neo4j/Examples/references/#1-httpwwwtpcorgtpch", 
            "text": "", 
            "title": "1- http://www.tpc.org/tpch"
        }, 
        {
            "location": "/Neo4j/Examples/tpch/", 
            "text": "In this example, I will show how we can model the data of a complete B2C application and how to write complex queries on this data model. I am going to use the data model used by the \nTPC-H benchmark\n which is shown below.\n\n\n\n\nThe idea behind this example is to show how to use Neo4j database to design a data model for a real B2C application. Additionally, we want to see how to write advance SQL queries with joins, grouping and sorting using neo4j Cypher language. I have chosen three queries from the TPCH benchmark that I think will cover most of the common query capabilities of SQL. The chosen queries are Q1, Q3 and Q4 as will be explained in the following sections.\n\n\nThe data used as input for this example is a small set from the data generated using TPCH DBGEN tool. The data is stored as CSV files that correspond to each object in the TPCH benchmark schema shown above (customer, supplier, part, lineitem, order , etc ...). For more details about how to generate the data, please have a look at \nthis\n blog post. \n\n\nThe complete code of this example can be found in Github along with the steps on how to run it. You can also change the used input data by changing the content of the \ninput files\n stored in the \"data\" folder.\n\n\nCreating the data model\n\n\nData model design techniques are explained previously in the \ndata layout section\n. Based on these techniques, we have modelled the TPCH data model as shown below:\n\n\n\n\nAlthough we have 8 entities in the TPCH data model, we have created only 7 node's labels (Supplier,Customer,Order,Nation,Region,Lineitem, and Part) since the Partsupp entity is replaced by a relationship. We have created 6 relationship's types (Contains, Created_By, From, Located_in, Has_details and Supplied_By) that represents all the relationships between the different entities in the TPCH model. The Has_details relationship type is the relationship that replaces the Partsupp entity in the TPCH model and we store inside the relationship the availqty and the supplycost values that comes from the Partsupp entity. \n\n\nNow after defining the Node's labels and relationship's types, we create the actual nodes and relationships and load them with data using a small set of data generated using the TPCH DBGEN tool as explained previously. We store the data as CSV files inside the data folder, then we iterate through the files line by line and create the corresponding nodes and relationships. To see the complete code, please have a look at the \nTPCHModel.java\n file. \n\n\nI will show below the main logic followed and an example code of two functions one to create nodes with label \"Lineitem\" and the other to create relationships with type \"CONTAINS\". All other nodes and relationships are created when calling the initialiseData() function. \n\n\n// Creating all the nodes with label \nLineitem\n\n\nvoid createLineItemNodes() {\n\n        File file = new File(\nsrc/main/java/org/neo4j/tpcH/data/lineitem.txt\n);\n\n        // Create a unique constraint on the LINENUMBER property\n\n        try (Transaction tx = this.graphDb.beginTx()) {\n\n            this.graphDb.schema().constraintFor(DynamicLabel.label(\nLineitem\n))\n                    .assertPropertyIsUnique(\nLINENUMBER\n).create();\n            tx.success();\n        } catch (Exception e) {\n            System.out\n                    .println(\nunique constraint on LINENUMBER creation failed\n);\n            System.out.println(\nError is \n + e.getLocalizedMessage());\n        }\n\n        try (BufferedReader br = new BufferedReader(new FileReader(file))) {\n            String line;\n\n            if (this.graphDb == null) {\n                System.out.println(\ngraphDb is null\n);\n            }\n\n            while ((line = br.readLine()) != null) {\n                // process the line.\n\n                String[] lineFields = line.split(Pattern.quote(\n|\n));\n\n                // System.out.println(Arrays.toString(lineFields));\n\n                try (Transaction tx = this.graphDb.beginTx()) {\n\n                    Node Lineitem = this.graphDb.createNode(DynamicLabel\n                            .label(\nLineitem\n));\n                    Lineitem.setProperty(\nLINENUMBER\n, lineFields[3]);\n                    Lineitem.setProperty(\nQUANTITY\n, Double.valueOf(lineFields[4]));\n                    Lineitem.setProperty(\nEXTENDEDPRICE\n, Double.valueOf(lineFields[5]));\n                    Lineitem.setProperty(\nDISCOUNT\n, Double.valueOf(lineFields[6]));\n                    Lineitem.setProperty(\nTAX\n, Double.valueOf(lineFields[7]));\n                    Lineitem.setProperty(\nRETURNFLAG\n, lineFields[8]);\n                    Lineitem.setProperty(\nLINESTATUS\n, lineFields[9]);\n                    Lineitem.setProperty(\nSHIPDATE\n, format.parse(lineFields[10]).getTime());\n                    Lineitem.setProperty(\nCOMMITDATE\n, format.parse(lineFields[11]).getTime());\n                    Lineitem.setProperty(\nRECEIPTDATE\n, format.parse(lineFields[12]).getTime());\n                    Lineitem.setProperty(\nSHIPINSTRUCT\n, lineFields[13]);\n                    Lineitem.setProperty(\nSHIPMODE\n, lineFields[14]);\n                    Lineitem.setProperty(\nCOMMENT\n, lineFields[15]);\n\n                    System.out.println(\nLineitem node with LINENUMBER: \n\n                            + lineFields[0] + \n have been created\n);\n                    tx.success();\n                } catch (Exception e) {\n                    System.out.println(\nLineitem node with LINENUMBER: \n\n                            + lineFields[0] + \n  creation failed\n);\n                    System.out.println(\nError is \n + e.getLocalizedMessage());\n                }\n\n            }\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n\n    }\n\n\n\n\n\n\nAs seen from the code above, we first create a unique constraint on the LINENUMBER property for all the nodes with the \"Lineitem\" label. This constraint is needed to make sure that we aren't creating the same node twice. Then we iterate through the Lineitem CSV file and create a new node with label \"Lineitem\" for each line in the file. Note that we first create a transaction before creating each node or when creating the unique constraint and commit the transaction at the end if no error was raised. \n\n\nWe do the same as the function above to create nodes for all other node's labels (Supplier,Customer,Order,Nation,Region, and Part). The functions are createCustomerNodes(), createSupplierNodes(), createNationNodes(), createOrderNodes(), createPartNodes() and createRegionNodes().\n\n\nTo create the CONTAINS relationships as an example, we iterate through the Lineitem CSV file. Then for each line we get the order node that has the ORDERKEY as a property inside the node. In the same way, we get the order node that has the LINENUMBER as a property inside the node. Finally we create a relationship that starts from the order node and ends at the lineitem node and make sure that this relationship doesn't already exist to prevent creating the same relationship twice. The code is shown below:\n\n\n\nvoid createContainsRelationship() {\n\n        File file = new File(\nsrc/main/java/org/neo4j/tpcH/data/lineitem.txt\n);\n\n        try (BufferedReader br = new BufferedReader(new FileReader(file))) {\n            String line;\n\n            if (this.graphDb == null) {\n                System.out.println(\ngraphDb is null\n);\n            }\n\n            while ((line = br.readLine()) != null) {\n                // process the line.\n\n                String[] lineFields = line.split(Pattern.quote(\n|\n));\n\n                // System.out.println(Arrays.toString(lineFields));\n\n                try (Transaction tx = this.graphDb.beginTx()) {\n\n                    Node lineitemNode = this.graphDb.findNode(\n                            DynamicLabel.label(\nLineitem\n), \nLINENUMBER\n,\n                            lineFields[3]);\n\n                    Node orderNode = this.graphDb.findNode(\n                            DynamicLabel.label(\nOrder\n), \nORDERKEY\n,\n                            lineFields[0]);\n\n                    Iterable\nRelationship\n orderRelationships = orderNode\n                            .getRelationships();\n                    Iterator\nRelationship\n orderRelationshipsIterator = orderRelationships\n                            .iterator();\n\n                    boolean relExists = false;\n\n                    while (orderRelationshipsIterator.hasNext()) {\n                        Relationship rel = orderRelationshipsIterator.next();\n\n                        if (rel.getEndNode().equals(lineitemNode)) {\n                            relExists = true;\n                        }\n                    }\n\n                    if (!relExists) {\n                        orderNode.createRelationshipTo(lineitemNode,\n                                RelTypes.CONTAINS);\n\n                        System.out\n                                .println(\nCONTAINS relationship have been created\n);\n                    }\n\n                    tx.success();\n                } catch (Exception e) {\n                    System.out.println(\nCONTAINS relationship creation failed\n);\n                    System.out.println(\nError is \n + e.getLocalizedMessage());\n                }\n\n            }\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n\n    }\n\n\n\n\n\nAfter creating all the nodes and relationships, it will look like the below:\n\n\n\n\nNow after we have our data model ready, we can try to run queries on it. As mentioned before, we have chosen three queries from the TPCH benchmarks to try them in Neo4j as will be shown in the following sections.\n\n\nPricing Summary Report Query (Q1)\n\n\nThis query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:\n\n\nselect\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate \n= date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;\n\n\n\n\nWriting a similar query in Neo4j using Cypher is simple as shown below:\n\n\nMATCH (item:Lineitem)\nWHERE item.SHIPDATE  \n= 912524220000\nRETURN item.RETURNFLAG,item.LINESTATUS,sum(item.QUANTITY) AS sum_qty, sum(item.EXTENDEDPRICE) AS sum_base_price, sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)) AS sum_disc_price,sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)*(1+item.TAX)) AS sum_charge,avg(item.QUANTITY) AS avg_qty, avg(item.EXTENDEDPRICE) AS avg_price, avg(item.DISCOUNT) AS avg_disc\nORDER BY item.RETURNFLAG, item.LINESTATUS\n\n\n\n\nBasically, we are searching all the nodes that are having the \"Lineitem\" label, filter the results using the shipdate property of the node and return only the fields that are needed in the original SQL query. You can also easily use mathematical operations on the returned fields exactly as we do with SQL and use the built-in aggregate function of Neo4j such as min, max, sum, avg, and count.  The aggregation is done automatically in Cypher and there is no need to provide a group by statement. Sorting is done using the ORDER BY statement exactly as we use it in SQL. The only difference is that the Cypher language has no date support, therefore we have stored the date properties as long value representing the Epoch time. When you store the date using the Epoch long value, we will be able to run range queries on the property as we have done above.\n\n\nThe complete implementation of the API call that will retrieve the results for Q1 is shown below:\n\n\n\n    @GET\n    @Path(\n/q1\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q1 using this model\n, notes = \nTPCH Queries\n, response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public String getQ1Results() {\n\n        Result result = null;\n\n\n        try {\n\n            try (Transaction trans = this.graphDb.beginTx()) {\n\n                String query1 = \nMATCH (item:Lineitem)\\n\n +\n                        \nWHERE item.SHIPDATE  \n= \n + format.parse(\n1998-12-01\n).getTime() + \n \\n\n +\n                        \nRETURN item.RETURNFLAG,item.LINESTATUS,sum(item.QUANTITY) AS sum_qty, sum(item.EXTENDEDPRICE) AS sum_base_price, sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)) AS sum_disc_price,\\n\n +\n                        \nsum(item.EXTENDEDPRICE*(1-item.DISCOUNT)*(1+item.TAX)) AS sum_charge,avg(item.QUANTITY) AS avg_qty, avg(item.EXTENDEDPRICE) AS avg_price, avg(item.DISCOUNT) AS avg_disc\\n\n +\n                        \nORDER BY item.RETURNFLAG, item.LINESTATUS\n;\n\n                result = this.graphDb\n                        .execute(query1);\n\n                trans.success();\n            }\n\n            return result.resultAsString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\nA sample result of the above query is shown below:\n\n\n\n\nShipping Priority Query (Q3)\n\n\nThis query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:\n\n\nselect\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate \n date '[DATE]'\n and l_shipdate \n date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;\n\n\n\n\nWriting a similar Cypher query for the above SQL is also simple as shown below:\n\n\nMATCH  (item:Lineitem) \n-[:CONTAINS]- (order:Order ) -[:CREATED_BY]-\n (customer:Customer)\nWHERE order.ORDERDATE \n 912524220000 AND item.SHIPDATE \n 631205820000 AND customer.MKTSEGMENT = 'AUTOMOBILE' \nRETURN order.ORDERKEY, sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)) AS REVENUE, order.ORDERDATE, order.SHIPPRIORITY\nORDER BY REVENUE DESC, order.ORDERDATE\n\n\n\n\nAs we can see above, representing relationships and joins in Cypher is very simple where we just want to use the MATCH statement (MATCH  (item:Lineitem) \n-[:CONTAINS]- (order:Order ) -[:CREATED_BY]-\n (customer:Customer)). We are searching for any node that have the \"Order\" label and is connected to another node with label \"Lineitem\" by a relationship with type \"Contains\". Then we filter the results by the order.ORDERDATE, item.SHIPDATE and  customer.MKTSEGMENT using the WHERE statement. Aggregation and ordering is done in a similar way like Q1.\n\n\nThe complete implementation of the API call that will retrieve the results for Q3 is shown below:\n\n\n\n    @GET\n    @Path(\n/q3\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q3 using this model\n, notes = \nTPCH Queries\n, response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public String getQ3Results() {\n\n        Result result = null;\n\n        try {\n            try (Transaction trans = this.graphDb.beginTx()) {\n\n                String query3 = \nMATCH  (item:Lineitem) \n-[:CONTAINS]- (order:Order ) -[:CREATED_BY]-\n (customer:Customer)\\n\n +\n                        \nWHERE order.ORDERDATE \n 912524220000 AND item.SHIPDATE \n 631205820000 AND customer.MKTSEGMENT = 'AUTOMOBILE' \\n\n +\n                        \nRETURN order.ORDERKEY, sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)) AS REVENUE, order.ORDERDATE, order.SHIPPRIORITY\\n\n +\n                        \nORDER BY REVENUE DESC, order.ORDERDATE\n;\n\n                result = this.graphDb\n                        .execute(query3);\n\n                trans.success();\n            }\n\n            return result.resultAsString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\nIf you call the above API, you will get a reply like the one shown below:\n\n\n\n\nOrder Priority Checking Query (Q4)\n\n\nThis query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:\n\n\nselect\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate \n= date '[DATE]'\n and o_orderdate \n date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate \n l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;\n\n\n\n\nThe above query can be represented in Cypher as shown below:\n\n\nMATCH  (order:Order) -[:CONTAINS]-\n (item:Lineitem) \nWHERE item.COMMITDATE \n item.RECEIPTDATE AND order.ORDERDATE \n= 631205820000 AND order.ORDERDATE \n 912524220000\nRETURN order.ORDERPRIORITY, count(*) AS ORDER_COUNT\nORDER BY order.ORDERPRIORITY\n\n\n\n\nSo we have represented the join part between the Lineitem entity and the Order entity using the MATCH clause (MATCH  (order:Order) -[:CONTAINS]-\n (item:Lineitem)) so that we search for any node having the \"Lineitem\" label that is connected to a node with the \"Order\" label by a relationship with type \"CONTAINS\". Then we filter the results using the item.COMMITDATE and the order.ORDERDATE. Finally we return only the fields that we require and sort the data using the ORDER BY clause. \n\n\nThe complete implementation of the API call that will retrieve the results for Q4 is shown below:\n\n\n    @GET\n    @Path(\n/q4\n)\n    @Timed\n    @ApiOperation(value = \nget result of TPCH Q4 using this model\n, notes = \nTPCH Queries\n, response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message = \ninternal server error !\n)})\n    public String getQ4Results() {\n\n        Result result = null;\n\n        try {\n            try (Transaction trans = this.graphDb.beginTx()) {\n\n                String query4 = \nMATCH  (order:Order) -[:CONTAINS]-\n (item:Lineitem) \\n\n +\n                        \nWHERE item.COMMITDATE \n item.RECEIPTDATE AND order.ORDERDATE \n= 631205820000 AND order.ORDERDATE \n 912524220000\\n\n +\n                        \nRETURN order.ORDERPRIORITY, count(*) AS ORDER_COUNT\\n\n +\n                        \nORDER BY order.ORDERPRIORITY\n;\n\n                result = this.graphDb\n                        .execute(query4);\n\n                trans.success();\n            }\n\n            return result.resultAsString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ninternal server error !\n + e.getLocalizedMessage());\n            final String shortReason = \ninternal server error !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIf you call the above API, you will get results as shown below:", 
            "title": "Tpch"
        }, 
        {
            "location": "/Neo4j/Examples/tpch/#creating-the-data-model", 
            "text": "Data model design techniques are explained previously in the  data layout section . Based on these techniques, we have modelled the TPCH data model as shown below:   Although we have 8 entities in the TPCH data model, we have created only 7 node's labels (Supplier,Customer,Order,Nation,Region,Lineitem, and Part) since the Partsupp entity is replaced by a relationship. We have created 6 relationship's types (Contains, Created_By, From, Located_in, Has_details and Supplied_By) that represents all the relationships between the different entities in the TPCH model. The Has_details relationship type is the relationship that replaces the Partsupp entity in the TPCH model and we store inside the relationship the availqty and the supplycost values that comes from the Partsupp entity.   Now after defining the Node's labels and relationship's types, we create the actual nodes and relationships and load them with data using a small set of data generated using the TPCH DBGEN tool as explained previously. We store the data as CSV files inside the data folder, then we iterate through the files line by line and create the corresponding nodes and relationships. To see the complete code, please have a look at the  TPCHModel.java  file.   I will show below the main logic followed and an example code of two functions one to create nodes with label \"Lineitem\" and the other to create relationships with type \"CONTAINS\". All other nodes and relationships are created when calling the initialiseData() function.   // Creating all the nodes with label  Lineitem \n\nvoid createLineItemNodes() {\n\n        File file = new File( src/main/java/org/neo4j/tpcH/data/lineitem.txt );\n\n        // Create a unique constraint on the LINENUMBER property\n\n        try (Transaction tx = this.graphDb.beginTx()) {\n\n            this.graphDb.schema().constraintFor(DynamicLabel.label( Lineitem ))\n                    .assertPropertyIsUnique( LINENUMBER ).create();\n            tx.success();\n        } catch (Exception e) {\n            System.out\n                    .println( unique constraint on LINENUMBER creation failed );\n            System.out.println( Error is   + e.getLocalizedMessage());\n        }\n\n        try (BufferedReader br = new BufferedReader(new FileReader(file))) {\n            String line;\n\n            if (this.graphDb == null) {\n                System.out.println( graphDb is null );\n            }\n\n            while ((line = br.readLine()) != null) {\n                // process the line.\n\n                String[] lineFields = line.split(Pattern.quote( | ));\n\n                // System.out.println(Arrays.toString(lineFields));\n\n                try (Transaction tx = this.graphDb.beginTx()) {\n\n                    Node Lineitem = this.graphDb.createNode(DynamicLabel\n                            .label( Lineitem ));\n                    Lineitem.setProperty( LINENUMBER , lineFields[3]);\n                    Lineitem.setProperty( QUANTITY , Double.valueOf(lineFields[4]));\n                    Lineitem.setProperty( EXTENDEDPRICE , Double.valueOf(lineFields[5]));\n                    Lineitem.setProperty( DISCOUNT , Double.valueOf(lineFields[6]));\n                    Lineitem.setProperty( TAX , Double.valueOf(lineFields[7]));\n                    Lineitem.setProperty( RETURNFLAG , lineFields[8]);\n                    Lineitem.setProperty( LINESTATUS , lineFields[9]);\n                    Lineitem.setProperty( SHIPDATE , format.parse(lineFields[10]).getTime());\n                    Lineitem.setProperty( COMMITDATE , format.parse(lineFields[11]).getTime());\n                    Lineitem.setProperty( RECEIPTDATE , format.parse(lineFields[12]).getTime());\n                    Lineitem.setProperty( SHIPINSTRUCT , lineFields[13]);\n                    Lineitem.setProperty( SHIPMODE , lineFields[14]);\n                    Lineitem.setProperty( COMMENT , lineFields[15]);\n\n                    System.out.println( Lineitem node with LINENUMBER:  \n                            + lineFields[0] +   have been created );\n                    tx.success();\n                } catch (Exception e) {\n                    System.out.println( Lineitem node with LINENUMBER:  \n                            + lineFields[0] +    creation failed );\n                    System.out.println( Error is   + e.getLocalizedMessage());\n                }\n\n            }\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n\n    }  As seen from the code above, we first create a unique constraint on the LINENUMBER property for all the nodes with the \"Lineitem\" label. This constraint is needed to make sure that we aren't creating the same node twice. Then we iterate through the Lineitem CSV file and create a new node with label \"Lineitem\" for each line in the file. Note that we first create a transaction before creating each node or when creating the unique constraint and commit the transaction at the end if no error was raised.   We do the same as the function above to create nodes for all other node's labels (Supplier,Customer,Order,Nation,Region, and Part). The functions are createCustomerNodes(), createSupplierNodes(), createNationNodes(), createOrderNodes(), createPartNodes() and createRegionNodes().  To create the CONTAINS relationships as an example, we iterate through the Lineitem CSV file. Then for each line we get the order node that has the ORDERKEY as a property inside the node. In the same way, we get the order node that has the LINENUMBER as a property inside the node. Finally we create a relationship that starts from the order node and ends at the lineitem node and make sure that this relationship doesn't already exist to prevent creating the same relationship twice. The code is shown below:  \nvoid createContainsRelationship() {\n\n        File file = new File( src/main/java/org/neo4j/tpcH/data/lineitem.txt );\n\n        try (BufferedReader br = new BufferedReader(new FileReader(file))) {\n            String line;\n\n            if (this.graphDb == null) {\n                System.out.println( graphDb is null );\n            }\n\n            while ((line = br.readLine()) != null) {\n                // process the line.\n\n                String[] lineFields = line.split(Pattern.quote( | ));\n\n                // System.out.println(Arrays.toString(lineFields));\n\n                try (Transaction tx = this.graphDb.beginTx()) {\n\n                    Node lineitemNode = this.graphDb.findNode(\n                            DynamicLabel.label( Lineitem ),  LINENUMBER ,\n                            lineFields[3]);\n\n                    Node orderNode = this.graphDb.findNode(\n                            DynamicLabel.label( Order ),  ORDERKEY ,\n                            lineFields[0]);\n\n                    Iterable Relationship  orderRelationships = orderNode\n                            .getRelationships();\n                    Iterator Relationship  orderRelationshipsIterator = orderRelationships\n                            .iterator();\n\n                    boolean relExists = false;\n\n                    while (orderRelationshipsIterator.hasNext()) {\n                        Relationship rel = orderRelationshipsIterator.next();\n\n                        if (rel.getEndNode().equals(lineitemNode)) {\n                            relExists = true;\n                        }\n                    }\n\n                    if (!relExists) {\n                        orderNode.createRelationshipTo(lineitemNode,\n                                RelTypes.CONTAINS);\n\n                        System.out\n                                .println( CONTAINS relationship have been created );\n                    }\n\n                    tx.success();\n                } catch (Exception e) {\n                    System.out.println( CONTAINS relationship creation failed );\n                    System.out.println( Error is   + e.getLocalizedMessage());\n                }\n\n            }\n\n        } catch (FileNotFoundException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n\n    }  After creating all the nodes and relationships, it will look like the below:   Now after we have our data model ready, we can try to run queries on it. As mentioned before, we have chosen three queries from the TPCH benchmarks to try them in Neo4j as will be shown in the following sections.", 
            "title": "Creating the data model"
        }, 
        {
            "location": "/Neo4j/Examples/tpch/#pricing-summary-report-query-q1", 
            "text": "This query is used to report the amount of billed, shipped and returned items. The SQL query is shown below:  select\n   l_returnflag,\n   l_linestatus,\n   sum(l_quantity) as sum_qty,\n   sum(l_extendedprice) as sum_base_price,\n   sum(l_extendedprice*(1-l_discount)) as sum_disc_price,\n   sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,\n   avg(l_quantity) as avg_qty,\n   avg(l_extendedprice) as avg_price,\n   avg(l_discount) as avg_disc,\n   count(*) as count_order\nfrom\n   lineitem\nwhere\n   l_shipdate  = date '1998-12-01' - interval '[DELTA]' day (3)\ngroup by\n   l_returnflag,\n   l_linestatus\norder by\n   l_returnflag,\n   l_linestatus;  Writing a similar query in Neo4j using Cypher is simple as shown below:  MATCH (item:Lineitem)\nWHERE item.SHIPDATE   = 912524220000\nRETURN item.RETURNFLAG,item.LINESTATUS,sum(item.QUANTITY) AS sum_qty, sum(item.EXTENDEDPRICE) AS sum_base_price, sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)) AS sum_disc_price,sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)*(1+item.TAX)) AS sum_charge,avg(item.QUANTITY) AS avg_qty, avg(item.EXTENDEDPRICE) AS avg_price, avg(item.DISCOUNT) AS avg_disc\nORDER BY item.RETURNFLAG, item.LINESTATUS  Basically, we are searching all the nodes that are having the \"Lineitem\" label, filter the results using the shipdate property of the node and return only the fields that are needed in the original SQL query. You can also easily use mathematical operations on the returned fields exactly as we do with SQL and use the built-in aggregate function of Neo4j such as min, max, sum, avg, and count.  The aggregation is done automatically in Cypher and there is no need to provide a group by statement. Sorting is done using the ORDER BY statement exactly as we use it in SQL. The only difference is that the Cypher language has no date support, therefore we have stored the date properties as long value representing the Epoch time. When you store the date using the Epoch long value, we will be able to run range queries on the property as we have done above.  The complete implementation of the API call that will retrieve the results for Q1 is shown below:  \n    @GET\n    @Path( /q1 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q1 using this model , notes =  TPCH Queries , response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public String getQ1Results() {\n\n        Result result = null;\n\n\n        try {\n\n            try (Transaction trans = this.graphDb.beginTx()) {\n\n                String query1 =  MATCH (item:Lineitem)\\n  +\n                         WHERE item.SHIPDATE   =   + format.parse( 1998-12-01 ).getTime() +   \\n  +\n                         RETURN item.RETURNFLAG,item.LINESTATUS,sum(item.QUANTITY) AS sum_qty, sum(item.EXTENDEDPRICE) AS sum_base_price, sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)) AS sum_disc_price,\\n  +\n                         sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)*(1+item.TAX)) AS sum_charge,avg(item.QUANTITY) AS avg_qty, avg(item.EXTENDEDPRICE) AS avg_price, avg(item.DISCOUNT) AS avg_disc\\n  +\n                         ORDER BY item.RETURNFLAG, item.LINESTATUS ;\n\n                result = this.graphDb\n                        .execute(query1);\n\n                trans.success();\n            }\n\n            return result.resultAsString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  A sample result of the above query is shown below:", 
            "title": "Pricing Summary Report Query (Q1)"
        }, 
        {
            "location": "/Neo4j/Examples/tpch/#shipping-priority-query-q3", 
            "text": "This query is used to get the 10 unshipped orders with the highest value. In order to do that, the query joins three tables (customer, order and lineitem) as shown below:  select\n l_orderkey,\n sum(l_extendedprice*(1-l_discount)) as revenue,\n o_orderdate,\n o_shippriority\nfrom\n customer,\n orders,\n lineitem\nwhere\n c_mktsegment = '[SEGMENT]'\n and c_custkey = o_custkey\n and l_orderkey = o_orderkey\n and o_orderdate   date '[DATE]'\n and l_shipdate   date '[DATE]'\ngroup by\n l_orderkey,\n o_orderdate,\n o_shippriority\norder by\n revenue desc,\n o_orderdate;  Writing a similar Cypher query for the above SQL is also simple as shown below:  MATCH  (item:Lineitem)  -[:CONTAINS]- (order:Order ) -[:CREATED_BY]-  (customer:Customer)\nWHERE order.ORDERDATE   912524220000 AND item.SHIPDATE   631205820000 AND customer.MKTSEGMENT = 'AUTOMOBILE' \nRETURN order.ORDERKEY, sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)) AS REVENUE, order.ORDERDATE, order.SHIPPRIORITY\nORDER BY REVENUE DESC, order.ORDERDATE  As we can see above, representing relationships and joins in Cypher is very simple where we just want to use the MATCH statement (MATCH  (item:Lineitem)  -[:CONTAINS]- (order:Order ) -[:CREATED_BY]-  (customer:Customer)). We are searching for any node that have the \"Order\" label and is connected to another node with label \"Lineitem\" by a relationship with type \"Contains\". Then we filter the results by the order.ORDERDATE, item.SHIPDATE and  customer.MKTSEGMENT using the WHERE statement. Aggregation and ordering is done in a similar way like Q1.  The complete implementation of the API call that will retrieve the results for Q3 is shown below:  \n    @GET\n    @Path( /q3 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q3 using this model , notes =  TPCH Queries , response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public String getQ3Results() {\n\n        Result result = null;\n\n        try {\n            try (Transaction trans = this.graphDb.beginTx()) {\n\n                String query3 =  MATCH  (item:Lineitem)  -[:CONTAINS]- (order:Order ) -[:CREATED_BY]-  (customer:Customer)\\n  +\n                         WHERE order.ORDERDATE   912524220000 AND item.SHIPDATE   631205820000 AND customer.MKTSEGMENT = 'AUTOMOBILE' \\n  +\n                         RETURN order.ORDERKEY, sum(item.EXTENDEDPRICE*(1-item.DISCOUNT)) AS REVENUE, order.ORDERDATE, order.SHIPPRIORITY\\n  +\n                         ORDER BY REVENUE DESC, order.ORDERDATE ;\n\n                result = this.graphDb\n                        .execute(query3);\n\n                trans.success();\n            }\n\n            return result.resultAsString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above API, you will get a reply like the one shown below:", 
            "title": "Shipping Priority Query (Q3)"
        }, 
        {
            "location": "/Neo4j/Examples/tpch/#order-priority-checking-query-q4", 
            "text": "This query is used to see how well the order priority system is working and gives indication of the customer satisfaction. The SQL query is shown below:  select\n o_orderpriority,\n count(*) as order_count\nfrom\n orders\nwhere\n o_orderdate  = date '[DATE]'\n and o_orderdate   date '[DATE]' + interval '3' month\n and exists (\nselect\n *\nfrom\n lineitem\nwhere\n l_orderkey = o_orderkey\n and l_commitdate   l_receiptdate\n)\ngroup by\n o_orderpriority\norder by\n o_orderpriority;  The above query can be represented in Cypher as shown below:  MATCH  (order:Order) -[:CONTAINS]-  (item:Lineitem) \nWHERE item.COMMITDATE   item.RECEIPTDATE AND order.ORDERDATE  = 631205820000 AND order.ORDERDATE   912524220000\nRETURN order.ORDERPRIORITY, count(*) AS ORDER_COUNT\nORDER BY order.ORDERPRIORITY  So we have represented the join part between the Lineitem entity and the Order entity using the MATCH clause (MATCH  (order:Order) -[:CONTAINS]-  (item:Lineitem)) so that we search for any node having the \"Lineitem\" label that is connected to a node with the \"Order\" label by a relationship with type \"CONTAINS\". Then we filter the results using the item.COMMITDATE and the order.ORDERDATE. Finally we return only the fields that we require and sort the data using the ORDER BY clause.   The complete implementation of the API call that will retrieve the results for Q4 is shown below:      @GET\n    @Path( /q4 )\n    @Timed\n    @ApiOperation(value =  get result of TPCH Q4 using this model , notes =  TPCH Queries , response = String.class)\n    @ApiResponses(value = {@ApiResponse(code = 500, message =  internal server error ! )})\n    public String getQ4Results() {\n\n        Result result = null;\n\n        try {\n            try (Transaction trans = this.graphDb.beginTx()) {\n\n                String query4 =  MATCH  (order:Order) -[:CONTAINS]-  (item:Lineitem) \\n  +\n                         WHERE item.COMMITDATE   item.RECEIPTDATE AND order.ORDERDATE  = 631205820000 AND order.ORDERDATE   912524220000\\n  +\n                         RETURN order.ORDERPRIORITY, count(*) AS ORDER_COUNT\\n  +\n                         ORDER BY order.ORDERPRIORITY ;\n\n                result = this.graphDb\n                        .execute(query4);\n\n                trans.success();\n            }\n\n            return result.resultAsString();\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                     internal server error !  + e.getLocalizedMessage());\n            final String shortReason =  internal server error ! ;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  If you call the above API, you will get results as shown below:", 
            "title": "Order Priority Checking Query (Q4)"
        }, 
        {
            "location": "/Neo4j/Getting Started/getting_started_main/", 
            "text": "back\n\n\nOverview\n\n\nInstallation\n\n\nBasic Concepts\n\n\nWhen to Use\n\n\nRefernces", 
            "title": "Getting started main"
        }, 
        {
            "location": "/Neo4j/Getting Started/getting_started_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Getting Started/getting_started_main/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/Neo4j/Getting Started/getting_started_main/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/Neo4j/Getting Started/getting_started_main/#basic-concepts", 
            "text": "", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/Neo4j/Getting Started/getting_started_main/#when-to-use", 
            "text": "", 
            "title": "When to Use"
        }, 
        {
            "location": "/Neo4j/Getting Started/getting_started_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Getting Started/installation/", 
            "text": "back\n\n\nIn this section, I will explain how to install Neo4j in most of the popular operating systems such as windows, Linux and Mac OSX.\n\n\nWindows OS\n\n\nYou can install Neo4j easily by using a windows installer as explained in the following steps:\n\n\n1- Go to Neo4j \ndownload page\n. \n\n\n2- Choose and download the version that goes with your windows architecture.\n\n\n3- Open the installer and follow the promote steps.\n\n\nA more advanced method to have more control over the installation can be done using the Windows PowerShell Module supported by Neo4j. For more details about how to use the PowerShell, please have a look to the \ndocumentation\n.\n\n\nLinux\n\n\nTo install Neo4j in Linux systems, you can easily use the apt-get package manager as shown blow:\n\n\nsudo apt-get install neo4j=2.3.1\n\n\n\n\nAnother way to install Neo4j in Linux-based systems, is by using the distribution package as shown below:\n\n\n1- Go to Neo4j \ndownload page\n. \n\n\n2- Choose and download the appropriate version based on your platform.\n\n\n3- Extract the downloaded tar file using the below command:\n\n\ntar -xf \nfilename\n\n\n\n\n\n4- Start the server using the command:\n\n\nRun: ./bin/neo4j console\n\n\n\n\nMac OSX\n\n\nNeo4j provides a dmg installer that you can use to install Neo4j as shown below:\n\n\n1- Go to Neo4j \ndownload page\n. \n\n\n2- Download the dmg file.\n\n\n3- Open the installer then drag the Neo4j icon to the application folder.\n\n\nThe server can be started and stopped from using the terminal commands \"neo4j start\" and \"neo4j stop\".", 
            "title": "Installation"
        }, 
        {
            "location": "/Neo4j/Getting Started/installation/#back", 
            "text": "In this section, I will explain how to install Neo4j in most of the popular operating systems such as windows, Linux and Mac OSX.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Getting Started/installation/#windows-os", 
            "text": "You can install Neo4j easily by using a windows installer as explained in the following steps:  1- Go to Neo4j  download page .   2- Choose and download the version that goes with your windows architecture.  3- Open the installer and follow the promote steps.  A more advanced method to have more control over the installation can be done using the Windows PowerShell Module supported by Neo4j. For more details about how to use the PowerShell, please have a look to the  documentation .", 
            "title": "Windows OS"
        }, 
        {
            "location": "/Neo4j/Getting Started/installation/#linux", 
            "text": "To install Neo4j in Linux systems, you can easily use the apt-get package manager as shown blow:  sudo apt-get install neo4j=2.3.1  Another way to install Neo4j in Linux-based systems, is by using the distribution package as shown below:  1- Go to Neo4j  download page .   2- Choose and download the appropriate version based on your platform.  3- Extract the downloaded tar file using the below command:  tar -xf  filename   4- Start the server using the command:  Run: ./bin/neo4j console", 
            "title": "Linux"
        }, 
        {
            "location": "/Neo4j/Getting Started/installation/#mac-osx", 
            "text": "Neo4j provides a dmg installer that you can use to install Neo4j as shown below:  1- Go to Neo4j  download page .   2- Download the dmg file.  3- Open the installer then drag the Neo4j icon to the application folder.  The server can be started and stopped from using the terminal commands \"neo4j start\" and \"neo4j stop\".", 
            "title": "Mac OSX"
        }, 
        {
            "location": "/Neo4j/Getting Started/overview/", 
            "text": "back\n\n\nNeo4j is one of the most popular NoSQL graph based databases. It is an open-source database that was implemented using Java and Scala programming languages. It is built completely based on the graph property model from the ground up. Neo4j is compliance with the database ACID properties (Atomicity, Consistency, Isolation, Durability) and supports scalability, availability and fault-tolerance requirements. Therefore it can be used in OLTP production environments. Additionally, Neo4j supports an easy to use declarative, pattern matching query language called Cypher to interact with the stored data. Cypher is very user friendly, understandable and can be used by non-expert users.", 
            "title": "Overview"
        }, 
        {
            "location": "/Neo4j/Getting Started/overview/#back", 
            "text": "Neo4j is one of the most popular NoSQL graph based databases. It is an open-source database that was implemented using Java and Scala programming languages. It is built completely based on the graph property model from the ground up. Neo4j is compliance with the database ACID properties (Atomicity, Consistency, Isolation, Durability) and supports scalability, availability and fault-tolerance requirements. Therefore it can be used in OLTP production environments. Additionally, Neo4j supports an easy to use declarative, pattern matching query language called Cypher to interact with the stored data. Cypher is very user friendly, understandable and can be used by non-expert users.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Getting Started/refernces/", 
            "text": "back\n\n\n1- http://neo4j.com/docs\n\n\n2- Learning Neo4j by Rik Van Bruggen", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Getting Started/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Getting Started/refernces/#1-httpneo4jcomdocs", 
            "text": "", 
            "title": "1- http://neo4j.com/docs"
        }, 
        {
            "location": "/Neo4j/Getting Started/refernces/#2-learning-neo4j-by-rik-van-bruggen", 
            "text": "", 
            "title": "2- Learning Neo4j by Rik Van Bruggen"
        }, 
        {
            "location": "/Neo4j/Getting Started/underline_data_structures/", 
            "text": "back\n\n\nIn this section, I will explain briefly the main concepts used in Neo4j.\n\n\nNodes\n\n\nThe nodes are the fundamental unit used in Neo4j to represent objects or entities that needs to be stored in the database. Each node can store the data related to the entity it represents using map-like properties. For example in a B2C application, each product can be stored as a node and then each node will store the product related information as set of key-value properties as shown below:\n\n\n\n\nRelationships\n\n\nThe connection between the entities are represented as relationships. A node can have one or more relationships with one or more nodes. The relationships are directed which means they must have a start and an end node. These relationships can be used later by Neo4j to find related data or to perform complex queries. The relationships can also store one or more properties to hold relationship related information. An example for a relationship between products and categories is shown below: \n\n\n\n\nAs shown above, the product node has an outgoing relationship while the category node has an incoming relationship. It is also possible that the node has a relationship to itself. Relationships are traversed in both directions which means that there is no need to duplicate a relationship for the opposite direction.\n\n\nProperties\n\n\nAs explained, both nodes and relationships can use one or more properties to store some information about the node or the relationship. Properties are key-value attributes with values having a variety of types such as numeric, strings, booleans or even a collection of these types.  However empty or null values can't be stored. \n\n\nLabels and Relationship Types\n\n\nA useful feature supported by Neo4j is the ability to assign some labels to nodes which can be used later to easily identify a set of related nodes. For example, it would be a good idea to assign a label to all nodes storing products information and differentiate them from the other nodes that store category information as seen below:\n\n\n\n\nNow you can run faster and more efficient queries on the nodes having the \"Product\" label since the queries will be executed only on the part of the graph having this label.  In the other hand, relationships can have a type to group similar relationships and later make it faster for the database to traverse the graph. In the above example, the relationship between the product and the category has the \"IS_PART_OF\" type.\n\n\nTraversal\n\n\nThe traversal is the name given for how the database search the graph to find results of your queries.  It means visiting the nodes and following relationships along the way according to rules given in your query.\n\n\nPaths\n\n\nThe path is basically the traversal result. It contains one or more nodes connected with some relationships. \n\n\nSchema\n\n\nAs most NoSQL database, Neo4j is schema-less. However it supports an optional way of enforcing a schema for your data using constraints. This can be very useful in some production environment and can increase performance or add some modelling benefits.", 
            "title": "Underline data structures"
        }, 
        {
            "location": "/Neo4j/Getting Started/underline_data_structures/#back", 
            "text": "In this section, I will explain briefly the main concepts used in Neo4j.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Getting Started/underline_data_structures/#nodes", 
            "text": "The nodes are the fundamental unit used in Neo4j to represent objects or entities that needs to be stored in the database. Each node can store the data related to the entity it represents using map-like properties. For example in a B2C application, each product can be stored as a node and then each node will store the product related information as set of key-value properties as shown below:", 
            "title": "Nodes"
        }, 
        {
            "location": "/Neo4j/Getting Started/underline_data_structures/#relationships", 
            "text": "The connection between the entities are represented as relationships. A node can have one or more relationships with one or more nodes. The relationships are directed which means they must have a start and an end node. These relationships can be used later by Neo4j to find related data or to perform complex queries. The relationships can also store one or more properties to hold relationship related information. An example for a relationship between products and categories is shown below:    As shown above, the product node has an outgoing relationship while the category node has an incoming relationship. It is also possible that the node has a relationship to itself. Relationships are traversed in both directions which means that there is no need to duplicate a relationship for the opposite direction.", 
            "title": "Relationships"
        }, 
        {
            "location": "/Neo4j/Getting Started/underline_data_structures/#properties", 
            "text": "As explained, both nodes and relationships can use one or more properties to store some information about the node or the relationship. Properties are key-value attributes with values having a variety of types such as numeric, strings, booleans or even a collection of these types.  However empty or null values can't be stored.", 
            "title": "Properties"
        }, 
        {
            "location": "/Neo4j/Getting Started/underline_data_structures/#labels-and-relationship-types", 
            "text": "A useful feature supported by Neo4j is the ability to assign some labels to nodes which can be used later to easily identify a set of related nodes. For example, it would be a good idea to assign a label to all nodes storing products information and differentiate them from the other nodes that store category information as seen below:   Now you can run faster and more efficient queries on the nodes having the \"Product\" label since the queries will be executed only on the part of the graph having this label.  In the other hand, relationships can have a type to group similar relationships and later make it faster for the database to traverse the graph. In the above example, the relationship between the product and the category has the \"IS_PART_OF\" type.", 
            "title": "Labels and Relationship Types"
        }, 
        {
            "location": "/Neo4j/Getting Started/underline_data_structures/#traversal", 
            "text": "The traversal is the name given for how the database search the graph to find results of your queries.  It means visiting the nodes and following relationships along the way according to rules given in your query.", 
            "title": "Traversal"
        }, 
        {
            "location": "/Neo4j/Getting Started/underline_data_structures/#paths", 
            "text": "The path is basically the traversal result. It contains one or more nodes connected with some relationships.", 
            "title": "Paths"
        }, 
        {
            "location": "/Neo4j/Getting Started/underline_data_structures/#schema", 
            "text": "As most NoSQL database, Neo4j is schema-less. However it supports an optional way of enforcing a schema for your data using constraints. This can be very useful in some production environment and can increase performance or add some modelling benefits.", 
            "title": "Schema"
        }, 
        {
            "location": "/Neo4j/Getting Started/when_to_use/", 
            "text": "back\n\n\nNeo4j is based on the graph property model and can handle complex queries very efficiently. Therefore, it is used in many use cases such as software analytics, scientific research, path finding problems, recommendation system, social networks, network management, business intelligence or warehouses and many more.", 
            "title": "When to use"
        }, 
        {
            "location": "/Neo4j/Getting Started/when_to_use/#back", 
            "text": "Neo4j is based on the graph property model and can handle complex queries very efficiently. Therefore, it is used in many use cases such as software analytics, scientific research, path finding problems, recommendation system, social networks, network management, business intelligence or warehouses and many more.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Results/results_main/", 
            "text": "back\n\n\nStrengths and Weaknesses\n\n\nSummary", 
            "title": "Results main"
        }, 
        {
            "location": "/Neo4j/Results/results_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Results/results_main/#strengths-and-weaknesses", 
            "text": "", 
            "title": "Strengths and Weaknesses"
        }, 
        {
            "location": "/Neo4j/Results/results_main/#summary", 
            "text": "", 
            "title": "Summary"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/", 
            "text": "In this section I will talk about the strengths and weaknesses of Neo4j and when using it makes sense.\n\n\nStrengths\n\n\nLike any graph database, Neo4j has the below advantages:\n\n\nPerformance\n\n\nGraph databases as well as Neo4j provides much better performance when it comes to querying deeply connected data that has many relationships expressed with complex joins. In relational databases, join-intensive query performance deteriorates when the dataset gets bigger. However when using graph databases, the performance stays relatively constant even with very large datasets.  This because in the graph data model, the query will check only the part of the graph that will be traversed by the query and not the whole graph.  \n\n\nFlexibility\n\n\nThe graph data model is more natural. It has no impedance mismatch and is whiteboard friendly. This means you can use the language of node, relationships and properties to describe the application domain instead of using complex models such as UML. Then this graph model is directly mapped and implemented in the database. This friendly data model used in the graph databases allows developers to be more productive and reduce project risk. Since the graph model is flexible, you can start with small model and improve it in the future easily by adding more nodes and relationships with fewer migration and maintenance overhead.  \n\n\nPowerful Query Model\n\n\nThe graph query model is so intuitive and makes it very suitable for applications with object oriented, semi-structured and network like data. The graph model is also very natural to express graph related problems such as the path finding problems.  Using this query model, you can write complex high performance traversals that can be beneficial in many use cases.  \n\n\nIn addition to the above advantages of the graph databases, Neo4j also provides the below advantages.\n\n\nEasy to Learn Query Language\n\n\nNeo4j provides a powerful traversal framework using an easy to learn query language called Cypher. Cypher is a declarative query language designed to be an efficient and human readable language.\n\n\nACID Compliant\n\n\nNeo4j is compliant with the ACID properties (Atomicity, Consistency, Isolation, Durability) and provides full transaction support\n\n\nWeaknesses\n\n\nNeo4j has the below main weaknesses:\n\n\nScalability\n\n\nNeo4j supports HA master-slave clusters that can linearly scale reads where slaves can share the read load.  As for the write load, only the master instance in the cluster can handle it. Other slave instances can still receive the write requests from clients but then these requests will be forwarded to the master node. Therefore, writing to the master instance is faster than writing to a slave instance. This means that Neo4j doesn't scale writes very well and in case of exceptionally high write loads, only vertical scaling of the master instance is possible. Although it is possible to implement some sharding logic in the client application to distribute the data across number of servers, however the sharding logic still not natively supported by Neo4j. This is because sharding the graph is a near impossible or NP hard mathematical problem. In general, sharding the data in the client side depends on the graph structure. If the graph has clear boundaries, then it can be sharded easily otherwise it can be really difficult. Additionally it is very difficult to shard a densely connected graph.\n\n\nStorage\n\n\nNeo4j has some upper bound limit for the graph size and can support single graphs having tens of billions of nodes, relationships and properties. The current Neo4j version supports up to around 34 billions of nodes and relationships and around 274 billions of properties. This is quite enough for large graphs of Facebook similar network graph size. These storage constraints doesn't pose any limitations in practice since only big businesses such as google can push these limits and these limits were set for storage optimisations and can be increased in future versions.\n\n\nNo date data type support support\n\n\nNeo4j doesn't have internal support for date data type but this can be overcome using different methods such as storing the Epoch linux long values instead.  \n\n\nSummary\n\n\nNeo4j is suitable for applications where you have very large connected data and you want to run complex join-intensive queries and still get high performance. It can be used for example to run live complex queries (reports) in production data since they will be faster. In addition, Neo4j is suitable if you have a data that will be more naturally represented using the graph model such as network like, semi-structured or highly connected data. However, if you have simple data model that don't require a lot of joining or aggregation, then you might not get any performance improvements if you use Neo4j or the performance advantage will be very small if not worse. Additionally, Neo4j has scalability weaknesses related to scaling writes, hence if your application is expected to have very large write throughputs, then Neo4j is not for you.", 
            "title": "Strengths weakness"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#strengths", 
            "text": "Like any graph database, Neo4j has the below advantages:", 
            "title": "Strengths"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#performance", 
            "text": "Graph databases as well as Neo4j provides much better performance when it comes to querying deeply connected data that has many relationships expressed with complex joins. In relational databases, join-intensive query performance deteriorates when the dataset gets bigger. However when using graph databases, the performance stays relatively constant even with very large datasets.  This because in the graph data model, the query will check only the part of the graph that will be traversed by the query and not the whole graph.", 
            "title": "Performance"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#flexibility", 
            "text": "The graph data model is more natural. It has no impedance mismatch and is whiteboard friendly. This means you can use the language of node, relationships and properties to describe the application domain instead of using complex models such as UML. Then this graph model is directly mapped and implemented in the database. This friendly data model used in the graph databases allows developers to be more productive and reduce project risk. Since the graph model is flexible, you can start with small model and improve it in the future easily by adding more nodes and relationships with fewer migration and maintenance overhead.", 
            "title": "Flexibility"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#powerful-query-model", 
            "text": "The graph query model is so intuitive and makes it very suitable for applications with object oriented, semi-structured and network like data. The graph model is also very natural to express graph related problems such as the path finding problems.  Using this query model, you can write complex high performance traversals that can be beneficial in many use cases.    In addition to the above advantages of the graph databases, Neo4j also provides the below advantages.", 
            "title": "Powerful Query Model"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#easy-to-learn-query-language", 
            "text": "Neo4j provides a powerful traversal framework using an easy to learn query language called Cypher. Cypher is a declarative query language designed to be an efficient and human readable language.", 
            "title": "Easy to Learn Query Language"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#acid-compliant", 
            "text": "Neo4j is compliant with the ACID properties (Atomicity, Consistency, Isolation, Durability) and provides full transaction support", 
            "title": "ACID Compliant"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#weaknesses", 
            "text": "Neo4j has the below main weaknesses:", 
            "title": "Weaknesses"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#scalability", 
            "text": "Neo4j supports HA master-slave clusters that can linearly scale reads where slaves can share the read load.  As for the write load, only the master instance in the cluster can handle it. Other slave instances can still receive the write requests from clients but then these requests will be forwarded to the master node. Therefore, writing to the master instance is faster than writing to a slave instance. This means that Neo4j doesn't scale writes very well and in case of exceptionally high write loads, only vertical scaling of the master instance is possible. Although it is possible to implement some sharding logic in the client application to distribute the data across number of servers, however the sharding logic still not natively supported by Neo4j. This is because sharding the graph is a near impossible or NP hard mathematical problem. In general, sharding the data in the client side depends on the graph structure. If the graph has clear boundaries, then it can be sharded easily otherwise it can be really difficult. Additionally it is very difficult to shard a densely connected graph.", 
            "title": "Scalability"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#storage", 
            "text": "Neo4j has some upper bound limit for the graph size and can support single graphs having tens of billions of nodes, relationships and properties. The current Neo4j version supports up to around 34 billions of nodes and relationships and around 274 billions of properties. This is quite enough for large graphs of Facebook similar network graph size. These storage constraints doesn't pose any limitations in practice since only big businesses such as google can push these limits and these limits were set for storage optimisations and can be increased in future versions.", 
            "title": "Storage"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#no-date-data-type-support-support", 
            "text": "Neo4j doesn't have internal support for date data type but this can be overcome using different methods such as storing the Epoch linux long values instead.", 
            "title": "No date data type support support"
        }, 
        {
            "location": "/Neo4j/Results/strengths_weakness/#summary", 
            "text": "Neo4j is suitable for applications where you have very large connected data and you want to run complex join-intensive queries and still get high performance. It can be used for example to run live complex queries (reports) in production data since they will be faster. In addition, Neo4j is suitable if you have a data that will be more naturally represented using the graph model such as network like, semi-structured or highly connected data. However, if you have simple data model that don't require a lot of joining or aggregation, then you might not get any performance improvements if you use Neo4j or the performance advantage will be very small if not worse. Additionally, Neo4j has scalability weaknesses related to scaling writes, hence if your application is expected to have very large write throughputs, then Neo4j is not for you.", 
            "title": "Summary"
        }, 
        {
            "location": "/Neo4j/Results/summary/", 
            "text": "Neo4j is an open-source graph based database that was built completely based on the graph property model. Neo4j is compliance with the database ACID properties and supports non-functional requirements such as scalability, availability and fault-tolerance. In the following sections, I will give a brief summary for most of the features that we look for in a database and how Neo4j supports them.\n\n\nUse cases\n\n\nNeo4j is based on the graph property model and can handle complex queries very efficiently. Therefore, it is used in many use cases such as software analytics, scientific research, path finding problems, recommendation system, social networks, network management, business intelligence or warehouses and many more. \n\n\nBasic Concepts\n\n\nMain concepts used in Neo4j.\n\n\nNodes: The fundamental unit used in Neo4j to represent objects or entities that needs to be stored in the database.\n\n\nRelationships: Represents the relationships between two nodes.\n\n\nProperties: Nodes and relationships can use one or more properties to store some information related to the node or the relationship.\n\n\nLabels: A marked used to group a set of similar nodes.\n\n\nRelationship Types:  A marked used to group a set of similar relationships.\n\n\nTraversal: How the database search the graph to find results of the queries\n\n\nPaths: The traversal result. It contains one or more nodes connected with some relationships\n\n\nInstalling\n\n\nNeo4j can be installed easily in most of the platforms. Neo4j supports a windows installer for windows systems and a DMG installer for mac systems. Neo4j can be installed using the apt-get package manager in linux-based systems.\n\n\nQuery language\n\n\nNeo4j uses a declarative query language called Cypher which is a user friendly language inspired by SQL and has a pattern matching expressions similar to SPARQL.  The Cypher language is used to create nodes and relationships and to run simple or complex queries. \n\n\nTransaction support\n\n\nNeo4j is compliance with the ACID properties and provides full transaction support. All operations in Neo4j that changes the graph will be run on a transaction. You can also  execute multiple update statement in one transaction. The transaction can be committed or rolled back. Usually a try-catch block is used where we commit at the end of the try block or rollback inside the catch block. Transactions in Neo4j are having read-committed isolation level which means that we can read only the committed changes. Default write locks are used in Neo4j whenever we create,update or delete a nodes or a relationships. The locks are added to the transaction and released whenever the transaction completes. Additionally, explicit write locks for nodes and relationships can also be enabled to provide higher serialisation isolation level. Neo4j is also fully durable and whenever the transaction is committed, the data will be persisted on disk.\n\n\nData Import and Export\n\n\nNeo4j supports loading data as parameters that can be either scalar values, maps, lists or lists of maps. The UNWIND clause can be used to expand and insert the imported data one by one to create the graph. Additionally, The data can be imported as JSON data pulled from some API or from CSV files using the LOAD CSV Cypher command. Neo4j supports also exporting the data easily in JSON format. \n\n\nData Layout\n\n\nModelling the data in Neo4j requires converting the data to nodes and relationships to build the graph model. For example to convert the data from the relational model to the graph model, we first create node labels that corresponds to table names. Then each row in the relational data model will correspond to a node in the graph model and will be tagged with the appropriate label. The columns in the relational tables can be stored as properties inside each node ignoring any columns having null values. Finally, primary keys are enforced using unique constraints and foreign keys will be replaced with relationships. \n\n\nRelational data\n\n\nIn Neo4j all relationships regardless of whether they are one-to-one, one-to-many or many-to-many are represented in the same way through one relationship. Although relationships in Neo4j are directed but they can be traversed in both direction which means that you just need to create one relationship between different nodes regardless of the direction. It makes sense to create more than one relationship between two different nodes if they are of a different type or meaning since later you can use these different relationships to answer different queries.\n\n\nNormalisation/Denormalisation\n\n\nUsing the graph data model, you can model your data as normalised as you want without any problems. Actually using the graph data model, you can keep your data as it is in the relational model: small, fully normalised and yet represent richly connected entities.\n\n\nReferential Integrity\n\n\nIn Neo4j, if we delete a node, all its relationships will be also removed since we can't have a hanging relationship without a start or end node. Data integrity in Neo4j ensures that there shouldn't be any relationship without start or end node and there shouldn't be any properties that aren't attached to a node or a relationship. \n\n\nNested Data\n\n\nIn Neo4j, the concept of nested data isn't applicable since storing nodes inside other nodes isn't support and it makes more sense to store related data as separate nodes having separate properties and then connect them using relationships. \n\n\nQuery\n\n\nNeo4j supports parameterized or range queries by using the MATCH and WHERE clauses. Querying the graph model is based on the matching patterns provided in the MATCH part. This could mean searching the whole graph or just a subgraph based on the node labels and relationship types. Neo4j Cypher language allows for writing simple or complex queries easily and traversing the graph to search for the results is usually so efficient.\n\n\nAggregation\n\n\nAggregation in Neo4j is easily supported using Cypher language by using many functions such as min, max, avg, sum, count, stdev, stdevp and collect. The aggregation is done in the RETURN clause and the grouping key(s) will be automatically figured out by the RETURN clause and doesn't need to be specified manually. \n\n\nFull Text Search\n\n\nNeo4j doesn't provide any internal full-text indexes that can be used to implement a full-text search. However the use of a external index engine such as Apache Lucene full-text index engine is supported.\n\n\nRegular Expressions\n\n\nNeo4j supports filtering results using java regular expressions which includes some flags to specify how strings are matched such as \"?m\" for multiline, \"?i\" for case-insensitive, and \"?s\" for dotall.\n\n\nIndexing\n\n\nNeo4j supports the creation of indexes on any property of a node if the node is already tagged to a label. Indexes will be managed automatically by Neo4j and be kept always up to date whenever graph data is updated. Other types of indexes such as full-text, spatial, and range indexes are still not addressed but are planned to be targeted in the future roadmap for Neo4j.  \n\n\nFiltering and Grouping\n\n\nFiltering: Filtering the data in Noe4j is done by providing a specific match pattern that would be the input for a Cypher MATCH clause and then filter the returned results later using the WHERE clause. The pattern can handle very complex queries and should describe what we are looking for in a human readable style. We can also do the matching in multiple stages by either using more than one MATCH clause or by combining results from different MATCH clause using the WITH clause. The results of the MATCH clause can be then filtered out easily using the WHERE clause.\n\n\nGrouping: Grouping in Neo4j is handled by using node labels and relationship types. A node can have one or more labels which can be used later to query only a certain group of nodes in the graph which results in faster queries. Relationships also can be grouped with relationship types. \n\n\nSorting\n\n\nNeo4j supports SQL-like clause called ORDER BY which is used at the end of your query after the RETURN or WITH clauses to sort the results by one of the returned properties. You can only sort by a property that has been projected by the RETURN or WITH clauses. Additionally, you can specify the sort order by using either DESC or ASC.\n\n\nConfiguration\n\n\nA Neo4j database server instance can start without configuration since it uses the default configurations. The configurations are stored inside the configuration file conf/neo4j-server.properties. Inside this file, many configurations can be changed such as configuring security, performance, and many other operating features.\n\n\nScalability\n\n\nNeo4j supports HA master-slave clusters that can be used to linearly scale reads where slaves can share the read load.  As for the write load, only the master instance in the cluster can handle it. Other slave instances can still receive the write requests from clients but then these requests will be forwarded to the master node. This means that Neo4j doesn't scale the write load very well and in case of exceptionally high write loads, only vertical scaling of the master instance is possible. Although it is possible to build some sharding logic in the client application to distribute the data across number of servers, however the sharding logic still not natively supported by Neo4j. This is because sharding the graph is a near impossible or NP hard problem. In general, sharding the data in the client side depends on the graph structure. If the graph has clear boundaries, then it can be sharded easily otherwise it can be really difficult. The future goal of Neo4j is to support sharding across multiple machines without the application-level intervention, however this might take time. Finally since each server in the cluster holds the full dataset, there is an upper bound limit for the graph size that can be stored in each server. For the current version, Neo4j supports up to around 34 billions of nodes and relationships and around 274 billions of properties. This is quite enough for large graphs of Facebook similar network graph size.\n\n\nPersistency\n\n\nNeo4j is fully durable and all written data is persisted on disk. Once transactions are marked as committed, it will be written directly to disk. \n\n\nBackup\n\n\nNeo4j support an online backup tool called neo4j-backup that will be enabled by default and will take a local copy of the database automatically. First it will take a full backup then it will take an incremental backup. Restoring the backup is as easy as replacing the database folder with the backed up folder.\n\n\nSecurity\n\n\nNeo4j supports different security options on the data level as well as on the server level. In the data level, you can use the security methods available in the Java programming language such as encrypting the data. In the server level, the server replies only to request that are coming from the pre-configured adress and port. Additionally, clients can supply authentication credentials when trying to call the REST API and SSL secured communication over HTTPS is supported.\n\n\nUpgrading\n\n\nNeo4j supports automatic upgrade for upgrades between patch releases and within the same minor version. However, upgrading a major release is manual.\n\n\nAvailability\n\n\nHigh availability features are supported using the HA cluster which is based on a master-slave configuration architecture. More than one slave can be configured to be the exact replica of a master and can be used to make the system continue working in case of hardware failure. The slaves can be also used to handle some of the read load and can accept write requests from the clients. However the slaves will synchronise the write with the master to maintain consistency. Once the master receives a write request from the slave, it will propagate the change to all other slaves. The instances in a HA cluster will monitor each other for availability and monitor if any new instance has joined or any existing instance has left the cluster. Automatic election will take place if the master left the cluster for any reason. During master failover, a new master will be automatically elected and during this time the writes will be blocked until the new master is available. It is also possible to use arbiter instances which are instances that have only one role which is to participate in election in case there is no odd number of nodes and an instance is needed to break the tie.", 
            "title": "Summary"
        }, 
        {
            "location": "/Neo4j/Results/summary/#use-cases", 
            "text": "Neo4j is based on the graph property model and can handle complex queries very efficiently. Therefore, it is used in many use cases such as software analytics, scientific research, path finding problems, recommendation system, social networks, network management, business intelligence or warehouses and many more.", 
            "title": "Use cases"
        }, 
        {
            "location": "/Neo4j/Results/summary/#basic-concepts", 
            "text": "Main concepts used in Neo4j.  Nodes: The fundamental unit used in Neo4j to represent objects or entities that needs to be stored in the database.  Relationships: Represents the relationships between two nodes.  Properties: Nodes and relationships can use one or more properties to store some information related to the node or the relationship.  Labels: A marked used to group a set of similar nodes.  Relationship Types:  A marked used to group a set of similar relationships.  Traversal: How the database search the graph to find results of the queries  Paths: The traversal result. It contains one or more nodes connected with some relationships", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/Neo4j/Results/summary/#installing", 
            "text": "Neo4j can be installed easily in most of the platforms. Neo4j supports a windows installer for windows systems and a DMG installer for mac systems. Neo4j can be installed using the apt-get package manager in linux-based systems.", 
            "title": "Installing"
        }, 
        {
            "location": "/Neo4j/Results/summary/#query-language", 
            "text": "Neo4j uses a declarative query language called Cypher which is a user friendly language inspired by SQL and has a pattern matching expressions similar to SPARQL.  The Cypher language is used to create nodes and relationships and to run simple or complex queries.", 
            "title": "Query language"
        }, 
        {
            "location": "/Neo4j/Results/summary/#transaction-support", 
            "text": "Neo4j is compliance with the ACID properties and provides full transaction support. All operations in Neo4j that changes the graph will be run on a transaction. You can also  execute multiple update statement in one transaction. The transaction can be committed or rolled back. Usually a try-catch block is used where we commit at the end of the try block or rollback inside the catch block. Transactions in Neo4j are having read-committed isolation level which means that we can read only the committed changes. Default write locks are used in Neo4j whenever we create,update or delete a nodes or a relationships. The locks are added to the transaction and released whenever the transaction completes. Additionally, explicit write locks for nodes and relationships can also be enabled to provide higher serialisation isolation level. Neo4j is also fully durable and whenever the transaction is committed, the data will be persisted on disk.", 
            "title": "Transaction support"
        }, 
        {
            "location": "/Neo4j/Results/summary/#data-import-and-export", 
            "text": "Neo4j supports loading data as parameters that can be either scalar values, maps, lists or lists of maps. The UNWIND clause can be used to expand and insert the imported data one by one to create the graph. Additionally, The data can be imported as JSON data pulled from some API or from CSV files using the LOAD CSV Cypher command. Neo4j supports also exporting the data easily in JSON format.", 
            "title": "Data Import and Export"
        }, 
        {
            "location": "/Neo4j/Results/summary/#data-layout", 
            "text": "Modelling the data in Neo4j requires converting the data to nodes and relationships to build the graph model. For example to convert the data from the relational model to the graph model, we first create node labels that corresponds to table names. Then each row in the relational data model will correspond to a node in the graph model and will be tagged with the appropriate label. The columns in the relational tables can be stored as properties inside each node ignoring any columns having null values. Finally, primary keys are enforced using unique constraints and foreign keys will be replaced with relationships.", 
            "title": "Data Layout"
        }, 
        {
            "location": "/Neo4j/Results/summary/#relational-data", 
            "text": "In Neo4j all relationships regardless of whether they are one-to-one, one-to-many or many-to-many are represented in the same way through one relationship. Although relationships in Neo4j are directed but they can be traversed in both direction which means that you just need to create one relationship between different nodes regardless of the direction. It makes sense to create more than one relationship between two different nodes if they are of a different type or meaning since later you can use these different relationships to answer different queries.", 
            "title": "Relational data"
        }, 
        {
            "location": "/Neo4j/Results/summary/#normalisationdenormalisation", 
            "text": "Using the graph data model, you can model your data as normalised as you want without any problems. Actually using the graph data model, you can keep your data as it is in the relational model: small, fully normalised and yet represent richly connected entities.", 
            "title": "Normalisation/Denormalisation"
        }, 
        {
            "location": "/Neo4j/Results/summary/#referential-integrity", 
            "text": "In Neo4j, if we delete a node, all its relationships will be also removed since we can't have a hanging relationship without a start or end node. Data integrity in Neo4j ensures that there shouldn't be any relationship without start or end node and there shouldn't be any properties that aren't attached to a node or a relationship.", 
            "title": "Referential Integrity"
        }, 
        {
            "location": "/Neo4j/Results/summary/#nested-data", 
            "text": "In Neo4j, the concept of nested data isn't applicable since storing nodes inside other nodes isn't support and it makes more sense to store related data as separate nodes having separate properties and then connect them using relationships.", 
            "title": "Nested Data"
        }, 
        {
            "location": "/Neo4j/Results/summary/#query", 
            "text": "Neo4j supports parameterized or range queries by using the MATCH and WHERE clauses. Querying the graph model is based on the matching patterns provided in the MATCH part. This could mean searching the whole graph or just a subgraph based on the node labels and relationship types. Neo4j Cypher language allows for writing simple or complex queries easily and traversing the graph to search for the results is usually so efficient.", 
            "title": "Query"
        }, 
        {
            "location": "/Neo4j/Results/summary/#aggregation", 
            "text": "Aggregation in Neo4j is easily supported using Cypher language by using many functions such as min, max, avg, sum, count, stdev, stdevp and collect. The aggregation is done in the RETURN clause and the grouping key(s) will be automatically figured out by the RETURN clause and doesn't need to be specified manually.", 
            "title": "Aggregation"
        }, 
        {
            "location": "/Neo4j/Results/summary/#full-text-search", 
            "text": "Neo4j doesn't provide any internal full-text indexes that can be used to implement a full-text search. However the use of a external index engine such as Apache Lucene full-text index engine is supported.", 
            "title": "Full Text Search"
        }, 
        {
            "location": "/Neo4j/Results/summary/#regular-expressions", 
            "text": "Neo4j supports filtering results using java regular expressions which includes some flags to specify how strings are matched such as \"?m\" for multiline, \"?i\" for case-insensitive, and \"?s\" for dotall.", 
            "title": "Regular Expressions"
        }, 
        {
            "location": "/Neo4j/Results/summary/#indexing", 
            "text": "Neo4j supports the creation of indexes on any property of a node if the node is already tagged to a label. Indexes will be managed automatically by Neo4j and be kept always up to date whenever graph data is updated. Other types of indexes such as full-text, spatial, and range indexes are still not addressed but are planned to be targeted in the future roadmap for Neo4j.", 
            "title": "Indexing"
        }, 
        {
            "location": "/Neo4j/Results/summary/#filtering-and-grouping", 
            "text": "Filtering: Filtering the data in Noe4j is done by providing a specific match pattern that would be the input for a Cypher MATCH clause and then filter the returned results later using the WHERE clause. The pattern can handle very complex queries and should describe what we are looking for in a human readable style. We can also do the matching in multiple stages by either using more than one MATCH clause or by combining results from different MATCH clause using the WITH clause. The results of the MATCH clause can be then filtered out easily using the WHERE clause.  Grouping: Grouping in Neo4j is handled by using node labels and relationship types. A node can have one or more labels which can be used later to query only a certain group of nodes in the graph which results in faster queries. Relationships also can be grouped with relationship types.", 
            "title": "Filtering and Grouping"
        }, 
        {
            "location": "/Neo4j/Results/summary/#sorting", 
            "text": "Neo4j supports SQL-like clause called ORDER BY which is used at the end of your query after the RETURN or WITH clauses to sort the results by one of the returned properties. You can only sort by a property that has been projected by the RETURN or WITH clauses. Additionally, you can specify the sort order by using either DESC or ASC.", 
            "title": "Sorting"
        }, 
        {
            "location": "/Neo4j/Results/summary/#configuration", 
            "text": "A Neo4j database server instance can start without configuration since it uses the default configurations. The configurations are stored inside the configuration file conf/neo4j-server.properties. Inside this file, many configurations can be changed such as configuring security, performance, and many other operating features.", 
            "title": "Configuration"
        }, 
        {
            "location": "/Neo4j/Results/summary/#scalability", 
            "text": "Neo4j supports HA master-slave clusters that can be used to linearly scale reads where slaves can share the read load.  As for the write load, only the master instance in the cluster can handle it. Other slave instances can still receive the write requests from clients but then these requests will be forwarded to the master node. This means that Neo4j doesn't scale the write load very well and in case of exceptionally high write loads, only vertical scaling of the master instance is possible. Although it is possible to build some sharding logic in the client application to distribute the data across number of servers, however the sharding logic still not natively supported by Neo4j. This is because sharding the graph is a near impossible or NP hard problem. In general, sharding the data in the client side depends on the graph structure. If the graph has clear boundaries, then it can be sharded easily otherwise it can be really difficult. The future goal of Neo4j is to support sharding across multiple machines without the application-level intervention, however this might take time. Finally since each server in the cluster holds the full dataset, there is an upper bound limit for the graph size that can be stored in each server. For the current version, Neo4j supports up to around 34 billions of nodes and relationships and around 274 billions of properties. This is quite enough for large graphs of Facebook similar network graph size.", 
            "title": "Scalability"
        }, 
        {
            "location": "/Neo4j/Results/summary/#persistency", 
            "text": "Neo4j is fully durable and all written data is persisted on disk. Once transactions are marked as committed, it will be written directly to disk.", 
            "title": "Persistency"
        }, 
        {
            "location": "/Neo4j/Results/summary/#backup", 
            "text": "Neo4j support an online backup tool called neo4j-backup that will be enabled by default and will take a local copy of the database automatically. First it will take a full backup then it will take an incremental backup. Restoring the backup is as easy as replacing the database folder with the backed up folder.", 
            "title": "Backup"
        }, 
        {
            "location": "/Neo4j/Results/summary/#security", 
            "text": "Neo4j supports different security options on the data level as well as on the server level. In the data level, you can use the security methods available in the Java programming language such as encrypting the data. In the server level, the server replies only to request that are coming from the pre-configured adress and port. Additionally, clients can supply authentication credentials when trying to call the REST API and SSL secured communication over HTTPS is supported.", 
            "title": "Security"
        }, 
        {
            "location": "/Neo4j/Results/summary/#upgrading", 
            "text": "Neo4j supports automatic upgrade for upgrades between patch releases and within the same minor version. However, upgrading a major release is manual.", 
            "title": "Upgrading"
        }, 
        {
            "location": "/Neo4j/Results/summary/#availability", 
            "text": "High availability features are supported using the HA cluster which is based on a master-slave configuration architecture. More than one slave can be configured to be the exact replica of a master and can be used to make the system continue working in case of hardware failure. The slaves can be also used to handle some of the read load and can accept write requests from the clients. However the slaves will synchronise the write with the master to maintain consistency. Once the master receives a write request from the slave, it will propagate the change to all other slaves. The instances in a HA cluster will monitor each other for availability and monitor if any new instance has joined or any existing instance has left the cluster. Automatic election will take place if the master left the cluster for any reason. During master failover, a new master will be automatically elected and during this time the writes will be blocked until the new master is available. It is also possible to use arbiter instances which are instances that have only one role which is to participate in election in case there is no odd number of nodes and an instance is needed to break the tie.", 
            "title": "Availability"
        }, 
        {
            "location": "/Neo4j/Search Data/filtering and grouping/", 
            "text": "back\n\n\nFiltering, grouping and aggregating in Neo4j is very easy, much similar to relational database using SQL-similar syntax. In the below sections I will explain how filtering, and grouping is supported using Neo4j.\n\n\nFiltering\n\n\nFiltering the data in Noe4j is done by providing a specific match pattern that would be the input for a Cypher MATCH clause. The pattern can handle very complex queries and should describe what we are looking for in a human readable style. We can specify a pattern that involves any number of nodes and relationships and we can go as deep as we want in the graph. We can also do the matching in multiple stages by either using more than one MATCH clause or by combining results from different MATCH clause using the WITH clause. Neo4j supports very flexible filtering functionalities that can be used to answer extensively complex queries. For more details on how to use the MATCH clause, please have a look at the previous section explaining \nCypher query language\n. The results of the MATCH clause can be then filtered out easily using the WHERE clause.\n\n\nGrouping\n\n\nGrouping in Neo4j is handled by using node labels as already explained. A node can have one or more labels which can be used later to query only a certain group of nodes in the graph which results in faster queries. Relationships also can have a type to be also grouped later on queries.", 
            "title": "Filtering and grouping"
        }, 
        {
            "location": "/Neo4j/Search Data/filtering and grouping/#back", 
            "text": "Filtering, grouping and aggregating in Neo4j is very easy, much similar to relational database using SQL-similar syntax. In the below sections I will explain how filtering, and grouping is supported using Neo4j.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Search Data/filtering and grouping/#filtering", 
            "text": "Filtering the data in Noe4j is done by providing a specific match pattern that would be the input for a Cypher MATCH clause. The pattern can handle very complex queries and should describe what we are looking for in a human readable style. We can specify a pattern that involves any number of nodes and relationships and we can go as deep as we want in the graph. We can also do the matching in multiple stages by either using more than one MATCH clause or by combining results from different MATCH clause using the WITH clause. Neo4j supports very flexible filtering functionalities that can be used to answer extensively complex queries. For more details on how to use the MATCH clause, please have a look at the previous section explaining  Cypher query language . The results of the MATCH clause can be then filtered out easily using the WHERE clause.", 
            "title": "Filtering"
        }, 
        {
            "location": "/Neo4j/Search Data/filtering and grouping/#grouping", 
            "text": "Grouping in Neo4j is handled by using node labels as already explained. A node can have one or more labels which can be used later to query only a certain group of nodes in the graph which results in faster queries. Relationships also can have a type to be also grouped later on queries.", 
            "title": "Grouping"
        }, 
        {
            "location": "/Neo4j/Search Data/fulltext/", 
            "text": "back\n\n\nNeo4j doesn't provide any internal full-text indexes that can be used to implement a full-text search. However the use of a external index engine such as Apache Lucene full-text index engine is supported. Since it isn't an internal feature, it won't be covered here. For more information about using lucene full-text index, please refer to the \ndocumentations\n.", 
            "title": "Fulltext"
        }, 
        {
            "location": "/Neo4j/Search Data/fulltext/#back", 
            "text": "Neo4j doesn't provide any internal full-text indexes that can be used to implement a full-text search. However the use of a external index engine such as Apache Lucene full-text index engine is supported. Since it isn't an internal feature, it won't be covered here. For more information about using lucene full-text index, please refer to the  documentations .", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Search Data/", 
            "text": "back\n\n\nNeo4j supports the creation of indexes on any property of a node if the node is already tagged to a label. Indexes will be managed automatically by Neo4j and be kept always up to date whenever graph data is updated. Once the index is created, Neo4j will start using the index to retrieve data efficiently whenever you run a query on it. As you might already know, index comes always with a space cost, so create an index only when you need it.\n\n\nFor example, we will create an index on the product name property of all the nodes having the \":Product\" label :\n\n\nCREATE INDEX ON :Product(name)\n\n\n\n\nThen you can easily run fast queries against it as shown below:\n\n\nMATCH (p:Product { name: 'Dell Laptop' })\nRETURN p\n\n\n\n\nor by using WHERE clause:\n\n\nMATCH (p:Product)\nWHERE p.name = 'Dell Laptop'\nRETURN p\n\n\n\n\nMATCH (p:Product)\nWHERE p.name \n 'L'\nRETURN p\n\n\n\n\nMATCH (p:Product)\nWHERE p.name STARTS WITH 'La'\nRETURN p\n\n\n\n\nMATCH (p:Product)\nWHERE HAS (p.name)\nRETURN p\n\n\n\n\nMATCH (p:Product)\nWHERE p.name IN  ['Dell Laptop', 'T-Shirt']\nRETURN person\n\n\n\n\nFinally to drop the index, you can run the below:\n\n\nDROP INDEX ON :Product(name)\n\n\n\n\nWith Neo4j 2.3, there is support only for automatic indexes which will be created on the properties of a node that is attached to a label and can be used for exact lookups of nodes based on their property values. Other types of indexes such as full-text, spatial, and range indexes are still not addressed but are planned to be targeted in the future roadmap.  However Neo4j still support legacy indexes, but because the use of legacy indexes isn't favoured by Neo4j, we aren't going to cover them here. If you need any information about using legacy indexes, you can have a look at \nNeo4j documentations\n.\n\n\nIf you need to create a composite index to ensure the uniqueness on two or more properties at the same time, then you can create one normal index and use MERGE clause when creating the data as shown below:\n\n\nCREATE CONSTRAINT ON (c:Customer) assert a.composite_id IS UNIQUE;\n\nMERGE (c:Customer {composite_id: [{firstName},{secondName},{age}]}) ON CREATE SET c.firstName = {firstName}, c.secondName={secondName}, c.age = {age};", 
            "title": "Home"
        }, 
        {
            "location": "/Neo4j/Search Data/#back", 
            "text": "Neo4j supports the creation of indexes on any property of a node if the node is already tagged to a label. Indexes will be managed automatically by Neo4j and be kept always up to date whenever graph data is updated. Once the index is created, Neo4j will start using the index to retrieve data efficiently whenever you run a query on it. As you might already know, index comes always with a space cost, so create an index only when you need it.  For example, we will create an index on the product name property of all the nodes having the \":Product\" label :  CREATE INDEX ON :Product(name)  Then you can easily run fast queries against it as shown below:  MATCH (p:Product { name: 'Dell Laptop' })\nRETURN p  or by using WHERE clause:  MATCH (p:Product)\nWHERE p.name = 'Dell Laptop'\nRETURN p  MATCH (p:Product)\nWHERE p.name   'L'\nRETURN p  MATCH (p:Product)\nWHERE p.name STARTS WITH 'La'\nRETURN p  MATCH (p:Product)\nWHERE HAS (p.name)\nRETURN p  MATCH (p:Product)\nWHERE p.name IN  ['Dell Laptop', 'T-Shirt']\nRETURN person  Finally to drop the index, you can run the below:  DROP INDEX ON :Product(name)  With Neo4j 2.3, there is support only for automatic indexes which will be created on the properties of a node that is attached to a label and can be used for exact lookups of nodes based on their property values. Other types of indexes such as full-text, spatial, and range indexes are still not addressed but are planned to be targeted in the future roadmap.  However Neo4j still support legacy indexes, but because the use of legacy indexes isn't favoured by Neo4j, we aren't going to cover them here. If you need any information about using legacy indexes, you can have a look at  Neo4j documentations .  If you need to create a composite index to ensure the uniqueness on two or more properties at the same time, then you can create one normal index and use MERGE clause when creating the data as shown below:  CREATE CONSTRAINT ON (c:Customer) assert a.composite_id IS UNIQUE;\n\nMERGE (c:Customer {composite_id: [{firstName},{secondName},{age}]}) ON CREATE SET c.firstName = {firstName}, c.secondName={secondName}, c.age = {age};", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Search Data/query/", 
            "text": "back\n\n\nNeo4j supports most query types by using the MATCH and WHERE clauses together. To supports parameterised or range queries by using the WHERE clause as shown below:\n\n\nparameterised queries :\n\n\nMATCH (p:Product)\nWHERE p.name = 'T-Shirt'\nRETURN p\n\n\n\n\nRange queries:\n\n\nMATCH (p:Product)\nWHERE p.price \n 2000 AND p.price \n 200\nRETURN p\n\n\n\n\nFor more details about the WHERE and MATCH clause, please read the previous section about \nCypher query language\n.", 
            "title": "Query"
        }, 
        {
            "location": "/Neo4j/Search Data/query/#back", 
            "text": "Neo4j supports most query types by using the MATCH and WHERE clauses together. To supports parameterised or range queries by using the WHERE clause as shown below:  parameterised queries :  MATCH (p:Product)\nWHERE p.name = 'T-Shirt'\nRETURN p  Range queries:  MATCH (p:Product)\nWHERE p.price   2000 AND p.price   200\nRETURN p  For more details about the WHERE and MATCH clause, please read the previous section about  Cypher query language .", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Search Data/queryOptions/", 
            "text": "back\n\n\nNeo4j supports many query functions that can be used to offer aggregations functions such as min, max, avg, count, DISTINCT, sum, stdev, stdevp and collect. Most of these functions works exactly in the same way in SQL such as min, max, sum, avg and count. The aggregation is done in the RETURN clause and the grouping key(s) will be automatically figured out by the RETURN clause and doesn't need to be specified manually. These basic function are used with the RETURN clause to return aggregated results as shown below:\n\n\nMATCH (p:Product)\nRETURN min(p.price), max(p.size) , count(*), sum(p.price), avg(p.price)\n\n\n\n\nDISTINCT is used to remove any duplicates from the returned results. This is usually used to return the count of unique nodes based on a certain node property as shown below:\n\n\nMATCH (p:Product)\nRETURN count(DISTINCT b.type)\n\n\n\n\nstdev and stdevp are used to calculate the standard deviation for a given value over the aggregate. Stdev uses a two-pass method with N-1 as denominator and used mainly for unbiased estimate. In the other hand, stdevp uses also two-pass method but with N as denominator and used mainly to calculate standard deviation for the entire population. Example is shown below:\n\n\nMATCH (p:Product)\nWHERE p.name IN ['A', 'B', 'C']\nRETURN stdevp(p.property)\n\n\n\n\nFinally the collect function is used to collect all the values and store them in a list as shown below:\n\n\nMATCH (p:Product {type: \nexpensive\n})\nRETURN collect(n.name)\n\n\n\n\nIn the above example, we are collecting all product names that are expensive in a list, so we get the results like ['A','B','C'].", 
            "title": "queryOptions"
        }, 
        {
            "location": "/Neo4j/Search Data/queryOptions/#back", 
            "text": "Neo4j supports many query functions that can be used to offer aggregations functions such as min, max, avg, count, DISTINCT, sum, stdev, stdevp and collect. Most of these functions works exactly in the same way in SQL such as min, max, sum, avg and count. The aggregation is done in the RETURN clause and the grouping key(s) will be automatically figured out by the RETURN clause and doesn't need to be specified manually. These basic function are used with the RETURN clause to return aggregated results as shown below:  MATCH (p:Product)\nRETURN min(p.price), max(p.size) , count(*), sum(p.price), avg(p.price)  DISTINCT is used to remove any duplicates from the returned results. This is usually used to return the count of unique nodes based on a certain node property as shown below:  MATCH (p:Product)\nRETURN count(DISTINCT b.type)  stdev and stdevp are used to calculate the standard deviation for a given value over the aggregate. Stdev uses a two-pass method with N-1 as denominator and used mainly for unbiased estimate. In the other hand, stdevp uses also two-pass method but with N as denominator and used mainly to calculate standard deviation for the entire population. Example is shown below:  MATCH (p:Product)\nWHERE p.name IN ['A', 'B', 'C']\nRETURN stdevp(p.property)  Finally the collect function is used to collect all the values and store them in a list as shown below:  MATCH (p:Product {type:  expensive })\nRETURN collect(n.name)  In the above example, we are collecting all product names that are expensive in a list, so we get the results like ['A','B','C'].", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Search Data/refernces/", 
            "text": "back\n\n\n1- http://neo4j.com/docs/\n\n\n2- Learning Neo4j by Rik Van Bruggen", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Search Data/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Search Data/refernces/#1-httpneo4jcomdocs", 
            "text": "", 
            "title": "1- http://neo4j.com/docs/"
        }, 
        {
            "location": "/Neo4j/Search Data/refernces/#2-learning-neo4j-by-rik-van-bruggen", 
            "text": "", 
            "title": "2- Learning Neo4j by Rik Van Bruggen"
        }, 
        {
            "location": "/Neo4j/Search Data/regx/", 
            "text": "back\n\n\nNeo4j supports filtering results using regular expressions. The java regular expressions syntax is used which includes some flags to specify how strings are matched such as \"?m\" for multiline, \"?i\" for case-insensitive, and \"?s\" for dotall. The regular expressions can be used in the MATCH clause with WHERE statement.  Some examples are shown below:\n\n\nMATCH (p:Product)\nWHERE p.name =~ 'Lap.*'\nRETURN n\n\n\n\n\nIn the above example, we are searching for all products that starts with \"Lap\" string. Results will contain products with names like \"laptop\" and etc.\n\n\nSimilar to java regular expressions, you can use a forward slash to escape some characters as seen in the example below:\n\n\nMATCH (p:Product)\nWHERE p.name =~ 'T\\\\-Shirt'\nRETURN n\n\n\n\n\nThe above example will return a product with name having the \"-\" character like \"T-Shirt\".", 
            "title": "Regx"
        }, 
        {
            "location": "/Neo4j/Search Data/regx/#back", 
            "text": "Neo4j supports filtering results using regular expressions. The java regular expressions syntax is used which includes some flags to specify how strings are matched such as \"?m\" for multiline, \"?i\" for case-insensitive, and \"?s\" for dotall. The regular expressions can be used in the MATCH clause with WHERE statement.  Some examples are shown below:  MATCH (p:Product)\nWHERE p.name =~ 'Lap.*'\nRETURN n  In the above example, we are searching for all products that starts with \"Lap\" string. Results will contain products with names like \"laptop\" and etc.  Similar to java regular expressions, you can use a forward slash to escape some characters as seen in the example below:  MATCH (p:Product)\nWHERE p.name =~ 'T\\\\-Shirt'\nRETURN n  The above example will return a product with name having the \"-\" character like \"T-Shirt\".", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/", 
            "text": "back\n\n\nFull Text Search\n\n\nRegular Expressions\n\n\nAggregation\n\n\nIndexing\n\n\nQuery\n\n\nFiltering and Grouping\n\n\nSorting\n\n\nRefernces", 
            "title": "Search data main"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/#full-text-search", 
            "text": "", 
            "title": "Full Text Search"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/#regular-expressions", 
            "text": "", 
            "title": "Regular Expressions"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/#aggregation", 
            "text": "", 
            "title": "Aggregation"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/#indexing", 
            "text": "", 
            "title": "Indexing"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/#query", 
            "text": "", 
            "title": "Query"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/#filtering-and-grouping", 
            "text": "", 
            "title": "Filtering and Grouping"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/#sorting", 
            "text": "", 
            "title": "Sorting"
        }, 
        {
            "location": "/Neo4j/Search Data/search_data_main/#refernces", 
            "text": "", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Search Data/sort/", 
            "text": "back\n\n\nTo sort the results returned by a query, Neo4j supports a SQL-like clause called ORDER BY which is used at the end of your query after the RETURN or WITH clauses. The ORDER BY clause will simply order the returned results based on a property field projected by the RETURN or WITH clauses. This means that you can only order what has been returned previously.\n\n\nFor example if your query returns only product name, size and price. The you can sort your results by either product name, size or price as shown below:\n\n\nMATCH (p:Product)\nRETURN p.name, p.size , p.price\nORDER BY p.name DESC\n\u2009\n\n\nYou can specify the sort order by using either DESC or ASC as shown above. You can't use any aggregation expressions with ORDER BY since it is meant only for sorting and not changing the results. Finally if you sort any results using ORDER BY, then any NULL values will come always at the end of the results.", 
            "title": "Sort"
        }, 
        {
            "location": "/Neo4j/Search Data/sort/#back", 
            "text": "To sort the results returned by a query, Neo4j supports a SQL-like clause called ORDER BY which is used at the end of your query after the RETURN or WITH clauses. The ORDER BY clause will simply order the returned results based on a property field projected by the RETURN or WITH clauses. This means that you can only order what has been returned previously.  For example if your query returns only product name, size and price. The you can sort your results by either product name, size or price as shown below:  MATCH (p:Product)\nRETURN p.name, p.size , p.price\nORDER BY p.name DESC \u2009  You can specify the sort order by using either DESC or ASC as shown above. You can't use any aggregation expressions with ORDER BY since it is meant only for sorting and not changing the results. Finally if you sort any results using ORDER BY, then any NULL values will come always at the end of the results.", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Special Features/graphgist/", 
            "text": "back", 
            "title": "Graphgist"
        }, 
        {
            "location": "/Neo4j/Special Features/graphgist/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Special Features/refernces/", 
            "text": "back\n\n\n1- https://docs.mongodb.org/manual\n\n\n2- https://www.mongodb.com/blog/post/building-mongodb-applications-binary-files-using-gridfs-part-1\n\n\n3- https://www.mongodb.com/blog/post/building-mongodb-applications-binary-files-using-gridfs-part-2", 
            "title": "Refernces"
        }, 
        {
            "location": "/Neo4j/Special Features/refernces/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Special Features/refernces/#1-httpsdocsmongodborgmanual", 
            "text": "", 
            "title": "1- https://docs.mongodb.org/manual"
        }, 
        {
            "location": "/Neo4j/Special Features/refernces/#2-httpswwwmongodbcomblogpostbuilding-mongodb-applications-binary-files-using-gridfs-part-1", 
            "text": "", 
            "title": "2- https://www.mongodb.com/blog/post/building-mongodb-applications-binary-files-using-gridfs-part-1"
        }, 
        {
            "location": "/Neo4j/Special Features/refernces/#3-httpswwwmongodbcomblogpostbuilding-mongodb-applications-binary-files-using-gridfs-part-2", 
            "text": "", 
            "title": "3- https://www.mongodb.com/blog/post/building-mongodb-applications-binary-files-using-gridfs-part-2"
        }, 
        {
            "location": "/Neo4j/Special Features/special_features_main/", 
            "text": "back\n\n\nGraphGist \n\n\nORM support", 
            "title": "Special features main"
        }, 
        {
            "location": "/Neo4j/Special Features/special_features_main/#back", 
            "text": "", 
            "title": "back"
        }, 
        {
            "location": "/Neo4j/Special Features/special_features_main/#graphgist", 
            "text": "", 
            "title": "GraphGist"
        }, 
        {
            "location": "/Neo4j/Special Features/special_features_main/#orm-support", 
            "text": "", 
            "title": "ORM support"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Availability/", 
            "text": "Redis provides a distributed system called Redis Sentinel to guarantee high availability. Redis Sentinel can resist certain types of failures automatically without any human intervention. Redis Sentinel can be used also for other tasks such as monitoring, as a configuration provider for clients and as a notification system. By Monitoring we mean that Redis Sentinel will monitor your master and slave instances and check if they are working as expected. It will also send notifications to the admin in case anything went wrong. Clients can also contact the Redis Sentinel to get any configurations parameters for a certain Redis instance such as address and port. Finally Redis Sentinel will start a failover process in case the master is not working to promote one of the slaves to replace it and reconfigure the other slaves to contact this new master.\n\n\nAn important note is that in order to get the best robustness out of Redis Sentinel, you should configure it in such a way that it will not be a single point of failure. This means that you use multiple instances in separate machines or virtual machines that fail independently which can cooperate together to handle Redis failure detection and recovery.  You need at least three Redis Sentinel instance to guarantee its Robustness. \n\n\nRunning Redis Sentinel is easy and can be done using the below command:\n\n\nredis-server /path/to/sentinel.conf --sentinel\n\n\n\n\nHowever before running Redis Sentinel, you should make sure that you have already created a configuration file. Otherwise Redis Sentinel will fail to start. In the configuration file, you need to set some minimal configuration parameters such as:\n\n\nsentinel monitor \nmaster-group-name\n \nip\n \nport\n \nquorum\n\n\n\n\n\nin the above example, we are asking Redis Sentinel to monitor a particular master instance. You need to repeat this configuration for each master instance in your system. The slaves are discovered automatically and no need to configure them. The \n above is the number of Redis Sentinel instances that need to agree when detecting a failure in master server. \n\n\nThere are other configuration parameters for each master node which follows the below pattern:\n\n\nsentinel \noption_name\n \nmaster_name\n \noption_value\n\n\n\n\n\nExamples of the options are down-after-milliseconds and parallel-syncs. The option down-after-milliseconds is used to define the time in milliseconds for the Redis Sentinel to ping a certain Redis instance to consider this instance to be down. The parallel-syncs option is the number of slave nodes that will be reconfigured with the new selected master node at the same time. The more this number, the less time it will take for the failure recovery process to complete. However, sometimes you need the slaves to still serve the old data for the system to continue operation. So you might want the recovery process to configure slaves one by one.\n\n\nThere are other configurations options that you can have a look at in \nRedis documentation\n. Please note that you still can use the command SENTINEL SET to change Redis Sentinel configuration at run time without restart. \n\n\nImportant commands that you can use to interact with Redis Sentinel are listed below:\n\n\n\n\nGet the state of a master\n\n\n\n\n127.0.0.1:5000\n sentinel master mymaster\n 1) \nname\n\n 2) \nmymaster\n\n 3) \nip\n\n 4) \n127.0.0.1\n\n 5) \nport\n\n 6) \n6379\n\n 7) \nrunid\n\n 8) \n953ae6a589449c13ddefaee3538d356d287f509b\n\n 9) \nflags\n\n....\n\n\n\n\nGet information about all slaves under a certain master using below command:\n\n\nSENTINEL slaves mymaster\n\n\n\n\n\n\nGet the address of a master\n\n\n\n\n127.0.0.1:5000\n SENTINEL get-master-addr-by-name mymaster\n1) \n127.0.0.1\n\n2) \n6379\n\n\n\n\n\nThere are many other interesting sentinel commands that are listed in \nRedis documentation\n.", 
            "title": "Availability"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Backup/", 
            "text": "Doing backup in Redis is so simple since it is basically the snapshots that were taken of your dataset if the snapshot option is enabled. Your data will be stored in an RDB file called dump.rdb file in your redis directory. As has been explained before, you can configure Redis to take a snapshot of your data periodically as configured in your configuration file. Also you can get a snapshot manually by running the command SAVE or BGSAVE to be run as background process. To restore your data, you just need to move this dump file to your Redis directory and then start up your server. If you don't know your Redis directory, just run the below command:\n\n\n127.0.0.1:6379\n CONFIG get dir\n\n1) \ndir\n\n2) \n/user/UserName/redis-version/src\n\n\n\n\n\nTo have a better backup, you can use snapshotting with replication so that you will have the same copy of your data in all your instances including masters and slaves. This will ensure that your data is save in case the master server crashed completely.", 
            "title": "Backup"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Configuration/", 
            "text": "As we already mentioned in the installation section, Redis can start without a configuration file and then it will take its built-in default configurations. However, in a production environment you will need to choose your own configuration parameters depending on your settings. Redis stores its configurations inside a file called redis.config and will contain configuration with the below format:\n\n\nconfugirationKeyWord argument1 argument2 ... argumentN\n\n\n\n\nYou can also pass configuration parameters via the command line which is very useful for development and testing environment. Example is shown below:\n\n\n./redis-server --port 6380 --slaveof 127.0.0.1 6379\n\n\n\n\nFinally configuring Redis while the server is running is also possible without the need to stop or restart the server. This is supported using the commands CONFIG GET , CONFIG SET which support most of the configuration keywords. Example is shown below:\n\n\nCONFIG SET SAVE \n900 1 300 10\n\n\n\n\n\nAbove command will change the configuration related to persistence which will be explained later.\n\n\nI am going to explain in the following sections how to configure Redis for various administration and maintenance tasks such as configuring security , scalability, upgrade, and backup. For a complete list of all the possible configurations that you can use in Redis, please check Redis \ndocumentation\n.", 
            "title": "Configuration"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Persistency/", 
            "text": "Redis supports two ways to persist data on disk. The first option is called snapshotting or RDB which takes a snapshot of the current data periodically (based on pre-configured value) and store it on disk. The second option is called append-only file or AOF which simply works by logging the operations that modifies the data (write operations) into a log file. Processing this log file later if needed will produce the same dataset that was in memory. More details about these two options and how to configure them will be explained in the following sections.\n\n\nSnapshotting or RDB Persistence\n\n\nSnapshotting is the simplest form of persistence provided by Redis. Using this option, Redis produces snapshots that contains the in memory dataset at a specific point-in-time and when certain conditions are met. For instance, you can configure Redis to produce snapshots each 20 minutes but only if there are already 50 new writes. Based on the previous example configuration, in the event of a sever crash, up to 20 minutes of writes can be lost. These snapshots will be stored in disk inside a single .rdb file. \n\n\nSnapshotting is an excellent persistence option in terms of performance and can be used for disaster recovery or backups. Redis also use it internally when performing synchronisation between slave and master instances for scalability. Unfortunately, Snapshotting doesn't provide good durability guarantees in case few minutes of data lose isn't acceptable. \n\n\nConfiguring Snapshotting is easy, you just need to specify when to create snapshots and in which conditions. For example, if you want your snapshots to be taken each 100 seconds and in the condition that there are 50 new writes, you would add the below in the configuration file:\n\n\nsave 100 50\n\n\n\n\nTo save the snapshot manually, you can issue the below command which will be forked and done in the background: \n\n\nBGSAVE\n\n\n\n\nAppend-Only File or AOF Persistence\n\n\nTo have a full durable persistency, you can use the AOF. This option can be easily configured to log any operation that modifies the in-memory dataset. You can turn this option by adding the below into the configuration file:\n\n\nappendonly yes \n\n\n\n\nAfter using this option, any write commands received by Redis will be automatically logged into the AOF. After restarting Redis instance it will re-play all the commands in the AOF and recover the original dataset. \n\n\nAs you might have already noticed, the log file will get bigger and bigger with time as all write commands are written to it. To solve this problem, Redis supports a process that runs in background without interrupting your work to rewrite the AOF with the shortest possible sequence of commands. You can also start this process manually by calling the below command:\n\n\nBGREWRITEAOF  \n\n\n\n\nDepending on your durability requirements, you need to set the fsync options. Redis has three configuration options which determines when to \nfsync\n data to disk as shown below:\n - \"fsync every\" when new command is logged to the AOF. This option is used if you want to have a full durability support but it is also a very slow option. \n - \"fsync every second\". This is the default configuration which will provide a good durability option but with a 1 second data lose risk in case of a disaster.This option is fast enough.\n - \"Never fsync\", using this option will let the operating system handle the fsync which can be very unsafe in term of durability. On the other hand, this option is very fast.\n\n\nWhich one to use\n\n\nUsually the best thing to use is to combine both options to get reliable and durable persistency for your application. This means that you use snapshotting to get a snapshot of your data from time to time which can be used in case of disaster recovery, backups or in case of issues in your AOF. At the same time, you use AOF to get full durability. This is of course depends on your application requirements because in some cases you care only about performance and can live with some minutes of data lose.\n\n\nFixning AOF or snapshots\n\n\nRedis provides two commands that can be used to verify the status of a snapshot or the AOF. The commands are redis-check-aof and redis-check-dump. An example is shown below:\n\n\n$ redis-check-aof\nUsage: redis-check-aof [--fix] \nfile.aof\n\n$ redis-check-dump\nUsage: redis-check-dump \ndump.rdb\n\n\n\n\n\nUsing --fix argument will fix the AOF by scanning the file for an incomplete or incorrect command. However unfortunately Redis doesn't support a way to fix the snapshot if it is corrupted. \n\n\nACID Support\n\n\nAtomicity can be guaranteed by executing a group of commands either using MULTI/EXEC blocks or by using a Lua script.\nConsistency is guaranteed by using the WATCH/UNWATCH blocks.\nIsolation is always guaranteed at command level, and for group of commands it can be also guaranteed by MULTI/EXEC block or a Lua script.\nFull Durability can be also guaranteed when using AOF with executing fsync with each new command as explained before.", 
            "title": "Persistency"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Persistency/#snapshotting-or-rdb-persistence", 
            "text": "Snapshotting is the simplest form of persistence provided by Redis. Using this option, Redis produces snapshots that contains the in memory dataset at a specific point-in-time and when certain conditions are met. For instance, you can configure Redis to produce snapshots each 20 minutes but only if there are already 50 new writes. Based on the previous example configuration, in the event of a sever crash, up to 20 minutes of writes can be lost. These snapshots will be stored in disk inside a single .rdb file.   Snapshotting is an excellent persistence option in terms of performance and can be used for disaster recovery or backups. Redis also use it internally when performing synchronisation between slave and master instances for scalability. Unfortunately, Snapshotting doesn't provide good durability guarantees in case few minutes of data lose isn't acceptable.   Configuring Snapshotting is easy, you just need to specify when to create snapshots and in which conditions. For example, if you want your snapshots to be taken each 100 seconds and in the condition that there are 50 new writes, you would add the below in the configuration file:  save 100 50  To save the snapshot manually, you can issue the below command which will be forked and done in the background:   BGSAVE", 
            "title": "Snapshotting or RDB Persistence"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Persistency/#append-only-file-or-aof-persistence", 
            "text": "To have a full durable persistency, you can use the AOF. This option can be easily configured to log any operation that modifies the in-memory dataset. You can turn this option by adding the below into the configuration file:  appendonly yes   After using this option, any write commands received by Redis will be automatically logged into the AOF. After restarting Redis instance it will re-play all the commands in the AOF and recover the original dataset.   As you might have already noticed, the log file will get bigger and bigger with time as all write commands are written to it. To solve this problem, Redis supports a process that runs in background without interrupting your work to rewrite the AOF with the shortest possible sequence of commands. You can also start this process manually by calling the below command:  BGREWRITEAOF    Depending on your durability requirements, you need to set the fsync options. Redis has three configuration options which determines when to  fsync  data to disk as shown below:\n - \"fsync every\" when new command is logged to the AOF. This option is used if you want to have a full durability support but it is also a very slow option. \n - \"fsync every second\". This is the default configuration which will provide a good durability option but with a 1 second data lose risk in case of a disaster.This option is fast enough.\n - \"Never fsync\", using this option will let the operating system handle the fsync which can be very unsafe in term of durability. On the other hand, this option is very fast.", 
            "title": "Append-Only File or AOF Persistence"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Persistency/#which-one-to-use", 
            "text": "Usually the best thing to use is to combine both options to get reliable and durable persistency for your application. This means that you use snapshotting to get a snapshot of your data from time to time which can be used in case of disaster recovery, backups or in case of issues in your AOF. At the same time, you use AOF to get full durability. This is of course depends on your application requirements because in some cases you care only about performance and can live with some minutes of data lose.", 
            "title": "Which one to use"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Persistency/#fixning-aof-or-snapshots", 
            "text": "Redis provides two commands that can be used to verify the status of a snapshot or the AOF. The commands are redis-check-aof and redis-check-dump. An example is shown below:  $ redis-check-aof\nUsage: redis-check-aof [--fix]  file.aof \n$ redis-check-dump\nUsage: redis-check-dump  dump.rdb   Using --fix argument will fix the AOF by scanning the file for an incomplete or incorrect command. However unfortunately Redis doesn't support a way to fix the snapshot if it is corrupted.", 
            "title": "Fixning AOF or snapshots"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Persistency/#acid-support", 
            "text": "Atomicity can be guaranteed by executing a group of commands either using MULTI/EXEC blocks or by using a Lua script.\nConsistency is guaranteed by using the WATCH/UNWATCH blocks.\nIsolation is always guaranteed at command level, and for group of commands it can be also guaranteed by MULTI/EXEC block or a Lua script.\nFull Durability can be also guaranteed when using AOF with executing fsync with each new command as explained before.", 
            "title": "ACID Support"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Scalability/", 
            "text": "In this section we will talk about scalability in Redis. The section will be divided into four parts to explain how to scale reads, writes, configurations, and complex queries.\n\n\nScaling Reads\n\n\nWhen your system grows bigger and the system read throughput increases more than what Redis server can handle, then you must scale your server to ensure that reads are as fast as possible.  The simplest way is to use the replication option that is supported by Redis by adding read-only slave servers. The replication is used mainly for scaling reads but it can  also be used for data redundancy and disaster recovery.\n\n\nReplication\n\n\nRedis supports a very simple to use master-slave replication option where you can configure as many as you need read-only slave servers to act like an exact copy of the master server. The master server will replicate its dataset frequently as configured with its slave. The replication process is done asynchronously where slaves periodically acknowledge how much data was successfully processed from the replication stream. The slaves can also have other slaves connected to them in a graph-like structure. Only the master will accept write operations and other slaves will be used just as read-only servers. The slave servers will throw errors if you try to write to them. Master instance will be available during the replication process since it is a non-blocking operation. However slaves can still be available during the replication but will serve requests with the old dataset only. \n\n\nConfiguring Replication\n\n\nRedis replication process between master and slave is simple. The master receives a SYNC command then it saves the current dataset to disk as RDB file. During saving the file, any write commands received by clients will be logged down to a file. Later when the RDB file is saved to disk, the master will send the file to all connected slaves along with the log file. Slaves will receive the RDB file and save it on disk. Then slaves will load the RDB file into memory and apply the log file commands to the new loaded dataset.  If the link between the master and its slave goes down for any reason, a full synchronisation will be done once reconnected. To configure a Redis server to be a slave, just run the below command on the slave instance:\n\n\nslaveof 192.168.1.1 6379\n\n\n\n\nThe above IP is the IP of the master server. If you want to stop the replication and make the slave server acts as a master, you can issue the below command:\n\n\nSLAVEOF NO ONE\n\n\n\n\nYou can also configure the slave to use certain credentials when syncing up with the master server. You can use the below command:\n\n\nconfig set masterauth \npassword\n\n\n\n\n\nWe will discuss later in the \navailability section\n how we can use Redis Sentinel to handle master or slave failures. \n\n\nScaling Writes\n\n\nIf your data grows beyond your memory or when you want to increase the overall system performance, you might need to scale up your Redis database by sharding your data among multiple Redis instances. This will allow for much larger database which will be the sum of the memory of all Redis instances.  In addition, the computational power and network bandwidth of your database will increase because you are using now multiple servers with multiple cores with multiple network adapters.  \n\n\nThere are different methods available to partition your data. The simplest way is range partitioning which is done by mapping your data by range to different Redis instances. For example, if you have two instances R1 and R2, then you could say that users with ID 1 to 100 assigned to R1 and from 101 to 200 assigned with R2 and so forth. This method is easy but not so practical because of the cost of maintaining a mapping table and the data distribution in uneven. The other method is by using a hash function that will hash the keys and decide where to distribute the data.\n\n\nThe important question is who does the partitioning. In Redis, the partitioning can be done in the client side where the clients directly select the right instance to be used to write or read a certain key. Most of the clients or drivers that are provided by Redis have implementations for partitioning. Another implementation for partitioning is by using a proxy which means that clients send request to a proxy that is able to interact with Redis and will forward the clients' requests to the right Redis instance then send back the reply to the clients. A famous proxy used by Redis and Memcached is \nTwemproxy\n. Finally it is also possible to use an implementation for partitioning called query routing where you just send your requests to a random instance that will forward your request to the right Redis instance. \n\n\nBefore Redis version 3.0, users usually tend to use client side implementations for partitioning their data to multiple Redis instances. But when Redis cluster was introduced, it is now the preferred way to get automatic partitioning and high availability. Redis cluster uses a hybrid implementation of query routing and some clients side implementation. I will explain in the following section how to use Redis cluster to do partitioning.\n\n\nRedis Cluster\n\n\nRedis cluster was introduced to allow users to do data partitioning automatically which is much easier and convenient. Redis cluster doesn't only provide a data partitioning capabilities but also allows for some degree of availability during partitioning. This means that the Redis database will continue to be operational even if some of the instances are experiencing failures.\n\n\nThe concept of Redis cluster is simple. The cluster will have a total of 16384 hash slots and will be distributed among all the instances in the cluster so that each instance will have a subset of these hash slots. Then each key will be mapped to its hash slot by simply taking the CRC16 of the key modulo \"16384\". Using this hash slots allow for easier management of the nodes inside the cluster. So you can easily add or remove nodes from the cluster by just either distributing its hash slots to the other nodes in case we want to remove it or by just moving a subset of the hash slots to it in case of adding. The node that has an empty set of hash slots will be automatically deleted by Redis cluster. Redis cluster also supports operations on multiple keys as long as all the keys are in the same hash slot by using a concept called hash tags.\n\n\nConfiguring Redis Cluster\n\n\nConfiguring Redis cluster isn't that easy and it would be better if you go through the Redis \ncluster specifications\n to do a serious configuration or deployment in a production environment. Below I will explain the configuration parameters related to Redis cluster that can be included in your redis.conf file.\n\n\ncluster-enabled is used to include this instance as a cluster instance otherwise it will act as a standalone instance.\ncluster-config-file is used to mention where Redis cluster can save its automatically generated configuration file for this instance.\ncluster-node-timeout  is the maximum amount of time that a Redis cluster instance can be down.\ncluster-slave-validity-factor is simply a factor which will be multiplied by the timeout to  determine how much time it will take for the slave to fail when the link to the master is down. \n\n\nTo simplest way to configure Redis cluster is to use the create-cluster script which can be founded under utils/create-cluster directory in the Redis distribution. This script is just a simple shell script that will configure a Redis cluster to start 6 nodes with 3 master nodes and 3 slaves. To run the script just run the below commands:\n\n\ncreate-cluster start\ncreate-cluster create\n\n\n\n\nYou will then get a message to accept the cluster layout, just reply with Yes. To stop the cluster you can run:\n\n\ncreate-cluster stop\n\n\n\n\nPlease have a look at the README file inside the same directory where the scrip exists. After configuring Redis Cluster, you can play with it using redis-cli command line utility. Examples of possible interaction is shown below:\n\n\n# interacting with the instance with port 7000\n$ redis-cli -c -p 7000\nredis 127.0.0.1:7000\n set foo bar\n-\n Redirected to slot [12182] located at 127.0.0.1:7002\nOK\nredis 127.0.0.1:7002\n set hello world\n-\n Redirected to slot [866] located at 127.0.0.1:7000\nOK\nredis 127.0.0.1:7000\n get foo\n-\n Redirected to slot [12182] located at 127.0.0.1:7002\n\nbar\n\nredis 127.0.0.1:7000\n get hello\n-\n Redirected to slot [866] located at 127.0.0.1:7000\n\nworld\n\n\n\n\n\nPlease note that using at least one slave for each master node is the best configuration that will help the Redis cluster to stay operational in case of failure. \n\n\nScaling Configurations\n\n\nRedis Sentinel acts as a configuration provider for clients and can be used in case you have a system with many master and slave nodes where managing configurations can be a great challenge.", 
            "title": "Scalability"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Scalability/#scaling-reads", 
            "text": "When your system grows bigger and the system read throughput increases more than what Redis server can handle, then you must scale your server to ensure that reads are as fast as possible.  The simplest way is to use the replication option that is supported by Redis by adding read-only slave servers. The replication is used mainly for scaling reads but it can  also be used for data redundancy and disaster recovery.", 
            "title": "Scaling Reads"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Scalability/#replication", 
            "text": "Redis supports a very simple to use master-slave replication option where you can configure as many as you need read-only slave servers to act like an exact copy of the master server. The master server will replicate its dataset frequently as configured with its slave. The replication process is done asynchronously where slaves periodically acknowledge how much data was successfully processed from the replication stream. The slaves can also have other slaves connected to them in a graph-like structure. Only the master will accept write operations and other slaves will be used just as read-only servers. The slave servers will throw errors if you try to write to them. Master instance will be available during the replication process since it is a non-blocking operation. However slaves can still be available during the replication but will serve requests with the old dataset only.", 
            "title": "Replication"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Scalability/#configuring-replication", 
            "text": "Redis replication process between master and slave is simple. The master receives a SYNC command then it saves the current dataset to disk as RDB file. During saving the file, any write commands received by clients will be logged down to a file. Later when the RDB file is saved to disk, the master will send the file to all connected slaves along with the log file. Slaves will receive the RDB file and save it on disk. Then slaves will load the RDB file into memory and apply the log file commands to the new loaded dataset.  If the link between the master and its slave goes down for any reason, a full synchronisation will be done once reconnected. To configure a Redis server to be a slave, just run the below command on the slave instance:  slaveof 192.168.1.1 6379  The above IP is the IP of the master server. If you want to stop the replication and make the slave server acts as a master, you can issue the below command:  SLAVEOF NO ONE  You can also configure the slave to use certain credentials when syncing up with the master server. You can use the below command:  config set masterauth  password   We will discuss later in the  availability section  how we can use Redis Sentinel to handle master or slave failures.", 
            "title": "Configuring Replication"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Scalability/#scaling-writes", 
            "text": "If your data grows beyond your memory or when you want to increase the overall system performance, you might need to scale up your Redis database by sharding your data among multiple Redis instances. This will allow for much larger database which will be the sum of the memory of all Redis instances.  In addition, the computational power and network bandwidth of your database will increase because you are using now multiple servers with multiple cores with multiple network adapters.    There are different methods available to partition your data. The simplest way is range partitioning which is done by mapping your data by range to different Redis instances. For example, if you have two instances R1 and R2, then you could say that users with ID 1 to 100 assigned to R1 and from 101 to 200 assigned with R2 and so forth. This method is easy but not so practical because of the cost of maintaining a mapping table and the data distribution in uneven. The other method is by using a hash function that will hash the keys and decide where to distribute the data.  The important question is who does the partitioning. In Redis, the partitioning can be done in the client side where the clients directly select the right instance to be used to write or read a certain key. Most of the clients or drivers that are provided by Redis have implementations for partitioning. Another implementation for partitioning is by using a proxy which means that clients send request to a proxy that is able to interact with Redis and will forward the clients' requests to the right Redis instance then send back the reply to the clients. A famous proxy used by Redis and Memcached is  Twemproxy . Finally it is also possible to use an implementation for partitioning called query routing where you just send your requests to a random instance that will forward your request to the right Redis instance.   Before Redis version 3.0, users usually tend to use client side implementations for partitioning their data to multiple Redis instances. But when Redis cluster was introduced, it is now the preferred way to get automatic partitioning and high availability. Redis cluster uses a hybrid implementation of query routing and some clients side implementation. I will explain in the following section how to use Redis cluster to do partitioning.", 
            "title": "Scaling Writes"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Scalability/#redis-cluster", 
            "text": "Redis cluster was introduced to allow users to do data partitioning automatically which is much easier and convenient. Redis cluster doesn't only provide a data partitioning capabilities but also allows for some degree of availability during partitioning. This means that the Redis database will continue to be operational even if some of the instances are experiencing failures.  The concept of Redis cluster is simple. The cluster will have a total of 16384 hash slots and will be distributed among all the instances in the cluster so that each instance will have a subset of these hash slots. Then each key will be mapped to its hash slot by simply taking the CRC16 of the key modulo \"16384\". Using this hash slots allow for easier management of the nodes inside the cluster. So you can easily add or remove nodes from the cluster by just either distributing its hash slots to the other nodes in case we want to remove it or by just moving a subset of the hash slots to it in case of adding. The node that has an empty set of hash slots will be automatically deleted by Redis cluster. Redis cluster also supports operations on multiple keys as long as all the keys are in the same hash slot by using a concept called hash tags.", 
            "title": "Redis Cluster"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Scalability/#configuring-redis-cluster", 
            "text": "Configuring Redis cluster isn't that easy and it would be better if you go through the Redis  cluster specifications  to do a serious configuration or deployment in a production environment. Below I will explain the configuration parameters related to Redis cluster that can be included in your redis.conf file.  cluster-enabled is used to include this instance as a cluster instance otherwise it will act as a standalone instance.\ncluster-config-file is used to mention where Redis cluster can save its automatically generated configuration file for this instance.\ncluster-node-timeout  is the maximum amount of time that a Redis cluster instance can be down.\ncluster-slave-validity-factor is simply a factor which will be multiplied by the timeout to  determine how much time it will take for the slave to fail when the link to the master is down.   To simplest way to configure Redis cluster is to use the create-cluster script which can be founded under utils/create-cluster directory in the Redis distribution. This script is just a simple shell script that will configure a Redis cluster to start 6 nodes with 3 master nodes and 3 slaves. To run the script just run the below commands:  create-cluster start\ncreate-cluster create  You will then get a message to accept the cluster layout, just reply with Yes. To stop the cluster you can run:  create-cluster stop  Please have a look at the README file inside the same directory where the scrip exists. After configuring Redis Cluster, you can play with it using redis-cli command line utility. Examples of possible interaction is shown below:  # interacting with the instance with port 7000\n$ redis-cli -c -p 7000\nredis 127.0.0.1:7000  set foo bar\n-  Redirected to slot [12182] located at 127.0.0.1:7002\nOK\nredis 127.0.0.1:7002  set hello world\n-  Redirected to slot [866] located at 127.0.0.1:7000\nOK\nredis 127.0.0.1:7000  get foo\n-  Redirected to slot [12182] located at 127.0.0.1:7002 bar \nredis 127.0.0.1:7000  get hello\n-  Redirected to slot [866] located at 127.0.0.1:7000 world   Please note that using at least one slave for each master node is the best configuration that will help the Redis cluster to stay operational in case of failure.", 
            "title": "Configuring Redis Cluster"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Scalability/#scaling-configurations", 
            "text": "Redis Sentinel acts as a configuration provider for clients and can be used in case you have a system with many master and slave nodes where managing configurations can be a great challenge.", 
            "title": "Scaling Configurations"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Security/", 
            "text": "Securing your Redis server from external threats is so important since if your server is exposed to the outsider world, a single command like FLUSHALL can delete all the data you have. This is why your Redis server shouldn't be exposed to the outside world and unauthorised requests should be denied. There should be always a layer between Redis server and the clients. For example a web application layer that provides ACLs and validating users and deciding which actions to be performed against Redis instance. Your Redis server can also be configured to bind to only one IP using the below command:\n\n\nbind 127.0.0.1\n\n\n\n\nSo Redis doesn't provide any access control and this should be provided by the a separate authorisation layer. However Redis provides an authentication mechanism that is optional and can be turned on from redis.conf. A client can send an AUTH command followed by a password which will be checked against the user's password that was set in the configuration file.\n\n\nRedis doesn't also support any encryption mechanism and this should also be implemented using a separate layer like using encryption mechanisms such as SSL proxy.\n\n\nA nice way of providing a security defence measures for your database is by using the \"rename-command\"  provided by Redis. Using this command you can simply rename the original commands names which reduces the risk that might happen if unauthorised clients access your server. An example of using this command is shown below:\n\n\nrename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52\n\n\n\n\nAs shown above, now CONIG command is renamed to an impossible to guess name. In addition, NoSQL injection attacks are impossible since Redis protocol has no concept of string escaping.", 
            "title": "Security"
        }, 
        {
            "location": "/Redis/Administration and Maintenance/Upgrading/", 
            "text": "Redis doesn't support online upgrades which means that the server must restart and the clients can't connect to it during the upgrade window. However there are some ways that you can use to support online upgrade. One way is to start a slave server and direct all the clients to it while you do an upgrade to the master. After the upgrade is finished in the master, we can stop the slave server and then redirect the clients requests again to the master server. An example is shown below:\n\n\nFirst we assume that the master server runs on the default port which 6379. Then we start another slave server with the below command:\n\n\nSLAVEOF localhost 6379\n\n\n\n\nNow the new server is a slave of the master server that we want to upgrade. This will also trigger the BGSAVE command in the master server which will create a snapshot of the dataset and transfer it to the slave server. Now since this new slave server is running fine, we can redirect our client to this server and do the upgrade in the master server. After the master server is upgraded and configured correctly, we can then redirect the client to it and stop the slave server.", 
            "title": "Upgrading"
        }, 
        {
            "location": "/Redis/Basics/Data Import and Export/", 
            "text": "Sometimes we want to migrate our old data or insert a large amount of preexistence data into our Redis server. Inserting a millions of keys of data to Redis at short amount of time and without blocking the server operations is very important feature. Luckily Redis supports a pipe mode that was designed in order to perform mass insertion. Using this utility is so simple, you just need to put all the data you want to insert in a file and then performs the below command:\n\n\ncat data.txt | redis-cli --pipe\n\n\n\n\nThis will produce an output similar to the below:\n\n\nAll data transferred. Waiting for the last reply...\nLast reply received from server.\nerrors: 0, replies: 1000000\n\n\n\n\nPipelining is also supported in Redis which is used to batch multiple commands and send them together to the server at once without waiting for their replies. At the end you can read the replies in a single step. Pipelining can improve system performance since the round trips between client and server are reduced. Pipelining is either provided using the Redis clients as shown below or by using Redis scripting that will be explained \nlater\n.\n\n\npipe = conn.pipeline(False) // False means without transaction support\npipe.zadd('viewed:' + token, item, timestamp)\npipe.zremrangebyrank('viewed:' + token, 0, -26)\npipe.zincrby('viewed:', item, -1)\npipe.execute()", 
            "title": "Data Import and Export"
        }, 
        {
            "location": "/Redis/Basics/Installation/", 
            "text": "Redis is very easy to install and it will take only couple of minutes for you to install it on your machine. This is because it comes with a simple shell that provides you with everything you need and it doesn't have any external dependencies that you need to consider before installation.\n\n\nThere are several options available for you to install Redis depending on your system configurations. I will show here how to install it using the most two commonly used options.\n\n\nSince Redis doesn't officially support windows, I will only show the installation steps on linux-based and Mac systems. However to install Redis in windows, there is a \nwindows port option\n developed by Microsoft Open Technologies that can be used.\n\n\n1- Building from the source:\n\n\nUsing the below scrip you can simply download, extract and build redis:\n\n\n$ wget http://download.redis.io/releases/redis-3.0.5.tar.gz\n$ tar xzf redis-3.0.5.tar.gz\n$ cd redis-3.0.5\n$ make\n\n\n\n\n\nNow the compiled binaries can be found inside src folder. To run the server run the below:\n\n\n$ cd src\n$ ./redis-server\n\n\n\n\n\nYou will get a warning stating that the configuration file (redis.conf) couldn't be found which is ok for now since Redis will use the defaults. We will talk about Redis configuration on details on later \nsection\n.\n\n\n2- Using package managers:\n\n\nInstalling redis using package managers is even more convienient and preferable for many users. Below you can see how to install Redis using the popular package managers:\n\n\nDebian/Ubuntu:\n\n\nsudo apt-get install redis-server\n\n\n\n\n\nFedora/Redhat/CentOS:\n\n\nsudo yum install redis\n\n\n\n\n\nMac OS X:\n\n\nport install redis\n\n\n\n\n\nbrew install redis\n\n\n\n\n\nThen after installation is complete, you can run Redis using:\n\n\nredis-server /usr/local/etc/redis.conf", 
            "title": "Installation"
        }, 
        {
            "location": "/Redis/Basics/Installation/#1-building-from-the-source", 
            "text": "Using the below scrip you can simply download, extract and build redis:  $ wget http://download.redis.io/releases/redis-3.0.5.tar.gz\n$ tar xzf redis-3.0.5.tar.gz\n$ cd redis-3.0.5\n$ make  Now the compiled binaries can be found inside src folder. To run the server run the below:  $ cd src\n$ ./redis-server  You will get a warning stating that the configuration file (redis.conf) couldn't be found which is ok for now since Redis will use the defaults. We will talk about Redis configuration on details on later  section .", 
            "title": "1- Building from the source:"
        }, 
        {
            "location": "/Redis/Basics/Installation/#2-using-package-managers", 
            "text": "Installing redis using package managers is even more convienient and preferable for many users. Below you can see how to install Redis using the popular package managers:  Debian/Ubuntu:  sudo apt-get install redis-server  Fedora/Redhat/CentOS:  sudo yum install redis  Mac OS X:  port install redis  brew install redis  Then after installation is complete, you can run Redis using:  redis-server /usr/local/etc/redis.conf", 
            "title": "2- Using package managers:"
        }, 
        {
            "location": "/Redis/Basics/Overview/", 
            "text": "Redis is an extremely fast NoSQL key-value in-memory database which stores data in different useful data structures such as strings, list, sets and sorted sets. Redis is much similar to memcached but with a built-in persistence which means it can keep your data even after server restarts.  In addition, you can use more complex data types to store your data instead of just strings. Redis supports many powerful features like built-in pub/sub, transaction support with optimistic locking and Lua Scripting. Redis is a very good choice if you are looking for a very fast and highly scalable and available data store solution. In the following sections, we are going through most of the features in more details.", 
            "title": "Overview"
        }, 
        {
            "location": "/Redis/Basics/Possible Use Cases/", 
            "text": "Generally many users prefer to choose redis or any other in-memory database to store their data only when performance or the supported data structures used in Redis is necessary. Otherwise they tend to use other databases for data where slower performance is not a problem or when the data is so big to fit in memory economically. \nRedis is a high performance database that can be scaled easily to hundreds of gigabytes of data and millions of requests per second. It also provides on-disk persistence and supports very unique data structures which makes it suitable for a variety of applications such as caching, as a message broker, as a chat server, in session management, as a distributed lock system, to implement queues, as a logging system, in any counting related applications,  in real time analytics, to implement leaderboards, as a voting system and many other applications.\n\n\nRedis can be used efficiently on ecommerce applications for cashing, session management, shopping cart management, real time analytics on cart, sessions, or in product related analytics such as to identify hot products or most and recent viewed products. I will explain in the examples section the complete implementation for a caching, session, shopping cart, and analytics management services that can be used on e-commerce  applications.", 
            "title": "Possible Use Cases"
        }, 
        {
            "location": "/Redis/Basics/Query Language/", 
            "text": "In order to do operations on the values stored in Redis such as add, update, get or remove, a set of commands for each data type are provided. Those commands can be executed on \nbulk\n and a partial \ntransaction\n is supported  as will be explained later in details. Executing these commands can be done using the built-in client \"redis-cli\" or by using one of the supported \nclients\n specific for many programming languages. Redis has a relatively short list of commands that can be easily learned in few hours. In the following sections I will explain the popular commands for each data type including commands on the \"key\". For a complete list of all the commands available in Redis, please check \nRedis documentation.\n.\n\n\nCommands on Key\n\n\nWe start with the \"DEL\" command that can be used to delete a single or multiple keys as shown below. A key is ignored if it doesn't exist. \n\n\nredis\n DEL key1 key2 key3\n(integer) 3\n\n\n\n\n\"EXISTS\" is another command that can be used to check if a single or multiple keys exists.\n\n\nredis\n EXISTS key1 key2\n(integer) 2\n\n\n\n\n\"KEYS pattern\" is used to search keys and returns all keys that match the given pattern. You should pay attention when using this command on a production environment since it can severely impact the system performance because it has a time complexity of O(N).\n\n\nredis\n KEYS *o*\n1) \ntwo\n\n2) \nfour\n\n\n\n\n\n\"RENAME\" is used to rename the key. \"RENAMENX\" is another command used to rename key only if the new key name doesn't exists.\n\n\nredis\n RENAME mykey myotherkey\nOK\n\n\n\n\nredis\n RENAMENX mykey myotherkey\n(integer) 0 // myotherkey already exists\n\n\n\n\n\"TYPE\" is a command to get the underline data type of a particular key.\n\n\nredis\n TYPE key1\nstring\nredis\n TYPE key2\nlist\n\n\n\n\nCommands on String\n\n\n\"GET\" and \"SET\" are used to retrieve/set the value of a certain key. When you use \"SET\", it will overwrite the value if it already exists. However \"SETNX\" will set the value only if it doesn't exist. Other similar commands are \"MGET,MSET,MSETNX\" which are used to get or set multiple keys at the same time.\n\n\nredis\n SET mykey \nHello\n\nOK\nredis\n GET mykey\n\nHello\n\nredis\n SET mykey \nWorld\n\nOK\nredis\n GET mykey\n\nWorld\n\nredis\n SETNX mykey \nAnotherWord\n\n(integer) 0\nredis\n SETNX otherKey \nAnotherWord\n\n(integer) 1\n\n\n\n\n\"INCR,DECR\" are two commands that can be used to increment or decrement the the integer/floating number stored inside the string by one. \"INCRBY,DECRBY\" can also be used to increment or decrement by a certain value.\n\n\nredis\n SET mykey 10\nOK\nredis\n INCR mykey\n(integer) 11\nredis\n DECR mykey\n(integer) 10\nredis\n INCRBY mykey 2\n(integer) 12\nredis\n DECRBY mykey 2\n(integer) 10\n\n\n\n\n\"GETRANGE,SETRANGE\" are similar to substr and replace functions in javascript which get or set a substring stored in a key based on a particular range.\n\n\nredis\n SET mykey \nThis is a string\n\nOK\nredis\n GETRANGE mykey 0 3\n\nThis\n\nredis\n SETRANGE mykey 0 \nThat\n\n(integer) 16\nredis\n GET mykey\n\nThat is a string\n\n\n\n\n\nFinally the \"APPEND\" command is used to appends a value to the end of a string if the string already exists.\n\n\nredis\n APPEND mykey \nHello\n\n(integer) 5\n\n\n\n\n\n\n\n\nCommands on Lists\n\n\n\n\n\n\n\"LPOP,LPUSH,RPOP,RPUSH\" are the most important commands on lists which are used if your want to insert or remove the first or last element in the list. \"BLPOP,BLPUSH,BRPOP,BRPUSH\" are doing the same but are the blocking version which means it blocks the connection when there are no elements to pop or push from the given list.\n\n\nredis\n RPUSH mylist \none\n\n(integer) 1\nredis\n RPUSH mylist \ntwo\n\n(integer) 2\nredis\n RPUSH mylist \nthree\n\n(integer) 3\nredis\n LPOP mylist\n\none\n\nredis\n RPOP mylist\n\nthree\n\n\n\n\n\n\"LRANGE\" is used to get elements within a specific range.\n\n\nredis\n RPUSH mylist \none\n\n(integer) 1\nredis\n RPUSH mylist \ntwo\n\n(integer) 2\nredis\n RPUSH mylist \nthree\n\n(integer) 3\nredis\n LRANGE mylist 0 0\n1) \none\n\n\n\n\n\n\"LLEN\" returns the length of the list.\n\n\nredis\n LLEN mylist\n(integer) 3\n\n\n\n\n\"LTRIM\" is used to remove elements from the list by a certain range.\n\n\nLPUSH mylist someelement\nLTRIM mylist 0 99 // Remove the first 100 elements of the list\n\n\n\n\n\"LINDEX\" to get an element at a specific index. This operation has a time complexity O(N) and might be slow for large lists.\n\n\nredis\n LINDEX mylist 0\n\nHello\n\n\n\n\n\nCommands on Hashes\n\n\n\"HGET,HSET,HMGET,HMSET\" commands are similar to string commands, they can be used to get or set a single or multiple value(s) of a field(s) inside the hash. \"HSETNX\" is also used to set a value of a field inside the hash only if it doesn't exist.\n\n\nredis\n HSET myhash field1 \nfoo\n\n(integer) 1\nredis\n HGET myhash field1\n\nfoo\n\n\n\n\n\n\"HGETALL\" is used to get all fields and their values inside a hash while \"HKEYS\" or \"HVALS\" are used to get only either all the field keys or all the values.\n\n\nredis\n HGETALL myhash\n1) \nfield1\n\n2) \nvalue1\n\n3) \nfield2\n\n4) \nvalue2\n\nredis\n HVALS myhash\n1) \nvalue1\n\n2) \nvalue2\n\nredis\n HKEYS myhash\n1) \nfield1\n\n2) \nfield2\n\n\n\n\n\n\"HEXISTS\" is used to check if a certain field has a certain value, \"HINCREBY\" or \"HINCREBYFLOAT\" are used to increment the integer or float value stored inside a certain field. Finally HDEL is used to delete a specific field inside the hash.\n\n\nredis\n HEXISTS myhash field1\n(integer) 1\nredis\n HINCRBY myhash field 1\n(integer) 6\nredis\n HDEL myhash field1\n(integer) 1\n\n\n\n\nCommands on Sets\n\n\n\"SADD\" is used to add a specific member to the set. If the member is already in the set, the operation will be just ignored. \"SISMEMBER\" is then used to check if an element is inside the set or not. You can use \"SMEMBERS\" to get all the elements inside the set and \"SCARD\" to get the number of members in the set.\n\n\nredis\n SADD myset \nHello\n\n(integer) 1\nredis\n SISMEMBER myset \nHello\n\n(integer) 1\nredis\n SMEMBERS myset\n1) \nHello\n\nredis\n SCARD myset\n(integer) 1\n\n\n\n\n\"SDIFF,SDIFFSTORE,SINTER,SINTERSTORE,SUNION,SUNIONSTORE\" are very important commands that can be executed on the sets to do group or filter the data inside different sets which will be covered on more details when discussing grouping and filtering data \nlater\n.\n\n\nCommands on Sorted Sets\n\n\n\"ZADD\" is used like normal sets to add elements to the sorted set but we should also provide a score for each element. The scroe can be used later to sort the set, and do range queries on the set.\n\n\nredis\n ZADD myzset 2 \ntwo\n 3 \nthree\n\n(integer) 2 \n\n\n\n\n\"ZCARD\" can be used to get the number of elements inside the sorted set but \"ZCOUNT\" get the number of elements with score between a certain range.\n\n\nredis\n ZADD myzset 1 \none\n\n(integer) 1\nredis\n ZADD myzset 2 \ntwo\n\n(integer) 1\nredis\n ZADD myzset 3 \nthree\n\n(integer) 1\nredis\n ZCARD myzset\n(integer) 3\nredis\n ZCOUNT myzset 1 2\n(integer) 2\n\n\n\n\n\"ZRANGE\" gets the elements in a sorted set between certain range. \"ZRANK\" determine the rank of an element in the sorted set based on its score ordered from low to high.  \"ZREVRANK\" will do the same but ordered from high to low. \"ZRANGEBYSCORE\" is used to get elements in the sorted set with score between a certain range.\n\n\nredis\n ZRANGE myzset 2 3\n1) \nvalue\n\nredis\n ZRANK myzset \nvalue\n\n(integer) 2\nredis\n ZRANGEBYSCORE myzset 3 5\n1) \none\n\n2) \ntwo\n\n\n\n\n\n\"ZREMRANGEBYRANK,ZREMRANGEBYSCORE\" remove elements from the sorted set based on either the range of score or rank.\n\n\nredis\n ZREMRANGEBYRANK myzset 0 1\n(integer) 2\nredis\n ZREMRANGEBYSCORE myzset 1 2\n(integer) 1\n\n\n\n\n\"ZINTERSTORE,ZUNIONSTORE\" are used to intersect or union two sorted sets. \n\n\nBitmaps Commands\n\n\n\"GETBIT \n SETBIT\" are used to get or set a bit value at a certain offset in the string value stored at the key. \"BITCOUNT\" returns the number of bits in a string.\n\n\nredis\n SETBIT mykey 7 0\n(integer) 1\nredis\n GETBIT mykey 0\n(integer) 0\nredis\n SET mykey \nfoobar\n\nOK\nredis\n BITCOUNT mykey\n(integer) 26\n\n\n\n\n\"BITOP\" is used to perform a bitwise operation between multiple keys and store the result in a destination key. Operations are AND, OR, XOR and NOT.\n\n\nredis\n BITOP AND dest key1 key2\n(integer) 6\n\n\n\n\nHyperLogLog Commands\n\n\nThere are only 3 commands for this data type as explained below:\n\n\n\"PFADD\" is used to add elements to the Hyperloglog structure.\n\n\nredis\n PFADD hll a b c d e f g\n(integer) 1\n\n\n\n\n\"PFCOUNT\" is used to count how many elements are inside the Hyperloglog\n\n\nredis\n PFADD hll foo bar zap\n(integer) 1\nredis\n PFCOUNT hll\n(integer) 3\n\n\n\n\n\"PFMERGE\" is used mainly to merge multiple Hyperloglog together into a unique Hyperloglog and eliminate any duplicates. The result is stored then into a destination Hyperloglog\n\n\nredis\n PFADD hll1 foo bar zap a\n(integer) 1\nredis\n PFADD hll2 a b c foo\n(integer) 1\nredis\n PFMERGE hll3 hll1 hll2\nOK\nredis\n PFCOUNT hll3\n(integer) 6", 
            "title": "Query Language"
        }, 
        {
            "location": "/Redis/Basics/Query Language/#commands-on-key", 
            "text": "We start with the \"DEL\" command that can be used to delete a single or multiple keys as shown below. A key is ignored if it doesn't exist.   redis  DEL key1 key2 key3\n(integer) 3  \"EXISTS\" is another command that can be used to check if a single or multiple keys exists.  redis  EXISTS key1 key2\n(integer) 2  \"KEYS pattern\" is used to search keys and returns all keys that match the given pattern. You should pay attention when using this command on a production environment since it can severely impact the system performance because it has a time complexity of O(N).  redis  KEYS *o*\n1)  two \n2)  four   \"RENAME\" is used to rename the key. \"RENAMENX\" is another command used to rename key only if the new key name doesn't exists.  redis  RENAME mykey myotherkey\nOK  redis  RENAMENX mykey myotherkey\n(integer) 0 // myotherkey already exists  \"TYPE\" is a command to get the underline data type of a particular key.  redis  TYPE key1\nstring\nredis  TYPE key2\nlist", 
            "title": "Commands on Key"
        }, 
        {
            "location": "/Redis/Basics/Query Language/#commands-on-string", 
            "text": "\"GET\" and \"SET\" are used to retrieve/set the value of a certain key. When you use \"SET\", it will overwrite the value if it already exists. However \"SETNX\" will set the value only if it doesn't exist. Other similar commands are \"MGET,MSET,MSETNX\" which are used to get or set multiple keys at the same time.  redis  SET mykey  Hello \nOK\nredis  GET mykey Hello \nredis  SET mykey  World \nOK\nredis  GET mykey World \nredis  SETNX mykey  AnotherWord \n(integer) 0\nredis  SETNX otherKey  AnotherWord \n(integer) 1  \"INCR,DECR\" are two commands that can be used to increment or decrement the the integer/floating number stored inside the string by one. \"INCRBY,DECRBY\" can also be used to increment or decrement by a certain value.  redis  SET mykey 10\nOK\nredis  INCR mykey\n(integer) 11\nredis  DECR mykey\n(integer) 10\nredis  INCRBY mykey 2\n(integer) 12\nredis  DECRBY mykey 2\n(integer) 10  \"GETRANGE,SETRANGE\" are similar to substr and replace functions in javascript which get or set a substring stored in a key based on a particular range.  redis  SET mykey  This is a string \nOK\nredis  GETRANGE mykey 0 3 This \nredis  SETRANGE mykey 0  That \n(integer) 16\nredis  GET mykey That is a string   Finally the \"APPEND\" command is used to appends a value to the end of a string if the string already exists.  redis  APPEND mykey  Hello \n(integer) 5", 
            "title": "Commands on String"
        }, 
        {
            "location": "/Redis/Basics/Query Language/#commands-on-lists", 
            "text": "\"LPOP,LPUSH,RPOP,RPUSH\" are the most important commands on lists which are used if your want to insert or remove the first or last element in the list. \"BLPOP,BLPUSH,BRPOP,BRPUSH\" are doing the same but are the blocking version which means it blocks the connection when there are no elements to pop or push from the given list.  redis  RPUSH mylist  one \n(integer) 1\nredis  RPUSH mylist  two \n(integer) 2\nredis  RPUSH mylist  three \n(integer) 3\nredis  LPOP mylist one \nredis  RPOP mylist three   \"LRANGE\" is used to get elements within a specific range.  redis  RPUSH mylist  one \n(integer) 1\nredis  RPUSH mylist  two \n(integer) 2\nredis  RPUSH mylist  three \n(integer) 3\nredis  LRANGE mylist 0 0\n1)  one   \"LLEN\" returns the length of the list.  redis  LLEN mylist\n(integer) 3  \"LTRIM\" is used to remove elements from the list by a certain range.  LPUSH mylist someelement\nLTRIM mylist 0 99 // Remove the first 100 elements of the list  \"LINDEX\" to get an element at a specific index. This operation has a time complexity O(N) and might be slow for large lists.  redis  LINDEX mylist 0 Hello", 
            "title": "Commands on Lists"
        }, 
        {
            "location": "/Redis/Basics/Query Language/#commands-on-hashes", 
            "text": "\"HGET,HSET,HMGET,HMSET\" commands are similar to string commands, they can be used to get or set a single or multiple value(s) of a field(s) inside the hash. \"HSETNX\" is also used to set a value of a field inside the hash only if it doesn't exist.  redis  HSET myhash field1  foo \n(integer) 1\nredis  HGET myhash field1 foo   \"HGETALL\" is used to get all fields and their values inside a hash while \"HKEYS\" or \"HVALS\" are used to get only either all the field keys or all the values.  redis  HGETALL myhash\n1)  field1 \n2)  value1 \n3)  field2 \n4)  value2 \nredis  HVALS myhash\n1)  value1 \n2)  value2 \nredis  HKEYS myhash\n1)  field1 \n2)  field2   \"HEXISTS\" is used to check if a certain field has a certain value, \"HINCREBY\" or \"HINCREBYFLOAT\" are used to increment the integer or float value stored inside a certain field. Finally HDEL is used to delete a specific field inside the hash.  redis  HEXISTS myhash field1\n(integer) 1\nredis  HINCRBY myhash field 1\n(integer) 6\nredis  HDEL myhash field1\n(integer) 1", 
            "title": "Commands on Hashes"
        }, 
        {
            "location": "/Redis/Basics/Query Language/#commands-on-sets", 
            "text": "\"SADD\" is used to add a specific member to the set. If the member is already in the set, the operation will be just ignored. \"SISMEMBER\" is then used to check if an element is inside the set or not. You can use \"SMEMBERS\" to get all the elements inside the set and \"SCARD\" to get the number of members in the set.  redis  SADD myset  Hello \n(integer) 1\nredis  SISMEMBER myset  Hello \n(integer) 1\nredis  SMEMBERS myset\n1)  Hello \nredis  SCARD myset\n(integer) 1  \"SDIFF,SDIFFSTORE,SINTER,SINTERSTORE,SUNION,SUNIONSTORE\" are very important commands that can be executed on the sets to do group or filter the data inside different sets which will be covered on more details when discussing grouping and filtering data  later .", 
            "title": "Commands on Sets"
        }, 
        {
            "location": "/Redis/Basics/Query Language/#commands-on-sorted-sets", 
            "text": "\"ZADD\" is used like normal sets to add elements to the sorted set but we should also provide a score for each element. The scroe can be used later to sort the set, and do range queries on the set.  redis  ZADD myzset 2  two  3  three \n(integer) 2   \"ZCARD\" can be used to get the number of elements inside the sorted set but \"ZCOUNT\" get the number of elements with score between a certain range.  redis  ZADD myzset 1  one \n(integer) 1\nredis  ZADD myzset 2  two \n(integer) 1\nredis  ZADD myzset 3  three \n(integer) 1\nredis  ZCARD myzset\n(integer) 3\nredis  ZCOUNT myzset 1 2\n(integer) 2  \"ZRANGE\" gets the elements in a sorted set between certain range. \"ZRANK\" determine the rank of an element in the sorted set based on its score ordered from low to high.  \"ZREVRANK\" will do the same but ordered from high to low. \"ZRANGEBYSCORE\" is used to get elements in the sorted set with score between a certain range.  redis  ZRANGE myzset 2 3\n1)  value \nredis  ZRANK myzset  value \n(integer) 2\nredis  ZRANGEBYSCORE myzset 3 5\n1)  one \n2)  two   \"ZREMRANGEBYRANK,ZREMRANGEBYSCORE\" remove elements from the sorted set based on either the range of score or rank.  redis  ZREMRANGEBYRANK myzset 0 1\n(integer) 2\nredis  ZREMRANGEBYSCORE myzset 1 2\n(integer) 1  \"ZINTERSTORE,ZUNIONSTORE\" are used to intersect or union two sorted sets.", 
            "title": "Commands on Sorted Sets"
        }, 
        {
            "location": "/Redis/Basics/Query Language/#bitmaps-commands", 
            "text": "\"GETBIT   SETBIT\" are used to get or set a bit value at a certain offset in the string value stored at the key. \"BITCOUNT\" returns the number of bits in a string.  redis  SETBIT mykey 7 0\n(integer) 1\nredis  GETBIT mykey 0\n(integer) 0\nredis  SET mykey  foobar \nOK\nredis  BITCOUNT mykey\n(integer) 26  \"BITOP\" is used to perform a bitwise operation between multiple keys and store the result in a destination key. Operations are AND, OR, XOR and NOT.  redis  BITOP AND dest key1 key2\n(integer) 6", 
            "title": "Bitmaps Commands"
        }, 
        {
            "location": "/Redis/Basics/Query Language/#hyperloglog-commands", 
            "text": "There are only 3 commands for this data type as explained below:  \"PFADD\" is used to add elements to the Hyperloglog structure.  redis  PFADD hll a b c d e f g\n(integer) 1  \"PFCOUNT\" is used to count how many elements are inside the Hyperloglog  redis  PFADD hll foo bar zap\n(integer) 1\nredis  PFCOUNT hll\n(integer) 3  \"PFMERGE\" is used mainly to merge multiple Hyperloglog together into a unique Hyperloglog and eliminate any duplicates. The result is stored then into a destination Hyperloglog  redis  PFADD hll1 foo bar zap a\n(integer) 1\nredis  PFADD hll2 a b c foo\n(integer) 1\nredis  PFMERGE hll3 hll1 hll2\nOK\nredis  PFCOUNT hll3\n(integer) 6", 
            "title": "HyperLogLog Commands"
        }, 
        {
            "location": "/Redis/Basics/Transaction Support/", 
            "text": "Redis provides a partial transactions support and can even provide almost full transactions when used with its built-in persistencey feature as will be explained \nlater\n.  \n\n\nRedis is a single threaded application which means that when a command is executed, it will be always the only one. So the idea behind transactions support in Redis is to wrap the transaction commands and execute them all as a single command. This is done by using MULTI/EXEC commands, you start the transaction by using MULTI command, then all the following commands will be queued. After all transaction commands are queued, they will be executed all as a single command using the EXEC command. The transaction commands will be executed sequentially and it will never happen that another command executed in the middle.\n\n\nThe transaction commands will be either executed together or none will be executed. This  provides us with an atomic transaction. This means that if the server crashes or the connection lost during executing the commands, none will be executed. However the problem occurs when any one of the transaction commands are mistakenly executed (not syntax mistakes) due to some programming bug. In this case unfortunately Redis doesn't provide any way for a Rollback as in traditional databases. Rollback isn't supported due to reasons related to performance.  \n\n\nDISCARD can be used to abort a transaction before running EXEC. Example of using MULTI/EXEC is shown below:  \n\n\n MULTI\nOK\n\n INCR foo\nQUEUED\n\n DISCARD\nOK\n\n MULTI\nOK\n\n INCR foo\nQUEUED\n\n EXEC\n\n\n\n\nTo avoid race conditions that will happen when another client tries to modify the same value used in a transaction, optimistic locking is provided using WATCH/UNWATCH commands. WATCH can be used to monitor a certain key during the transaction. If the value is modified during the transaction, you get notified then you can abort the transaction and try again. You can either unwatch a certain key or flush all watched keys by using UNWATCH command. Example is given below:\n\n\n        try {\n            this.jedisClient.watch(\ninventory:\n + sellerID);\n            if (!this.jedisClient.sismember(\ninventory:\n + sellerID, sku)) {\n                // inventory doesn't have this item\n                this.jedisClient.unwatch();\n                tryAgain();\n            }\n\n            Transaction transaction = this.jedisClient.multi();\n\n            responses.add(transaction.srem(\ninventory:\n + sellerID, sku));\n            responses.add(transaction.sadd(\nbought:\n + token, sku));\n\n            transaction.exec();\n\n            return checkReponses(responses);\n\n        } catch (Exception e) {\n        // race condition\n        tryAgain();\n        }\n\n\n\n\nFinally, Redis support scripting using LUA which is transactional by definition since it can be executed as a single command. Hence Lua scripting can be used to support transactions. More about Redis scripting will be given \nlater\n.", 
            "title": "Transaction Support"
        }, 
        {
            "location": "/Redis/Basics/Underline Structure/", 
            "text": "Underline data structures:\n\n\nAlthough Redis is a key-value datastore, it supports more than just the plain mapping of string keys to the string values. Instead it maps a key to different useful data structures that can be used to develop complex use cases. The key itself is binary safe which means it can be either a normal string or even a binary content of a file. However it is a good practice not to use long keys since it will impact performance and increase memory usage. In addition, keys should be consistent and can be used to define namespaces for your applications e.g school:teacher:john. In this section, I am going to give a quick overview of the supported data types that can be used in Redis to store your data.\n\n\nStrings\n\n\nString is the simplest data type in Redis. It is also the most typically used data type in other key-value data stores. Inside this data type, you can store not only sting but also integers, floating point values or even a binary data such as an image or any file. However, the size of the stored value shouldn't exceed a maximum 512MB of data.\n\n\nLists\n\n\nThis data type is a sequence of ordered elements that follows the well-known linked list structure. The elements are added only to the head or the tail of the list which is an extremely fast operation and takes a constant time. However accessing the elements by index is not so fast and will requires an amount of work proportional to the accessed index. This data type is the best candidate be used to implement queues.\n\n\nHashes\n\n\nRedis hashes can be used to store mappings of keys to values where values are of a String data type. The hash data type in Redis can be used to store complete complex objects and can be compared to the documents in a documents-based database or like a row in a relational database. An example would be to use the hash to store the shopping cart object of an e-commerce application.\n\n\nSets and Sorted Sets\n\n\nSets can be used to store unordered sequence of strings that are unique. It means that if you try to add an already existing element to the set, it will just ignore the operation. Sets support very useful operations such as intersect and union which can be used to group and filter the data. Sorted Sets on the other hand are a special case of the set implementation where it defines a score for each string. The score allows you to get the elements inside the sorted sets by a particular order. Also you can retrieve ordered elements in the sorted set by score range.\n\n\nbitmaps\n\n\nThese aren't actually a separate data type, but a special case of string data type when using a special set of commands. Bitmap commands are mainly used to increase performance by saving memory space when storing information as bits. \n\n\nHyperLogLogs\n\n\nHyperloglogs are probabilistic data structures that are used to count unique items. The idea behind this data type is to avoid storing amount of data in memory that is proportional to the items that need to be counted. Instead by using a special algorithm to store only a constant amount of memory (around 12K bytes in the worse case) but with an error (less than 1% in Redis implementation) of the estimated count. Hyperloglogs are encoded as strings, hence they are sharing similar commands.", 
            "title": "Underline Structure"
        }, 
        {
            "location": "/Redis/Basics/Underline Structure/#underline-data-structures", 
            "text": "Although Redis is a key-value datastore, it supports more than just the plain mapping of string keys to the string values. Instead it maps a key to different useful data structures that can be used to develop complex use cases. The key itself is binary safe which means it can be either a normal string or even a binary content of a file. However it is a good practice not to use long keys since it will impact performance and increase memory usage. In addition, keys should be consistent and can be used to define namespaces for your applications e.g school:teacher:john. In this section, I am going to give a quick overview of the supported data types that can be used in Redis to store your data.", 
            "title": "Underline data structures:"
        }, 
        {
            "location": "/Redis/Basics/Underline Structure/#strings", 
            "text": "String is the simplest data type in Redis. It is also the most typically used data type in other key-value data stores. Inside this data type, you can store not only sting but also integers, floating point values or even a binary data such as an image or any file. However, the size of the stored value shouldn't exceed a maximum 512MB of data.", 
            "title": "Strings"
        }, 
        {
            "location": "/Redis/Basics/Underline Structure/#lists", 
            "text": "This data type is a sequence of ordered elements that follows the well-known linked list structure. The elements are added only to the head or the tail of the list which is an extremely fast operation and takes a constant time. However accessing the elements by index is not so fast and will requires an amount of work proportional to the accessed index. This data type is the best candidate be used to implement queues.", 
            "title": "Lists"
        }, 
        {
            "location": "/Redis/Basics/Underline Structure/#hashes", 
            "text": "Redis hashes can be used to store mappings of keys to values where values are of a String data type. The hash data type in Redis can be used to store complete complex objects and can be compared to the documents in a documents-based database or like a row in a relational database. An example would be to use the hash to store the shopping cart object of an e-commerce application.", 
            "title": "Hashes"
        }, 
        {
            "location": "/Redis/Basics/Underline Structure/#sets-and-sorted-sets", 
            "text": "Sets can be used to store unordered sequence of strings that are unique. It means that if you try to add an already existing element to the set, it will just ignore the operation. Sets support very useful operations such as intersect and union which can be used to group and filter the data. Sorted Sets on the other hand are a special case of the set implementation where it defines a score for each string. The score allows you to get the elements inside the sorted sets by a particular order. Also you can retrieve ordered elements in the sorted set by score range.", 
            "title": "Sets and Sorted Sets"
        }, 
        {
            "location": "/Redis/Basics/Underline Structure/#bitmaps", 
            "text": "These aren't actually a separate data type, but a special case of string data type when using a special set of commands. Bitmap commands are mainly used to increase performance by saving memory space when storing information as bits.", 
            "title": "bitmaps"
        }, 
        {
            "location": "/Redis/Basics/Underline Structure/#hyperloglogs", 
            "text": "Hyperloglogs are probabilistic data structures that are used to count unique items. The idea behind this data type is to avoid storing amount of data in memory that is proportional to the items that need to be counted. Instead by using a special algorithm to store only a constant amount of memory (around 12K bytes in the worse case) but with an error (less than 1% in Redis implementation) of the estimated count. Hyperloglogs are encoded as strings, hence they are sharing similar commands.", 
            "title": "HyperLogLogs"
        }, 
        {
            "location": "/Redis/Data Model/Data Layout/", 
            "text": "In case of relational databases, we need to think about our data layout and do a schema design before start interacting with our database. However in Redis there is no schema and hence no need for data design. We just need to think about our underline data structure that we need to store. Then we need to think about which data types we need to use in order to represent this data. An example would be to model an article voting system. We would store the article object in Hash. Then we can store the list of articles ordered by votes in a sorted sets where score is the number of votes. Finally a normal set can store the name of the users who have voted for a particular article.", 
            "title": "Data Layout"
        }, 
        {
            "location": "/Redis/Data Model/Nested Data Structures/", 
            "text": "Redis doesn't support nested structures. For instance, the hash fields can only have a string data type and hence can't embed other hashes, sets, sorted sets or lists. However you can still store in the string data type any stream of bytes which can be JSON data but that wont be helpful since you can't query this nested data.", 
            "title": "Nested Data Structures"
        }, 
        {
            "location": "/Redis/Data Model/Normalization or Denormalization/", 
            "text": "Denormalisation can be used in Redis wherever it is needed if it will give better performance or model data relationships.", 
            "title": "Normalization or Denormalization"
        }, 
        {
            "location": "/Redis/Data Model/Referential Integrity/", 
            "text": "Since Redis isn't a relational database, referential integrity is not maintained by Redis, and must be enforced by the client application. Also there is no concept of Foreign key for the same reasons.", 
            "title": "Referential Integrity"
        }, 
        {
            "location": "/Redis/Data Model/Relational Data Support/", 
            "text": "NoSQL databases are not always the best choice when it comes to relational data. When you think of using key-value database, you are usually looking for limited use cases such as for caching, session management or to maintain a queue. The same applies for Redis, however it can still be used to store application domain data because it has a built-in persistency. Using Redis for complex relational data isn't usually recommended, however Redis supports modelling data relationships as will be explained in this section. \n\n\nA small example is if you want to model the customer order one-to-many relationship where each customer have many orders. You can model it in Redis by storing the customer and order details in a hash to act like a table row in RDMS. Then you can use a set to store all the order ids of a particular customer. Additionally, we can use a sorted set to store the same thing where the score can be a timestamp that will allow us later to retrieve customer orders sorted by time.  \n\n\n// Below are few customers set inside hashes:\n\n hmset customer:1 name John   ... more fields ...\n\n hmset customer:2 name Tom    ... more fields ...\n\n hmset customer:3 name Chris  ... more fields ...\n\n// Below are few orders\n\n hmset order:1    ...  fields ...\n\n hmset order:2    ...  fields ...\n\n hmset order:3    ...  fields ...\n\n hmset order:4    ...  fields ...\n\n hmset order:5    ...  fields ...\n\n// Defining one to many relations\n// customer:1 has made order:1 , Order:2\n// customer:2 has made order:3\n// customer:3 has made order:4, order:5\n// For each customer, we keep the references to orders in a set\n\n sadd orders:customer1 1 2\n\n sadd orders:customer2 3\n\n sadd orders:customer3 4 5\n\n// Then it will be easy to get the orders of a particular customer\n\n smembers orders:customer1\n\n 1) \n1\n\n2) \n2\n\n\n// then get the order details\n\n hmget order:1 order:2\n\n\n\n\nIn the above example, we show how to model a one-to-many relationship. However modelling one- to-one or many-to-many relationships are achieved in similar way. \n\n\nJoins\n\n\nThe closest concept in Redis to joins in relational database will be the use of operations such as intersect, union or difference in the sets. Sets supports below operations:\n\n\nSDIFF which can be used to get the elements of a set that results from taking the difference between the first set and all the following sets. SDIFFSTORE is the same but can store the result in a destination set. Example is given below:\n\n\nredis\n SADD key1 \na\n\n(integer) 1\nredis\n SADD key1 \nb\n\n(integer) 1\nredis\n SADD key1 \nc\n\n(integer) 1\nredis\n SADD key2 \nc\n\n(integer) 1\nredis\n SADD key2 \nd\n\n(integer) 1\nredis\n SADD key2 \ne\n\n(integer) 1\nredis\n SADD key3 \nb\n\n(integer) 1\nredis\n SDIFF key1 key2 key3\n1) \na\n\n\n\n\n\nSINTER is another set command that is used to return the elements of a set that results from the intersection of all the provided sets. SINTERSTORE is the same but can store the result in a destinations set. Example is given below:\n\n\nredis\n SADD key1 \na\n\n(integer) 1\nredis\n SADD key1 \nb\n\n(integer) 1\nredis\n SADD key1 \nc\n\n(integer) 1\nredis\n SADD key2 \nc\n\n(integer) 1\nredis\n SADD key2 \nd\n\n(integer) 1\nredis\n SADD key2 \ne\n\n(integer) 1\nredis\n SINTERSTORE key key1 key2\n(integer) 1\nredis\n SMEMBERS key\n1) \nc\n\n\n\n\n\nSUNION is used to return the elements of a set that results from the union of all the provided sets. SUNIONSTORE is the same but can store the result in a destinations set. Example is given below: \n\n\nredis\n SADD key1 \na\n\n(integer) 1\nredis\n SADD key1 \nb\n\n(integer) 1\nredis\n SADD key1 \nc\n\n(integer) 1\nredis\n SADD key2 \nc\n\n(integer) 1\nredis\n SADD key2 \nd\n\n(integer) 1\nredis\n SADD key2 \ne\n\n(integer) 1\nredis\n SUNION key1 key2\n1) \na\n\n2) \nb\n\n3) \nc\n\n4) \nd\n\n5) \ne\n\n\n\n\n\nSorted sets provide similar operations which are explained below:\n\n\nZINTERSTORE is used to calculate the intersection of some keys in sorted sets (you need to provide the number of keys) and stores the result in a destination set. You can use two options for the resulted intersection either WEIGHTS or AGGREGATE. WEIGHTS option is used to specify a multiplication factor for each input in the sorted set which will be used to calculate the score in the resulted set. AGGREGATE option is used to specify how the results of the intersection is aggregated. Examples are SUM (default), MIN, or MAX which means that the resulting set will contain either the sum, minimum or maximum score across the input sets. Example is shown below:\n\n\nredis\n ZADD zset1 1 \none\n\n(integer) 1\nredis\n ZADD zset1 2 \ntwo\n\n(integer) 1\nredis\n ZADD zset2 1 \none\n\n(integer) 1\nredis\n ZADD zset2 2 \ntwo\n\n(integer) 1\nredis\n ZADD zset2 3 \nthree\n\n(integer) 1\nredis\n ZINTERSTORE out 2 zset1 zset2 WEIGHTS 2 3\n(integer) 2\nredis\n ZRANGE out 0 -1 WITHSCORES\n1) \none\n\n2) \n5\n\n3) \ntwo\n\n4) \n10\n\n\n\n\n\nZUNIONSTORE is the same as ZINTERSTORE explained above but instead it calculate the union of multiple input sorted sets. It uses also the same options (WEIGHTS/AGGREGATE). Example is given below:\n\n\nredis\n ZADD zset1 1 \none\n\n(integer) 1\nredis\n ZADD zset1 2 \ntwo\n\n(integer) 1\nredis\n ZADD zset2 1 \none\n\n(integer) 1\nredis\n ZADD zset2 2 \ntwo\n\n(integer) 1\nredis\n ZADD zset2 3 \nthree\n\n(integer) 1\nredis\n ZUNIONSTORE out 2 zset1 zset2 WEIGHTS 2 3\n(integer) 3\nredis\n ZRANGE out 0 -1 WITHSCORES\n1) \none\n\n2) \n5\n\n3) \nthree\n\n4) \n9\n\n5) \ntwo\n\n6) \n10", 
            "title": "Relational Data Support"
        }, 
        {
            "location": "/Redis/Data Model/Relational Data Support/#joins", 
            "text": "The closest concept in Redis to joins in relational database will be the use of operations such as intersect, union or difference in the sets. Sets supports below operations:  SDIFF which can be used to get the elements of a set that results from taking the difference between the first set and all the following sets. SDIFFSTORE is the same but can store the result in a destination set. Example is given below:  redis  SADD key1  a \n(integer) 1\nredis  SADD key1  b \n(integer) 1\nredis  SADD key1  c \n(integer) 1\nredis  SADD key2  c \n(integer) 1\nredis  SADD key2  d \n(integer) 1\nredis  SADD key2  e \n(integer) 1\nredis  SADD key3  b \n(integer) 1\nredis  SDIFF key1 key2 key3\n1)  a   SINTER is another set command that is used to return the elements of a set that results from the intersection of all the provided sets. SINTERSTORE is the same but can store the result in a destinations set. Example is given below:  redis  SADD key1  a \n(integer) 1\nredis  SADD key1  b \n(integer) 1\nredis  SADD key1  c \n(integer) 1\nredis  SADD key2  c \n(integer) 1\nredis  SADD key2  d \n(integer) 1\nredis  SADD key2  e \n(integer) 1\nredis  SINTERSTORE key key1 key2\n(integer) 1\nredis  SMEMBERS key\n1)  c   SUNION is used to return the elements of a set that results from the union of all the provided sets. SUNIONSTORE is the same but can store the result in a destinations set. Example is given below:   redis  SADD key1  a \n(integer) 1\nredis  SADD key1  b \n(integer) 1\nredis  SADD key1  c \n(integer) 1\nredis  SADD key2  c \n(integer) 1\nredis  SADD key2  d \n(integer) 1\nredis  SADD key2  e \n(integer) 1\nredis  SUNION key1 key2\n1)  a \n2)  b \n3)  c \n4)  d \n5)  e   Sorted sets provide similar operations which are explained below:  ZINTERSTORE is used to calculate the intersection of some keys in sorted sets (you need to provide the number of keys) and stores the result in a destination set. You can use two options for the resulted intersection either WEIGHTS or AGGREGATE. WEIGHTS option is used to specify a multiplication factor for each input in the sorted set which will be used to calculate the score in the resulted set. AGGREGATE option is used to specify how the results of the intersection is aggregated. Examples are SUM (default), MIN, or MAX which means that the resulting set will contain either the sum, minimum or maximum score across the input sets. Example is shown below:  redis  ZADD zset1 1  one \n(integer) 1\nredis  ZADD zset1 2  two \n(integer) 1\nredis  ZADD zset2 1  one \n(integer) 1\nredis  ZADD zset2 2  two \n(integer) 1\nredis  ZADD zset2 3  three \n(integer) 1\nredis  ZINTERSTORE out 2 zset1 zset2 WEIGHTS 2 3\n(integer) 2\nredis  ZRANGE out 0 -1 WITHSCORES\n1)  one \n2)  5 \n3)  two \n4)  10   ZUNIONSTORE is the same as ZINTERSTORE explained above but instead it calculate the union of multiple input sorted sets. It uses also the same options (WEIGHTS/AGGREGATE). Example is given below:  redis  ZADD zset1 1  one \n(integer) 1\nredis  ZADD zset1 2  two \n(integer) 1\nredis  ZADD zset2 1  one \n(integer) 1\nredis  ZADD zset2 2  two \n(integer) 1\nredis  ZADD zset2 3  three \n(integer) 1\nredis  ZUNIONSTORE out 2 zset1 zset2 WEIGHTS 2 3\n(integer) 3\nredis  ZRANGE out 0 -1 WITHSCORES\n1)  one \n2)  5 \n3)  three \n4)  9 \n5)  two \n6)  10", 
            "title": "Joins"
        }, 
        {
            "location": "/Redis/Examples/Analytics Service/", 
            "text": "Redis is suitable to get real-time analytics since it can answer queries very fast. In a B2C application, it is possible to use Redis to answer queries such as what are the most viewed products, how many users currently logged in, what are the last viewed products, and what are the most  added to cart or deleted from cart products. Of course, we can answer such queries using any other database. However, Redis is much faster since it is in-memory database and can delete the data used for the queries automatically after sometime when it is not needed. For example, if you want to store the most viewed 100 products, you can use the sorted set to store only the 100 most viewed products as will be shown in this section. So Redis is more efficient in terms of performance and storage when it comes to analytics assuming you want to answer simple direct questions.\n\n\nTo show how to use Redis to answer analytical queries, a service has been build that will cover the below APIs:\n\n\n\n\nAs seen above, the service will answer the following questions:\n\n\n// queries related to the products\nwhat are the most 10 viewed products for the current user ?\nwhat are the most 20 viewed products in general ?\nwhat are the most 10 recently viewed products by a user ?\nwhat are the most 20 recently viewed products in general ?\n\n// queries related to the cache performance\nwhat are the most 100 requested pages that was found in the cache ?\nwhat are the most 100 requested pages that was not found in the cache ?\n\n// queries related to the cart\nwhat are the most 20 products added to cart ?\nwhat are the most 20 products removed from cart ?\n\n// queries related to the log in\nhow many users are currently logged in ?\n\n\n\n\nFor the first set of queries related to the products, the below API will be called whenever a product is being viewed by a user.\n\n\n    @POST\n    @Path(\n/item_viewed/{sku}/token/{token}\n)\n    @Timed\n    @ApiOperation(value = \nrecord that item is veiwed\n, notes = \nReturns true or false\n, response = Boolean.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nSku or token aren't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public boolean recordItemViewed(\n            @ApiParam(value = \nsku of viewed item\n, required = true) @PathParam(\nsku\n) String sku,\n            @ApiParam(value = \nlogin token of the user\n, required = true) @PathParam(\ntoken\n) String token) {\n\n        if (sku == null || token == null) {\n            final String shortReason = \nsku or token aren't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            // set the recent products viewed by a user\n            long timestamp = System.currentTimeMillis() / 1000;\n            this.jedisClient.zadd(\nviewed:\n + token, timestamp, sku);\n            // just keep the last 10 recently viewed item for the user\n            this.jedisClient.zremrangeByRank(\nviewed:\n + token, 0, -11);\n            // keep the most viewed items by user\n            this.jedisClient.zincrby(\nMostViewed:\n + token, -1, sku);\n            // just keep the last 10 most viewed items by the user\n            this.jedisClient.zremrangeByRank(\nMostViewed:\n + token, 0, -11);\n            // keep the recently viewed items by all users\n            this.jedisClient.zadd(\nviewed:\n, timestamp, sku);\n            // just keep the last 20 recently viewed items by all users\n            this.jedisClient.zremrangeByRank(\nviewed:\n, 0, -21);\n            // keep the most viewed items by all users\n            this.jedisClient.zincrby(\nMostViewed:\n, -1, sku);\n            // just keep the last 20 most viewed items by all users\n            this.jedisClient.zremrangeByRank(\nMostViewed:\n + token, 0, -21);\n\n            return true;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\nAs can be seen above, we will use 4 sorted set to store the information we need to answer the queries. We store the information related to the most viewed items per user inside a sorted set with a key like \"MostViewed: token\". Similarly, we store the information for most viewed products in general using another sorted set with key like \"MostViewed\". In the same way, we store the recently viewed products inside two sorted sets with keys \"viewed:token\", and \"viewed\". \n\n\nWe will store the sku of the products that has been viewed with a score as a timestamp in case we want to answer the recently viewed queries. In case of the most viewed queries, we will store the sku of the products that has been viewed with a score as a number which represent how many times it has been viewed. The sorted sets will be as shown below:\n\n\n\n\nNote that we are using the zremrangeByRank command to set a limit for the size of the sorted set so that it will keep only the top elements and won't grow forever.\n\n\nAfter that we can answer the product related queries as shown below:\n\n\n\n\nwhat are the most 10 viewed products for the current user ?\n\n\n\n\n    @GET\n    @Path(\n/get10MostViewedItemsByUser/{token}\n)\n    @Timed\n    @ApiOperation(value = \nget the recent 10 most viewed items per user \n, notes = \nReturns array of items sku\n, response = String.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \ntoken wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public ArrayList\nString\n get10MostViewedItemsByUser(\n            @ApiParam(value = \ntoken of the user\n, required = true) @PathParam(\ntoken\n) String token) {\n\n        if (token == null) {\n            final String shortReason = \ntoken wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            ArrayList\nString\n listOfSku = new ArrayList\nString\n();\n\n            long sortedSetSize = this.jedisClient.zcard(\nMostViewed:\n + token);\n            Set\nString\n allRecentItems = this.jedisClient.zrange(\nMostViewed:\n\n                    + token, 0, sortedSetSize - 1);\n\n            String[] skuList = allRecentItems.toArray(new String[allRecentItems\n                    .size()]);\n            for (String sku : skuList) {\n                listOfSku.add(sku);\n            }\n\n            return listOfSku;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\nwhat are the most 20 viewed products in general ?\n\n\n\n\n    @GET\n    @Path(\n/get20MostViewedItems\n)\n    @Timed\n    @ApiOperation(value = \nget the recent 20  most viewed items \n, notes = \nReturns array of items sku\n, response = String.class, responseContainer = \nlist\n)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public ArrayList\nString\n get20MostViewedItems() {\n\n        try {\n\n            ArrayList\nString\n listOfSku = new ArrayList\nString\n();\n\n            long sortedSetSize = this.jedisClient.zcard(\nMostViewed:\n);\n            Set\nString\n allRecentItems = this.jedisClient.zrange(\nMostViewed:\n,\n                    0, sortedSetSize - 1);\n\n            String[] skuList = allRecentItems.toArray(new String[allRecentItems\n                    .size()]);\n            for (String sku : skuList) {\n                listOfSku.add(sku);\n            }\n\n            return listOfSku;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\n\nwhat are the most 10 recently viewed products by a user ?\n\n\n\n\n    @GET\n    @Path(\n/get10RecentViewedItemsByUser/{token}\n)\n    @Timed\n    @ApiOperation(value = \nget the recent 10 viewed items per user \n, notes = \nReturns array of items sku\n, response = String.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \ntoken wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public ArrayList\nString\n get10RecentViewedItemsByUser(\n            @ApiParam(value = \ntoken of the user\n, required = true) @PathParam(\ntoken\n) String token) {\n\n        if (token == null) {\n            final String shortReason = \ntoken wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            ArrayList\nString\n listOfSku = new ArrayList\nString\n();\n\n            long sortedSetSize = this.jedisClient.zcard(\nviewed:\n + token);\n            Set\nString\n allRecentItems = this.jedisClient.zrange(\nviewed:\n\n                    + token, 0, sortedSetSize - 1);\n\n            String[] skuList = allRecentItems.toArray(new String[allRecentItems\n                    .size()]);\n            for (String sku : skuList) {\n                listOfSku.add(sku);\n            }\n\n            return listOfSku;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\n\nwhat are the most 20 recently viewed products in general ?\n\n\n\n\n    @GET\n    @Path(\n/get20RecentViewedItems\n)\n    @Timed\n    @ApiOperation(value = \nget the recent 20  viewed items \n, notes = \nReturns array of items sku\n, response = String.class, responseContainer = \nlist\n)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public ArrayList\nString\n get20RecentViewedItems() {\n\n        try {\n\n            ArrayList\nString\n listOfSku = new ArrayList\nString\n();\n\n            long sortedSetSize = this.jedisClient.zcard(\nviewed:\n);\n            Set\nString\n allRecentItems = this.jedisClient.zrange(\nviewed:\n, 0,\n                    sortedSetSize - 1);\n\n            String[] skuList = allRecentItems.toArray(new String[allRecentItems\n                    .size()]);\n            for (String sku : skuList) {\n                listOfSku.add(sku);\n            }\n\n            return listOfSku;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nTo answer the queries related to the cache performance, we would call the below API whenever a page is requested from the cache with a flag indicating whether the page was found or not in the cache.\n\n\n    @POST\n    @Path(\n/page_requested/{url}/hit/{hit}\n)\n    @Timed\n    @ApiOperation(value = \nrecord that a page requested from cache and whether a cache miss or hit \n, notes = \nReturns true or false\n, response = Boolean.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nurl or hit wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public boolean recordRequestPageFromCache(\n            @ApiParam(value = \nurl of the requested page\n, required = true) @PathParam(\nurl\n) String url,\n            @ApiParam(value = \npage found in cache or not\n, required = true) @PathParam(\nhit\n) String hit) {\n\n        if (url == null || hit == null) {\n            final String shortReason = \nurl or hit wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            // keep most requested page with hit response from cache\n            if (hit.equals(\ntrue\n)) {\n                this.jedisClient.zincrby(\nMostHitPages:\n, -1, url);\n            } else {\n                this.jedisClient.zincrby(\nMostMissPages:\n, -1, url);\n            }\n\n            // keep the last 100 pages in both sets\n            this.jedisClient.zremrangeByRank(\nMostHitPages:\n, 0, -101);\n            this.jedisClient.zremrangeByRank(\nMostMissPages:\n, 0, -101);\n\n            return true;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nThen we will answer the queries as show below:\n\n\n\n\nwhat are the most 100 requested pages that was found in the cache ?\n\n\n\n\n    @GET\n    @Path(\n/get100MostHitPagedFromCache\n)\n    @Timed\n    @ApiOperation(value = \nget the 100 most hit pages from cache\n, notes = \nReturns list of pages urls\n, response = String.class, responseContainer = \nlist\n)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public ArrayList\nString\n get100MostHitPagedFromCache() {\n\n        try {\n\n            ArrayList\nString\n listOfUrl = new ArrayList\nString\n();\n\n            long sortedSetSize = this.jedisClient.zcard(\nMostHitPages:\n);\n            Set\nString\n allRecentItems = this.jedisClient.zrange(\n                    \nMostHitPages:\n, 0, sortedSetSize - 1);\n\n            String[] urlList = allRecentItems.toArray(new String[allRecentItems\n                    .size()]);\n            for (String url : urlList) {\n                listOfUrl.add(url);\n            }\n\n            return listOfUrl;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\nwhat are the most 100 requested pages that was not found in the cache ?\n\n\n\n\n    @GET\n    @Path(\n/get100MostMissedPagesFromCache\n)\n    @Timed\n    @ApiOperation(value = \nget the 100 most missed pages from cache\n, notes = \nReturns list of pages urls\n, response = String.class, responseContainer = \nlist\n)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public ArrayList\nString\n get100MostMissedPagesFromCache() {\n\n        try {\n\n            ArrayList\nString\n listOfUrl = new ArrayList\nString\n();\n\n            long sortedSetSize = this.jedisClient.zcard(\nMostMissPages:\n);\n            Set\nString\n allRecentItems = this.jedisClient.zrange(\n                    \nMostMissPages:\n, 0, sortedSetSize - 1);\n\n            String[] urlList = allRecentItems.toArray(new String[allRecentItems\n                    .size()]);\n            for (String url : urlList) {\n                listOfUrl.add(url);\n            }\n\n            return listOfUrl;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\nSimilarly to answer the queries related to the cart. We will need to call the below APIs whenever a product was added or deleted from the cart.\n\n\nA product added to a cart:\n\n\n    @POST\n    @Path(\n/added_to_cart/{sku}\n)\n    @Timed\n    @ApiOperation(value = \nrecord that item is added to cart\n, notes = \nReturns true or false\n, response = Boolean.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nSku wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public boolean recordItemAddedToCart(\n            @ApiParam(value = \nsku of the added to cart item\n, required = true) @PathParam(\nsku\n) String sku) {\n\n        if (sku == null) {\n            final String shortReason = \nSku wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            // keep the most products that are added to cart by all users\n            this.jedisClient.zincrby(\nAddedToCart:\n, -1, sku);\n            // just keep the 20 most added to cart products\n            this.jedisClient.zremrangeByRank(\nAddedToCart:\n, 0, -21);\n\n            return true;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nA product is deleted from cart:\n\n\n    @POST\n    @Path(\n/deleted_from_cart/{sku}\n)\n    @Timed\n    @ApiOperation(value = \nrecord that item is deleted from cart\n, notes = \nReturns true or false\n, response = Boolean.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nSku wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public boolean deletedItemAddedToCart(\n            @ApiParam(value = \nsku of the deleted from cart item\n, required = true) @PathParam(\nsku\n) String sku) {\n\n        if (sku == null) {\n            final String shortReason = \nSku wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            // keep the most products that are deleted from cart by all users\n            this.jedisClient.zincrby(\nDeletedFromCart:\n, -1, sku);\n            // just keep the 20 most deleted from cart products\n            this.jedisClient.zremrangeByRank(\nDeletedFromCart:\n, 0, -21);\n\n            return true;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\nThen we can answer the below queries easily as shown below:\n\n\n\n\nwhat are the most 20 products added to cart ?\n\n\n\n\n    @GET\n    @Path(\n/get20MostAddedToCartItems\n)\n    @Timed\n    @ApiOperation(value = \nget the 20  most added to cart items \n, notes = \nReturns array of items sku\n, response = String.class, responseContainer = \nlist\n)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public ArrayList\nString\n get20MostAddedToCartItems() {\n\n        try {\n\n            ArrayList\nString\n listOfSku = new ArrayList\nString\n();\n\n            long sortedSetSize = this.jedisClient.zcard(\nAddedToCart:\n);\n            Set\nString\n allRecentItems = this.jedisClient.zrange(\n                    \nAddedToCart:\n, 0, sortedSetSize - 1);\n\n            String[] skuList = allRecentItems.toArray(new String[allRecentItems\n                    .size()]);\n            for (String sku : skuList) {\n                listOfSku.add(sku);\n            }\n\n            return listOfSku;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\nwhat are the most 20 products removed from cart ?\n\n\n\n\n    @GET\n    @Path(\n/get20MostDeletedFromCartItems\n)\n    @Timed\n    @ApiOperation(value = \nget the 20  most deleted from cart items \n, notes = \nReturns array of items sku\n, response = String.class, responseContainer = \nlist\n)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public ArrayList\nString\n get20MostDeletedFromCartItems() {\n\n        try {\n\n            ArrayList\nString\n listOfSku = new ArrayList\nString\n();\n\n            long sortedSetSize = this.jedisClient.zcard(\nDeletedFromCart:\n);\n            Set\nString\n allRecentItems = this.jedisClient.zrange(\n                    \nDeletedFromCart:\n, 0, sortedSetSize - 1);\n\n            String[] skuList = allRecentItems.toArray(new String[allRecentItems\n                    .size()]);\n            for (String sku : skuList) {\n                listOfSku.add(sku);\n            }\n\n            return listOfSku;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nFinally to answer queries related to log in, the below API needs to be called whenever a user logs in :\n\n\n    @POST\n    @Path(\n/logged_user/{token}\n)\n    @Timed\n    @ApiOperation(value = \nrecord that a user has been logged in\n, notes = \nReturns true or false\n, response = Boolean.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \ntoken wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public boolean recordUserLoggedIn(\n            @ApiParam(value = \ntoken of the logged in user\n, required = true) @PathParam(\ntoken\n) String token) {\n\n        if (token == null) {\n            final String shortReason = \ntoken wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            // keep all logged in users during a day\n            long timestamp = System.currentTimeMillis() / 1000;\n            this.jedisClient.zincrby(\nlogin:\n, timestamp, token);\n\n            // we don't want this set to grow for ever , so we record the top\n            // 200000 recent logged in users (based on your website)\n            this.jedisClient.zremrangeByRank(\nlogin:\n, 0, -200001);\n\n            return true;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nThen we can easily answer the below query:\n\n\n\n\nhow many users are currently logged in since last one hour ?\n\n\n\n\n    @GET\n    @Path(\n/getCountLoggedInUserWithinLastHour\n)\n    @Timed\n    @ApiOperation(value = \nget the count of logged in users within last hour\n, notes = \nReturns integer count\n, response = Integer.class)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public long getCountLoggedInUserWithinLastHour() {\n\n        try {\n\n            long count = 0;\n\n            long currentTimeStamp = System.currentTimeMillis() / 1000;\n\n            long lastHourTimeStamp = currentTimeStamp - 3600;\n\n            count = this.jedisClient.zcount(\nlogin:\n, lastHourTimeStamp,\n                    currentTimeStamp);\n\n            return count;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }", 
            "title": "Analytics Service"
        }, 
        {
            "location": "/Redis/Examples/Bulk Transactions Support/", 
            "text": "In this example, a service was built to show how to execute a batch of commands at the same time. Assuming we want to add all existing logged in customers to a VIP set, then we can do that using a pipeline feature of Redis and execute multiple commands together as shown in the below API implementation:\n\n\n    @GET\n    @Path(\n/addToVIP\n)\n    @Timed\n    @ApiOperation(value = \nadd All current Logged In Customers to VIP \n, notes = \nReturns execution summery\n, response = String.class)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public String addToVIP() {\n\n        try {\n\n            long sortedSetSize = this.jedisClient.zcard(\nlogin:\n);\n            Set\nString\n allLoggedInUsers = this.jedisClient.zrange(\nlogin:\n, 0,\n                    sortedSetSize - 1);\n\n            String[] tokenList = allLoggedInUsers\n                    .toArray(new String[allLoggedInUsers.size()]);\n            Pipeline pipe = this.jedisClient.pipelined();\n            ArrayList\nResponse\nLong\n responses = new ArrayList\nResponse\nLong\n();\n            for (String token : tokenList) {\n                responses.add(pipe.sadd(\nVIP:\n, token));\n            }\n            pipe.sync();\n\n            return checkReponses(responses);\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nAs can be seen above, we start a pipeline, execute all the commands and then check the results at once. We check the results, using the below function:\n\n\nprivate String checkReponses(ArrayList\nResponse\nLong\n responses) {\n        int notExecutedCount = 0;\n        int executedCount = 0;\n        int totalCount = 0;\n        for (Response\nLong\n response : responses) {\n            totalCount++;\n            if (response.get() == 0) {\n                notExecutedCount++;\n            } else if (response.get() == 1) {\n                executedCount++;\n            } else {\n                LOGGER.info(\nresponse is\n + response.get());\n            }\n        }\n        LOGGER.info(\nout of:\n + totalCount\n                + \n commands, number of successfully executed commands are:\n\n                + executedCount + \n and number of errored out commands are:\n\n                + notExecutedCount);\n\n        return (\nout of:\n + totalCount\n                + \n commands, number of successfully executed commands are:\n\n                + executedCount + \n and number of errored out commands are:\n + notExecutedCount).toString();\n    }", 
            "title": "Bulk Transactions Support"
        }, 
        {
            "location": "/Redis/Examples/Cache Service/", 
            "text": "For many websites nowadays, large part of the pages are rarely changing. To improve the performance, a caching mechanism needs to be used when generating web pages in the server side. As a very fast in-memory database, Redis can be easily used to act like a cache service. There are many production-ready cache implementations available which were build using Redis. Redis can be configured as a cache and to support directly out-of-box popular cache eviction algorithms such as LRU and others. However in this example we are showing how to build the basic functionalities of a cache service using Redis. This service will support the below rest API:\n\n\n\n\nUsing this API you can check if the page has been already cached (a cache hit) and return the cached content. Otherwise in case of a cache miss, we cache the page content for the upcoming requests. The implementation of this API is shown below:\n\n\n    @GET\n    @Path(\n/{pageRequestURL}\n)\n    @Timed\n    @ApiOperation(value = \nget a cached request\n, notes = \nReturns a cached request by url\n, response = Request.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \npageRequestURL wasn't given!\n),\n            @ApiResponse(code = 500, message = \ncoudln't get from Redis !\n) })\n    public Request getRequest(\n            @ApiParam(value = \npageRequestURL\n, required = true) @PathParam(\npageRequestURL\n) String pageRequestURL) {\n\n        if (pageRequestURL == null) {\n            final String shortReason = \npageRequestURL wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n            // get the cached request page by url\n            Request resultRequest = new Request();\n            resultRequest.setPageRequestURL(pageRequestURL);\n\n            String cachedPageKey = \ncache:\n + pageRequestURL.hashCode();\n            // check if the request already cached\n            String chachedPageContent = this.jedisClient.get(cachedPageKey);\n\n            // if cache miss\n            if (chachedPageContent == null) {\n                String pageContent = getPageContent(pageRequestURL);\n                this.jedisClient.setex(cachedPageKey, this.cacheValidityTime,\n                        pageContent);\n                resultRequest.setPageContent(pageContent);\n            } else // cache hit\n            {\n                resultRequest.setPageContent(chachedPageContent);\n            }\n\n            return resultRequest;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ncoudln't get from Redis\n + e.getLocalizedMessage());\n            final String shortReason = \ncoudln't get from Redis !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nSo we store the page contents inside a Redis String data structure and then retrieve the content using the get command. In case the page wasn't cached previously we use the useful Reids command setex to store the page contents and set an expiration time at the same time.", 
            "title": "Cache Service"
        }, 
        {
            "location": "/Redis/Examples/Cart Management Service/", 
            "text": "In this example, I will show how to use Redis to store the shopping cart of a B2C application. Usually the shopping cart data is stored in the client side as a cookie. The advantage of doing this is that you wouldn't need to store such a temporary data in your database. However this will require you to send the cookies with every web request which can slow down the request in case of large cookies. Storing shopping cart data in Redis is a good idea since you can retrieve them very faster at any time and persist this data is possible if needed.\n\n\nTo show how you can use Redis to store shopping cart information, I have created a shopping cart management service which will support the below rest APIs:\n\n\n\n\nThe cart can be stored easily in Redis inside a Hash data structure. In the example I used, the cart has many cart items and each cart item has the below fields:\n\n\nprivate String sku;  // the sku of the product\nprivate Double amount; // the amount from this product\nprivate Double price; // the price of the product\n\n\n\n\nSo we store for each user session, a cart object inside a hash with a key like \"cart:{sessionID}\". Inside this hash, we store the cart-item's sku as a key and the value is the item details in JSON format.\n\n\n\n\n\n\nAdd cart item to cart:\n\n\n\n\nThe customer can add items to his cart by calling /cart/add/{session} and the below will be executed:\n\n\n    @POST\n    @Path(\n/add/{session}\n)\n    @Timed\n    @ApiOperation(value = \nadd new cart item\n, notes = \nReturns the added Cart Item\n, response = CartItem.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nCart item details or session wasn't given!\n),\n            @ApiResponse(code = 500, message = \ncoudln't add or delete from Redis !\n) })\n    public CartItem addCartItem(\n            @ApiParam(value = \nSession id\n, required = true) @PathParam(\nsession\n) String session,\n            @ApiParam(value = \nCart Item\n, required = true) CartItem cartItem) {\n\n        if (session == null || cartItem.getSku() == null\n                || cartItem.getPrice() == null || cartItem.getAmount() == null) {\n            final String shortReason = \nCart item details or session wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n            if (cartItem.getAmount() == 0) {\n                jedisClient.hdel(\ncart:\n + session, cartItem.getSku()\n                        .toString());\n            } else {\n                jedisClient.hset(\ncart:\n + session, cartItem.getSku()\n                        .toString(), cartItem.toJson());\n                jedisClient.expire(\ncart:\n + session, this.cartValidityTime);\n            }\n\n            return cartItem;\n        } catch (Exception e) {\n            LOGGER.log(\n                    Level.SEVERE,\n                    \ncoudln't add or delete from Redis\n\n                            + e.getLocalizedMessage());\n            final String shortReason = \ncoudln't add or delete from Redis !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nAs you can see it is quite simple, we just need to insert the cart item inside the user's cart using hset command. Before that we need to check if the amount of this item is zero, then we will update the cart by removing this item. Otherwise, we will add the item to the cart using hset command.  We also expire this cart after some cart validation time using the expire command so that we don't persist the cart information forever. In this example we expire the cart each 10 hours.\n\n\n\n\ndelete item from the cart\n\n\n\n\nDeleting a product from the user's cart is similar to adding the product into the cart but we are using hdel command instead as seen below:\n\n\n    @DELETE\n    @Path(\n/delete/{session}\n)\n    @Timed\n    @ApiOperation(value = \ndelete cart item from cart\n, notes = \nReturns the deleted Cart Item\n, response = CartItem.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nCart item details or session wasn't given!\n),\n            @ApiResponse(code = 500, message = \ncoudln't add or delete from Redis !\n) })\n    public CartItem deleteCartItem(\n            @ApiParam(value = \nSession id\n, required = true) @PathParam(\nsession\n) String session,\n            @ApiParam(value = \nCart Item\n, required = true) CartItem cartItem) {\n\n        if (session == null || cartItem.getSku() == null) {\n            final String shortReason = \nCart item details or session wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            jedisClient.hdel(\ncart:\n + session, cartItem.getSku().toString());\n\n            return cartItem;\n        } catch (Exception e) {\n            LOGGER.log(\n                    Level.SEVERE,\n                    \ncoudln't add or delete from Redis\n\n                            + e.getLocalizedMessage());\n            final String shortReason = \ncoudln't add or delete from Redis !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\nGet cart details\n\n\n\n\nTo get the cart details, we use the Redis command hgetAll which will return all the products inside the cart as seen below:\n\n\n    @GET\n    @Path(\n/{session}\n)\n    @Timed\n    @ApiOperation(value = \nget cart\n, notes = \nReturns the cart details\n, response = CartItem.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nsession wasn't given!\n),\n            @ApiResponse(code = 500, message = \ncoudln't get from Redis !\n) })\n    public ArrayList\nCartItem\n getCart(\n            @ApiParam(value = \nSession id\n, required = true) @PathParam(\nsession\n) String session) {\n\n        if (session == null) {\n            final String shortReason = \nsession wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n        ArrayList\nCartItem\n result = new ArrayList\nCartItem\n();\n\n        try {\n\n            Map\nString, String\n restul = jedisClient.hgetAll(\ncart:\n + session);\n            Iterator it = restul.entrySet().iterator();\n            while (it.hasNext()) {\n                CartItem cart = new CartItem();\n                Map.Entry pair = (Map.Entry) it.next();\n                JsonObject jObject = new JsonParser().parse(\n                        pair.getValue().toString()).getAsJsonObject();\n                cart.setSku(jObject.get(\nsku\n).toString());\n                cart.setAmount(jObject.get(\namount\n).getAsDouble());\n                cart.setPrice(jObject.get(\nprice\n).getAsDouble());\n                result.add(cart);\n                it.remove(); // avoids a ConcurrentModificationException\n            }\n\n            return result;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ncoudln't get from Redis\n + e.getLocalizedMessage());\n            final String shortReason = \ncoudln't get from Redis !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\n\nDelete the whole cart\n\n\n\n\nTo delete the cart, we need to delete the whole cart hash by using the expire command where time is zero as shown below:\n\n\n    @DELETE\n    @Path(\n/{session}\n)\n    @Timed\n    @ApiOperation(value = \ndelete cart\n, notes = \nReturns true or false\n, response = Boolean.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nsession wasn't given!\n),\n            @ApiResponse(code = 500, message = \ncoudln't delete from Redis !\n) })\n    public boolean deleteCart(\n            @ApiParam(value = \nSession id\n, required = true) @PathParam(\nsession\n) String session) {\n\n        if (session == null) {\n            final String shortReason = \nsession wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            this.jedisClient.expire(\ncart:\n + session, 0);\n\n            return true;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ncoudln't delete from Redis\n + e.getLocalizedMessage());\n            final String shortReason = \ncoudln't delete from Redis !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }", 
            "title": "Cart Management Service"
        }, 
        {
            "location": "/Redis/Examples/Handling Relational Data/", 
            "text": "In this example, I will show to model data relationships using Redis assuming the below model:\n\n\n\n\nAs you can be seen above, we have four data entities (customer, order, payment and shipping). Each customer can have more than one order and each order can have one payment method and multiple shipping addresses. To model these relationships, we use sets as explained already in the \nrelational data section\n. \n\n\nNow in this service, it will answer the below APIs :\n\n\n\n\nSo first the below APIs will be called whenever an order or a customer is added:\n\n\n\n\nAdd customer\n\n\n\n\n    @POST\n    @Path(\n/addCustomer\n)\n    @Timed\n    @ApiOperation(value = \nadd a user\n, notes = \nReturns the added customer\n, response = Customer.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 500, message = \nCoudln't access Redis \n),\n            @ApiResponse(code = 400, message = \ncustomerID or details wasn't given! \n) })\n    public Customer addUser(\n            @ApiParam(value = \ncustomer\n, required = true) Customer customer) {\n\n        if (customer == null || customer.getCustomerID() == null\n                || customer.getOtherCustomerDetails() == null) {\n            final String shortReason = \ncustomerID or details wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n            this.jedisClient.hset(\ncustomers:\n + customer.getCustomerID(),\n                    \ncustomerOtherDetails\n, customer.getOtherCustomerDetails());\n\n            return customer;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis \n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis \n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }  \n\n\n\n\n\n\nSubmit an order\n\n\n\n\n    @POST\n    @Path(\n/submitOrder/{customerID}\n)\n    @Timed\n    @ApiOperation(value = \nsubmit an order by customer \n, notes = \nReturns the submitted order \n, response = Order.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 500, message = \nCoudln't access Redis \n),\n            @ApiResponse(code = 400, message = \norder,shipping,payment or customerID weren't given! \n) })\n    public Order submitOrder(\n            @ApiParam(value = \ncutomerID\n, required = true) @PathParam(\ncustomerID\n) String customerID,\n            @ApiParam(value = \norder to be submitted\n, required = true) OrderRequest orderInput) {\n\n        Order order = orderInput.getOrder();\n        Shipping shipping = orderInput.getShipping();\n        Payment payment = orderInput.getPayment();\n\n        if (customerID == null || order == null || order.getOrderID() == null\n                || order.getOtherOrderDetails() == null || payment == null\n                || payment.getPaymentID() == null\n                || payment.getOtherPaymentDetails() == null || shipping == null\n                || shipping.getShippingID() == null\n                || shipping.getOtherShippingDetails() == null) {\n            final String shortReason = \norder,shipping,payment or customerID weren't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n            // set the payment details in a hash\n            this.jedisClient.hset(\npayments:\n + payment.getPaymentID(),\n                    \npaymentOtherDetails\n, payment.getOtherPaymentDetails());\n            // set the shipping details in a hash\n            this.jedisClient.hset(\nshippings:\n + shipping.getShippingID(),\n                    \nshippingOtherDetails\n, shipping.getOtherShippingDetails());\n\n            // set the order details in a hash including the one to one\n            // relationship with customer and payment\n            HashMap\nString, String\n orderDetails = new HashMap\nString, String\n();\n            orderDetails.put(\norderOtherDetails\n, order.getOtherOrderDetails());\n            orderDetails.put(\ncustomerID\n, customerID);\n            orderDetails.put(\npaymentID\n, payment.getPaymentID());\n            this.jedisClient\n                    .hmset(\norders:\n + order.getOrderID(), orderDetails);\n\n            // set the many to one relationship between orders and user\n            this.jedisClient.sadd(customerID + \n:orders\n, order.getOrderID());\n\n            // set the many to one relationship between orders and payment\n            this.jedisClient.sadd(payment.getPaymentID() + \n:orders\n,\n                    order.getOrderID());\n\n            // set the many to many relationship between orders and shippings\n            this.jedisClient.sadd(shipping.getShippingID() + \n:orders\n,\n                    order.getOrderID());\n            this.jedisClient.sadd(order.getOrderID() + \n:shippings\n,\n                    shipping.getShippingID());\n\n            return order;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis \n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis \n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nNote that we will set the values of the order, payment, and shipping entities whenever an order is submitted. We also need to set the details of the relationships entities with keys (customerID:orders, paymentID:orders, shippingID:orders, and orderID:shippings) which we store them inside a set data structure.\n\n\nNow we can run the below queries:\n\n\n\n\nget all customer orders\n\n\n\n\n    @GET\n    @Path(\n/getShippingsPerCustomer/{customerID}\n)\n    @Timed\n    @ApiOperation(value = \nget all customer's shippings\n, notes = \nReturns list of all customer shippings\n, response = Shipping.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {\n            @ApiResponse(code = 500, message = \nCoudln't access Redis \n),\n            @ApiResponse(code = 400, message = \ncustomerID wasn't given! \n) })\n    public ArrayList\nShipping\n getShippingsPerCustomer(\n            @ApiParam(value = \ncutomerID\n, required = true) @PathParam(\ncustomerID\n) String customerID) {\n\n        if (customerID == null) {\n            final String shortReason = \ncustomerID wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            ArrayList\nShipping\n allCustomerShippings = new ArrayList\nShipping\n();\n\n            // get all customer orders\n            Set\nString\n setOfCustomerOrders = this.jedisClient\n                    .smembers(customerID + \n:orders\n);\n\n            String[] orderIDList = setOfCustomerOrders\n                    .toArray(new String[setOfCustomerOrders.size()]);\n            for (String orderID : orderIDList) {\n                Set\nString\n setOfAllShippings = this.jedisClient\n                        .smembers(orderID + \n:shippings\n);\n                String[] shippingIDsList = setOfAllShippings\n                        .toArray(new String[setOfAllShippings.size()]);\n                for (String shippingID : shippingIDsList) {\n                    Shipping shipping = new Shipping();\n                    shipping.setShippingID(shippingID);\n                    shipping.setOtherShippingDetails(this.jedisClient.hget(\n                            \nshippings:\n + shippingID, \nshippingOtherDetails\n));\n                    allCustomerShippings.add(shipping);\n                }\n            }\n\n            return allCustomerShippings;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis \n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis \n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\nAll the below queries will be answered using this API:\n\n\n\n\n// queries\n\nget the orders that are having this paymentID but not this shippingID (difference) ?\nget the orders that are having this shippingID or that are having this paymentID (union) ?\nget the the orders that are having this paymentID and shippingID at the same time (join) ?\n\nAPI:\n\n\n    @GET\n    @Path(\n/getOrdersQuery/{queryType}/payment/{paymentID}/shipping/{shippingID}\n)\n    @Timed\n    @ApiOperation(value = \nget orders difference, Union, or Join shipping and payment\n, notes = \nReturns list of all orders\n, response = Order.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {\n            @ApiResponse(code = 500, message = \nCoudln't access Redis \n),\n            @ApiResponse(code = 400, message = \nqueryType or shippingID or shippingID wasn't given! \n),\n            @ApiResponse(code = 400, message = \nqueryType isn't supported ! \n) })\n    public ArrayList\nOrder\n getOrdersQuery(\n            @ApiParam(value = \nhow to query, either Difference,Union,Join\n, required = true) @PathParam(\nqueryType\n) String queryType,\n            @ApiParam(value = \npaymentID\n, required = true) @PathParam(\npaymentID\n) String paymentID,\n            @ApiParam(value = \nshippingID\n, required = true) @PathParam(\nshippingID\n) String shippingID) {\n\n        if (queryType == null || paymentID == null || shippingID == null) {\n            final String shortReason = \nqueryType or shippingID or shippingID wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            ArrayList\nOrder\n resultedOrders = new ArrayList\nOrder\n();\n            Set\nString\n resultedSet;\n\n            if (queryType.equals(\nDifference\n)) {\n\n                // get the orders that are having this paymentID but not the\n                // shippingID (not in, difference)\n                resultedSet = this.jedisClient.sdiff(paymentID + \n:orders\n,\n                        shippingID + \n:orders\n);\n\n            } else if (queryType.equals(\nUnion\n)) {\n\n                // get the orders that are having this shippingID or that are\n                // having this paymentID (union)\n                resultedSet = this.jedisClient.sunion(paymentID + \n:orders\n,\n                        shippingID + \n:orders\n);\n\n            } else if (queryType.equals(\nJoin\n)) {\n\n                // get the the orders that are only having this paymentID and\n                // shippingID at the same time (join)\n                resultedSet = this.jedisClient.sinter(paymentID + \n:orders\n,\n                        shippingID + \n:orders\n);\n            } else {\n                final String shortReason = \nqueryType isn't supported !\n;\n                Exception cause = new IllegalArgumentException(shortReason);\n                throw new WebApplicationException(cause,\n                        javax.ws.rs.core.Response.Status.BAD_REQUEST);\n\n            }\n\n            String[] orderIDList = resultedSet.toArray(new String[resultedSet\n                    .size()]);\n            for (String orderID : orderIDList) {\n                Order order = new Order();\n                order.setOrderID(orderID);\n                order.setOtherOrderDetails(this.jedisClient.hget(\norders:\n\n                        + orderID, \norderOtherDetails\n));\n                resultedOrders.add(order);\n            }\n\n            return resultedOrders;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis \n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis \n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }", 
            "title": "Handling Relational Data"
        }, 
        {
            "location": "/Redis/Examples/Job Queue System/", 
            "text": "In the example, I will show how to build a queue system functionalities using Redis. Redis supports the list data structure which is very suitable to build queue system. For this a service has been built to support the below APIs:\n\n\n\n\nTo add a job to queue, the below API can be called:\n\n\n    @POST\n    @Path(\n/enqueue\n)\n    @Timed\n    @ApiOperation(value = \nenqueue a job \n, notes = \nReturns the enqueued job\n, response = Job.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \njob wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public Job enqueue(\n            @ApiParam(value = \njob to be queued\n, required = true) Job job) {\n\n        if (job == null || job.getQueueName() == null || job.getData() == null) {\n            final String shortReason = \njob wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n            // enqueue the job inside the queue list\n            this.jedisClient\n                    .rpush(\nqueue:\n + job.getQueueName(), job.getData());\n            // add the queue name to a set\n            this.jedisClient.sadd(\nqueues:\n, job.getQueueName());\n\n            return job;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nThe job is inserted inside the list using the rpush command. A set is used to store the used queue names.\n\n\nTo remove an element from the queue, the queue type needs to be specified. If the queue type is \"FIFO\", then the job will be removed from the end of the queue. However if the queue type is \"LIFO\", the job will be removed from the head of the queue as shown below:\n\n\n    @GET\n    @Path(\n/dequeue/{queueName}/type/{queueType}\n)\n    @Timed\n    @ApiOperation(value = \ndequeue a job \n, notes = \nReturns the dequeued job\n, response = Job.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nqueue Name or type wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n),\n            @ApiResponse(code = 400, message = \nqueue type not supported !\n) })\n    public Job dequeue(\n            @ApiParam(value = \nqueue Name to be dequeued\n, required = true) @PathParam(\nqueueName\n) String queueName,\n            @ApiParam(value = \nqueue type,FIFO or LIFO\n, required = true) @PathParam(\nqueueType\n) String queueType) {\n\n        if (queueName == null || queueType == null) {\n            final String shortReason = \nqueue Name or type wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            Job result = new Job();\n            result.setQueueName(queueName);\n            if (queueType.equals(\nFIFO\n)) {\n                // dequeue the job from the queue list\n                result.setData(this.jedisClient.lpop(\nqueue:\n + queueName));\n            } else if (queueType.equals(\nLIFO\n)) {\n                // dequeue the job from the queue list\n                result.setData(this.jedisClient.rpop(\nqueue:\n + queueName));\n            } else {\n                final String shortReason = \nqueue type not supported !\n;\n                Exception cause = new IllegalArgumentException(shortReason);\n                throw new WebApplicationException(cause,\n                        javax.ws.rs.core.Response.Status.BAD_REQUEST);\n            }\n\n            // check if the list is empty then remove it from queues set\n            if (this.jedisClient.llen(\nqueue:\n + queueName) == 0) {\n                this.jedisClient.srem(\nqueues:\n, queueName);\n            }\n\n            return result;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nIt is also possible to peak into the queue using the below API:\n\n\n    @GET\n    @Path(\n/peak/{queueName}/number/{elelementsNumber}\n)\n    @Timed\n    @ApiOperation(value = \npeak elements from head \n, notes = \nReturns some elements from head\n, response = String.class, responseContainer = \nlist\n)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nqueue name or elelemnts number wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public List\nString\n peakHead(\n            @ApiParam(value = \nqueue Name\n, required = true) @PathParam(\nqueueName\n) String queueName,\n            @ApiParam(value = \nelelments number to return\n, required = true) @PathParam(\nelelementsNumber\n) int elelementsNumber) {\n\n        if (queueName == null || elelementsNumber == 0) {\n            final String shortReason = \nqueue name or elelemnts number wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            List\nString\n results = this.jedisClient.lrange(\n                    \nqueue:\n + queueName, 0, elelementsNumber);\n\n            return results;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nTo remove a particular job from that can be in the middle of the queue, the lrem command can be used which will search for the job and delete it if it exists as shown below:\n\n\n    @DELETE\n    @Path(\n/DeleteJob\n)\n    @Timed\n    @ApiOperation(value = \ndelete element from a queue \n, notes = \nReturns the deleted element\n, response = Job.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \njob wasn't given!\n),\n            @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public Job deleteElement(\n            @ApiParam(value = \njob to be deleted\n, required = true) Job job) {\n\n        if (job == null || job.getQueueName() == null || job.getData() == null) {\n            final String shortReason = \njob wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n\n            this.jedisClient.lrem(\nqueue:\n + job.getQueueName(), 0,\n                    job.getData());\n\n            // check if the list is empty then remove it from queues set\n            if (this.jedisClient.llen(\nqueue:\n + job.getQueueName()) == 0) {\n                this.jedisClient.srem(\nqueues:\n, job.getQueueName());\n            }\n\n            return job;\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nFinally, to get the queue names we can use the set that we used previously to store the queue names as shown below:\n\n\n    @GET\n    @Path(\n/queues\n)\n    @Timed\n    @ApiOperation(value = \nget all queue names\n, notes = \nReturns the name of all queues\n, response = String.class, responseContainer = \nset\n)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \nCoudln't access Redis\n) })\n    public Set\nString\n allQueues() {\n\n        try {\n            Set\nString\n results = this.jedisClient.smembers(\nqueues:\n);\n\n            return results;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis\n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }", 
            "title": "Job Queue System"
        }, 
        {
            "location": "/Redis/Examples/Session Management Service/", 
            "text": "As we have already mentioned in the cart management example, using cookies is a good idea for temporary data such as login information and carts. However, the cookies will be sent back and forth between the server and the client with each request. This is usually fine since cookies are most of the time very small but when cookies getting larger, it might slow down the requests. Since Redis is a very fast in-memory database, we can use it to store login cookies or to handle login sessions. In this example I will be showing how to use Redis to do just that.\n\n\nTo show how to use Redis to manage login sessions, I have created a session management service with the following rest API support:\n\n\n\n\nStoring a session in Redis can be achieved by using a simple String data structure where the key is a login token and the value is the login details. \n\n\n\n\nadd new session\n\n\n\n\nAdding a new session is done using the below:\n\n\n    @POST\n    @Path(\n/\n)\n    @Timed\n    @ApiOperation(value = \nadd new session\n, notes = \nReturns the added session\n, response = Session.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \nSession details wasn't given!\n),\n            @ApiResponse(code = 500, message = \ncoudln't add from Redis !\n) })\n    public Session addSession(\n            @ApiParam(value = \nSession details\n, required = true) Session session) {\n\n        if (session == null || session.getToken() == null\n                || session.getData() == null) {\n            final String shortReason = \nSession details wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n            long timestamp = System.currentTimeMillis() / 1000;\n            // add the session\n            this.jedisClient.set(\nlogin:\n + session.getToken(),\n                    session.getData());\n            // add it to the recent sorted set with timestamp as score\n            this.jedisClient.zadd(\nrecent:\n, timestamp, session.getToken());\n            // expire session each 10 hours\n            this.jedisClient.expire(\nlogin:\n + session.getToken(),\n                    this.sessionValidityTime);\n            // keep only the top 100 recent session\n            this.jedisClient.zremrangeByRank(\nrecent:\n, 0, -101);\n            return session;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ncoudln't add from Redis\n + e.getLocalizedMessage());\n            final String shortReason = \ncoudln't add from Redis !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nSo we store the login session as a string using the set command. We also store the recent login sessions inside a sorted set along with the timestamp so that we can later retrieve the recent sessions sorted by time. Adding an element to a sorted set is achieved by using the zadd command and we use the zremrangeByRank command to keep just the last 100 elements sorted by timestamp and remove the rest which will prevent the set from growing beyond 100 elements. Finally we use expire command to delete the session after particular time (in the example we are using 10 hours).\n\n\n\n\nGet session details \n\n\n\n\nTo retrieve the session details we use the get Redis command as seen below:\n\n\n    @GET\n    @Path(\n/{token}\n)\n    @Timed\n    @ApiOperation(value = \nget a session\n, notes = \nReturns a session by token\n, response = Session.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \ntoken wasn't given!\n),\n            @ApiResponse(code = 500, message = \ncoudln't get from Redis !\n) })\n    public Session getSession(\n            @ApiParam(value = \ntoken\n, required = true) @PathParam(\ntoken\n) String token) {\n\n        if (token == null) {\n            final String shortReason = \ntoken wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n            // get the session by token\n            Session result = new Session();\n            result.setData(this.jedisClient.get(\nlogin:\n + token));\n            result.setToken(token);\n\n            return result;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ncoudln't get from Redis\n + e.getLocalizedMessage());\n            final String shortReason = \ncoudln't get from Redis !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\nDelete the session\n\n\n\n\nWe delete the session using the del Redis command. To delete this session from the recent sorted set we can use zrem and providing the session token as seen below:\n\n\n    @DELETE\n    @Path(\n/{token}\n)\n    @Timed\n    @ApiOperation(value = \ndelete a session\n, notes = \nReturns true or false\n, response = Boolean.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 400, message = \ntoken wasn't given!\n),\n            @ApiResponse(code = 500, message = \ncoudln't delete from Redis !\n) })\n    public Boolean deleteSession(\n            @ApiParam(value = \ntoken\n, required = true) @PathParam(\ntoken\n) String token) {\n\n        if (token == null) {\n            final String shortReason = \ntoken wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n            // delete the session\n            this.jedisClient.del(\nlogin:\n + token);\n            this.jedisClient.zrem(\nrecent:\n, token);\n            return true;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ncoudln't get from Redis\n + e.getLocalizedMessage());\n            final String shortReason = \ncoudln't get from Redis !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\n\nGet the last 100 recent login sessions\n\n\n\n\nTo retrieve the 100 recent login sessions, we can use the recent sorted set that we have been maintaining so far, code is shown below:\n\n\n    @GET\n    @Path(\n/recent100\n)\n    @Timed\n    @ApiOperation(value = \nget recent 100 sessions\n, notes = \nReturns list of sessions\n, response = Session.class, responseContainer = \nlist\n)\n    @ApiResponses(value = { @ApiResponse(code = 500, message = \ncoudln't get from Redis !\n) })\n    public ArrayList\nSession\n getRecent100Sessions() {\n\n        try {\n            ArrayList\nSession\n result = new ArrayList\nSession\n();\n\n            // get all recent tokens from the recent sorted set\n            long sortedSetSize = this.jedisClient.zcard(\nrecent:\n);\n            Set\nString\n allRecentSessions = this.jedisClient.zrange(\nrecent:\n,\n                    0, sortedSetSize - 1);\n\n            String[] sessionTokens = allRecentSessions\n                    .toArray(new String[allRecentSessions.size()]);\n            for (String token : sessionTokens) {\n                Session session = new Session();\n                session.setToken(token);\n                session.setData(this.jedisClient.get(\nlogin:\n + token));\n                result.add(session);\n            }\n            return result;\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \ncoudln't get from Redis\n + e.getLocalizedMessage());\n            final String shortReason = \ncoudln't get from Redis !\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\nAs can seen above, we used zcard command to get the number of elements inside the set or the set size, then we can retrieve all the elements in the set using zrange command with min as zero and max as the size for the set.", 
            "title": "Session Management Service"
        }, 
        {
            "location": "/Redis/Examples/Transaction Support/", 
            "text": "In this example, we show how we can execute command in a transaction using the MULTI/EXEC/WATCH/UNWATCH commands. In this example, we try to execute a transaction where we take a product from the inventory of the seller and put in the bought list of the customer. The code is shown below:\n\n\n\n    @GET\n    @Path(\n/buy/{sku}/seller/{sellerID}/token/{token}\n)\n    @Timed\n    @ApiOperation(value = \nbuy a product from seller \n, notes = \nReturns execution summery\n, response = String.class)\n    @ApiResponses(value = {\n            @ApiResponse(code = 500, message = \nCoudln't access Redis \n),\n            @ApiResponse(code = 400, message = \nsku or sellerID wasn't given! \n),\n            @ApiResponse(code = 400, message = \nseller inventory is empty \n) })\n    public String buyProduct(\n            @ApiParam(value = \nproduct sku\n, required = true) @PathParam(\nsku\n) String sku,\n            @ApiParam(value = \nseller id\n, required = true) @PathParam(\nsellerID\n) String sellerID,\n            @ApiParam(value = \ntoken\n, required = true) @PathParam(\ntoken\n) String token) {\n\n        if (sku == null || sellerID == null) {\n            final String shortReason = \nsku or sellerID wasn't given!\n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.BAD_REQUEST);\n        }\n\n        try {\n            ArrayList\nResponse\nLong\n responses = new ArrayList\nResponse\nLong\n();\n\n            this.jedisClient.watch(\ninventory:\n + sellerID);\n            if (!this.jedisClient.sismember(\ninventory:\n + sellerID, sku)) {\n                this.jedisClient.unwatch();\n\n                final String shortReason = \nseller inventory is empty\n;\n                Exception cause = new IllegalArgumentException(shortReason);\n                throw new WebApplicationException(cause,\n                        javax.ws.rs.core.Response.Status.BAD_REQUEST);\n\n            }\n\n            Transaction transaction = this.jedisClient.multi();\n\n            responses.add(transaction.srem(\ninventory:\n + sellerID, sku));\n            responses.add(transaction.sadd(\nbought:\n + token, sku));\n\n            transaction.exec();\n\n            return checkReponses(responses);\n\n        } catch (Exception e) {\n            LOGGER.log(Level.SEVERE,\n                    \nCoudln't access Redis \n + e.getLocalizedMessage());\n            final String shortReason = \nCoudln't access Redis \n;\n            Exception cause = new IllegalArgumentException(shortReason);\n            throw new WebApplicationException(cause,\n                    javax.ws.rs.core.Response.Status.INTERNAL_SERVER_ERROR);\n        }\n    }\n\n\n\n\n\nAs seen above, we first watch the inventory of the seller to ensure the consistency of the data by making sure that no one is changing the inventory at the same time. Then we start a transaction using the multi command and adding command to the transaction (removing a product from the seller inventory and adding it to the bought list of the customer). Finally, we execute the transaction using the exec command. Later we can check the result of executing this transaction using the checkReponses() function.", 
            "title": "Transaction Support"
        }, 
        {
            "location": "/Redis/Query Model/Aggregation and Filtering/", 
            "text": "Aggregation\n\n\nIn general, aggregation isn't internally supported by Redis but there is a workaround to group data in Redis using sets and sorted sets. In this section I will show how you can group your data in Redis.\n\n\nTo group your data in Redis, the set data structure is the best candidate since you will be able to easily insert elements from the same group type as members of the set. An example is if you want to group all products that were produced by a certain manufacturer. \n\n\nthis.jedisClient.sadd(\nProduct.Manufacture.Index:\n + map.get(\nManufacture\n),\n                map.get(\nProductID\n));\n\n\n\n\nIn the above example, we created a set for each manufacturer and then inserted all product Ids of the products produced by this manufacturer. Then to get all the products produced by this manufacturer, we can run a query like below:\n\n\nthis.jedisClient.smembers(\nProduct.Manufacture.Index:\n + manufacturer) ;\n\n\n\n\nFiltering\n\n\nTo filter data in Redis, you can use the intersection, union, and difference functionalities provided by the set and sorted sets. For example if you want to get all products that are produced by a certain manufacturer and having the red colour, then you can intersect the \"Product.Manufacture.Index:manufacturer\" and \"Product.Color.Index:colour\" sets as shown below:\n\n\nthis.jedisClient.sinter(\"Product.Manufacture.Index:Boss\" , \"Product.Color.Index:Red\");\n\n\nIn the same way, you get all products from both sets using the union command as shown below:\n\n\nthis.jedisClient.sunion(\"Product.Manufacture.Index:Boss\" , \"Product.Color.Index:Red\");\n\n\nAnd to get the products that are in the first set but not in the second set, you can use the difference command as shown below:\n\n\nthis.jedisClient.sdiff(\nProduct.Manufacture.Index:Boss\n , \nProduct.Color.Index:Red\n);", 
            "title": "Aggregation and Filtering"
        }, 
        {
            "location": "/Redis/Query Model/Aggregation and Filtering/#aggregation", 
            "text": "In general, aggregation isn't internally supported by Redis but there is a workaround to group data in Redis using sets and sorted sets. In this section I will show how you can group your data in Redis.  To group your data in Redis, the set data structure is the best candidate since you will be able to easily insert elements from the same group type as members of the set. An example is if you want to group all products that were produced by a certain manufacturer.   this.jedisClient.sadd( Product.Manufacture.Index:  + map.get( Manufacture ),\n                map.get( ProductID ));  In the above example, we created a set for each manufacturer and then inserted all product Ids of the products produced by this manufacturer. Then to get all the products produced by this manufacturer, we can run a query like below:  this.jedisClient.smembers( Product.Manufacture.Index:  + manufacturer) ;", 
            "title": "Aggregation"
        }, 
        {
            "location": "/Redis/Query Model/Aggregation and Filtering/#filtering", 
            "text": "To filter data in Redis, you can use the intersection, union, and difference functionalities provided by the set and sorted sets. For example if you want to get all products that are produced by a certain manufacturer and having the red colour, then you can intersect the \"Product.Manufacture.Index:manufacturer\" and \"Product.Color.Index:colour\" sets as shown below:  this.jedisClient.sinter(\"Product.Manufacture.Index:Boss\" , \"Product.Color.Index:Red\");  In the same way, you get all products from both sets using the union command as shown below:  this.jedisClient.sunion(\"Product.Manufacture.Index:Boss\" , \"Product.Color.Index:Red\");  And to get the products that are in the first set but not in the second set, you can use the difference command as shown below:  this.jedisClient.sdiff( Product.Manufacture.Index:Boss  ,  Product.Color.Index:Red );", 
            "title": "Filtering"
        }, 
        {
            "location": "/Redis/Query Model/Full Text Search Support/", 
            "text": "Redis doesn't support full-text search internally, but some search engines can be used with Redis such as elastic search and Apache Solr.", 
            "title": "Full Text Search Support"
        }, 
        {
            "location": "/Redis/Query Model/Indexing/", 
            "text": "Since Redis is a key-value store, the key acts as the primary index in all data structures. Redis doesn't support internally other type of indexes such secondary and compound indexes. However, secondary and compound indexes can be implemented and maintained manually by the clients using sets and sorted sets.  In the below section I will show how to use primary, secondary and compound indexes in Redis.\n\n\nAlthough creating secondary indexes and compound indexes is still possible somehow using sets and sorted sets, maintaining theses indexes manually at each write operation can be difficult and is prone to human error. \n\n\nPrimary Index\n\n\nThe key used to access any value in Redis acts as the primary index. An example is shown below:\n\n\nProduct object stored in a hash :\n\n\nHashMap\nString, String\n map = new HashMap\nString, String\n();\n        map.put(\nProductID\n, \nThisIsProductID\n);\n        map.put(\nSKU\n, \nThisIsSKU\n);\n        map.put(\nName\n, \nshirt\n);\n        map.put(\nColor\n, \nRed\n);\n        map.put(\nPrice\n, \n20\n);\n        this.jedisClient.hmset(\nProducts:\n + map.get(\nProductID\n), map);\n\n// get the product object using the key (primary index)\nthis.jedisClient.hgetAll(\nProducts:\n + productID);\n\n\n\n\nIn the above example we have used the productID as the primary key for the product hash object.\n\n\nSecondary Index\n\n\nIf we want to access the product object shown in the previous example using other fields instead of the primary key such as using the product colour, we can create a set for each colour that contains all product Ids having this colour as shown in the example below:\n\n\nthis.jedisClient.sadd(\nProduct.Colour.Index:\n + map.get(\nColour\n),\n                map.get(\nProductID\n));\n\n\n\n\nThen we can use this secondary index as shown below:\n\n\nthis.jedisClient.smembers(\nProduct.Colour.Index:\n + colour) ; \n// Then hgetAll for each productID\n\n\n\n\nYou can also use a sorted sets to create secondary indexes in case you want to index numeric values such as the product price. An example is shown below:\n\n\nthis.jedisClient.zadd(\nProduct.Price.Index\n,\n                Double.valueOf(map.get(\nPrice\n)), map.get(\nProductID\n));\n\n\n\n\nIn the above example, we use sorted set as a secondary index to be able to query easily for the product price. Each product id is mapped to price inside the sorted set which allows you later to do range queries like the below:\n\n\nthis.jedisClient.zrangeByScore(\nProduct.Price.Index\n, min, max); \n\n\n\n\nIn the above query, we are returning all the products with price between a min and max values.\n\n\nCompound Index\n\n\nYou can use sorted sets to create a compound index. To do that, you need to set the score of the sorted set all to zero which makes Redis sort the set lexicographically which then can compare two field at the same time. An example is shown below:\n\n\n    this.jedisClient.zadd(\n                \nProduct.Category.Price.Index\n,\n                0,\n                map.get(\nCategoryID\n) + \n:\n + map.get(\nPrice\n) + \n:\n\n                        + map.get(\nProductID\n));\n\n\n\n\nIn the above example, we are inserting the product id and the category id inside a sorted set and setting the score to zero. The order in the sorted set will be now a lexicographically order which means Redis is going to order the string having \"categoryID:price:productID\" byte by byte. So if you want to return all product Ids in the category id 2 between price 20 and 30, you can run the below query:\n\n\n// get price between 20-30 for category 2\nthis.jedisClient.zrangeByLex(\nProduct.Catgeory.Price.Index\n, \n[2:20\n, \n[2:30\n);", 
            "title": "Indexing"
        }, 
        {
            "location": "/Redis/Query Model/Indexing/#primary-index", 
            "text": "The key used to access any value in Redis acts as the primary index. An example is shown below:  Product object stored in a hash :  HashMap String, String  map = new HashMap String, String ();\n        map.put( ProductID ,  ThisIsProductID );\n        map.put( SKU ,  ThisIsSKU );\n        map.put( Name ,  shirt );\n        map.put( Color ,  Red );\n        map.put( Price ,  20 );\n        this.jedisClient.hmset( Products:  + map.get( ProductID ), map);\n\n// get the product object using the key (primary index)\nthis.jedisClient.hgetAll( Products:  + productID);  In the above example we have used the productID as the primary key for the product hash object.", 
            "title": "Primary Index"
        }, 
        {
            "location": "/Redis/Query Model/Indexing/#secondary-index", 
            "text": "If we want to access the product object shown in the previous example using other fields instead of the primary key such as using the product colour, we can create a set for each colour that contains all product Ids having this colour as shown in the example below:  this.jedisClient.sadd( Product.Colour.Index:  + map.get( Colour ),\n                map.get( ProductID ));  Then we can use this secondary index as shown below:  this.jedisClient.smembers( Product.Colour.Index:  + colour) ; \n// Then hgetAll for each productID  You can also use a sorted sets to create secondary indexes in case you want to index numeric values such as the product price. An example is shown below:  this.jedisClient.zadd( Product.Price.Index ,\n                Double.valueOf(map.get( Price )), map.get( ProductID ));  In the above example, we use sorted set as a secondary index to be able to query easily for the product price. Each product id is mapped to price inside the sorted set which allows you later to do range queries like the below:  this.jedisClient.zrangeByScore( Product.Price.Index , min, max);   In the above query, we are returning all the products with price between a min and max values.", 
            "title": "Secondary Index"
        }, 
        {
            "location": "/Redis/Query Model/Indexing/#compound-index", 
            "text": "You can use sorted sets to create a compound index. To do that, you need to set the score of the sorted set all to zero which makes Redis sort the set lexicographically which then can compare two field at the same time. An example is shown below:      this.jedisClient.zadd(\n                 Product.Category.Price.Index ,\n                0,\n                map.get( CategoryID ) +  :  + map.get( Price ) +  : \n                        + map.get( ProductID ));  In the above example, we are inserting the product id and the category id inside a sorted set and setting the score to zero. The order in the sorted set will be now a lexicographically order which means Redis is going to order the string having \"categoryID:price:productID\" byte by byte. So if you want to return all product Ids in the category id 2 between price 20 and 30, you can run the below query:  // get price between 20-30 for category 2\nthis.jedisClient.zrangeByLex( Product.Catgeory.Price.Index ,  [2:20 ,  [2:30 );", 
            "title": "Compound Index"
        }, 
        {
            "location": "/Redis/Query Model/Query Options/", 
            "text": "Redis is a key-value database which means you can query your data easily using the keys. In this section I will show how range and parameterised queries are supported.\n\n\nParameterised Queries\n\n\nIf you want to run parameterized queries in the data stored inside the different Redis data structures, you can easily query them using the data structure key. Examples for each data structure is given below:\n\n\nA string data structure:\n\n\nredis\n SET mykey \nHello\n\nOK\nredis\n GET mykey\n\nHello\n\n\n\n\n\nIn a list data structure, you can query a certain element by index:\n\n\nredis\n LPUSH mylist \nworld\n\n(integer) 1\nredis\n LPUSH mylist \nhello\n\n(integer) 2\nredis\n LINDEX mylist 0\n\nHello\n\n\n\n\n\nIn a set data structure, you can only query using the set key and a certain member to check if the member exists inside the set or not :\n\n\nredis\n SADD myset \none\n\n(integer) 1\nredis\n SISMEMBER myset \none\n\n(integer) 1\n\n\n\n\nIn a sorted set data structure, you can query your data by both a key and a score:\n\n\nredis\n ZADD myzset 1 \none\n\n(integer) 1\nredis\n ZSCORE myzset \none\n\n\n1\n\n\n\n\n\nIn a hash data structure, you can query your data by the hash key and fields inside each hash as shown below:\n\n\n(integer) 1\nredis\n HGET myhash field1\n\nfoo\n\nredis\n HGET myhash field2\n(nil)\nredis\n HGETALL myhash\n(all fields and values)\n\n\n\n\nBeside querying the data using the primary key, you can use secondary or compound keys as has been already explained in the \nprevious section.\n\n\nRange Queries\n\n\nRange queries is supported in Redis using sorted sets. For example if you have a product object stored in hash data structure, then you can easily get all products within a specific price range by storing the product Ids in a sorted set data structure with product prices as the sorted set scores. Then to do the price range query, you can get elements from the sorted set using scores range as seen below:\n\n\nthis.jedisClient.zrangeByScore(\nProduct.Price.Index\n, min, max); \n\n\n\n\nIn the above query, we are retrieving all product Ids within a min and a max price range. Then you can get the product details in the hash using the product Ids as seen below:\n\n\nthis.jedisClient.hgetAll(\nProducts:\n + productID);\n\n\n\n\nBuilt-in query functions\n\n\nIn this section, I will talk about how Redis supports some common query functions such as count, max, min, sum, average, and others.\n\n\nCount\n\n\nTo get the number of all elements inside a Redis data structure, different commands are supported for each data structures as shown below:\n\n\nIn the list data structure, the LLEN command will give you the length of the list as shown below:\n\n\n(integer) 1\nredis\n LPUSH mylist \nHello\n\n(integer) 2\nredis\n LLEN mylist\n(integer) 2\nredis\n\n\n\n\n\nIn the set data structure, the SCARD will give you the number of members in the set as shown below:\n\n\nredis\n SADD myset \nHello\n\n(integer) 1\nredis\n SADD myset \nWorld\n\n(integer) 1\nredis\n SCARD myset\n(integer) 2 \n\n\n\n\nAnd in the sorted set data structure, the ZCARD command can be used to return the number of members inside the sorted set as shown below:\n\n\nredis\n ZADD myzset 1 \none\n\n(integer) 1\nredis\n ZADD myzset 2 \ntwo\n\n(integer) 1\nredis\n ZCARD myzset\n(integer) 2\n\n\n\n\nmin and max\n\n\nUsually min and max can be achieved using a sorted set. For instance, if you want to get the minimum and maximum price for a products, you will insert these products inside a sorted set with score as the product price. Then to get the minimum or maximum price, you will just get use the ZRANGEBYSCORE or ZREVRANGEBYSCORE commands to get the minimum and maximum prices. Example is shown below:\n\n\nredis\n ZADD myzset 100 \nproductOne\n\n(integer) 1\nredis\n ZADD myzset 90 \nproductTwo\n\n(integer) 1\nredis\n ZADD myzset 200 \nproductThree\n\n(integer) 1\nredis\n ZREVRANGEBYSCORE myzset 0 0\n1) \nproductThree\n\nredis\n ZRANGEBYSCORE myzset 0 0\n1) \nproductTwo\n\n\n\n\n\nsum, average and similar queries\n\n\nThis kind of queries needs to be handled in the client side. For example if you want to get the sum price of all products in the cart, the client needs to get all products then go through all of them and sum the price.", 
            "title": "Query Options"
        }, 
        {
            "location": "/Redis/Query Model/Query Options/#parameterised-queries", 
            "text": "If you want to run parameterized queries in the data stored inside the different Redis data structures, you can easily query them using the data structure key. Examples for each data structure is given below:  A string data structure:  redis  SET mykey  Hello \nOK\nredis  GET mykey Hello   In a list data structure, you can query a certain element by index:  redis  LPUSH mylist  world \n(integer) 1\nredis  LPUSH mylist  hello \n(integer) 2\nredis  LINDEX mylist 0 Hello   In a set data structure, you can only query using the set key and a certain member to check if the member exists inside the set or not :  redis  SADD myset  one \n(integer) 1\nredis  SISMEMBER myset  one \n(integer) 1  In a sorted set data structure, you can query your data by both a key and a score:  redis  ZADD myzset 1  one \n(integer) 1\nredis  ZSCORE myzset  one  1   In a hash data structure, you can query your data by the hash key and fields inside each hash as shown below:  (integer) 1\nredis  HGET myhash field1 foo \nredis  HGET myhash field2\n(nil)\nredis  HGETALL myhash\n(all fields and values)  Beside querying the data using the primary key, you can use secondary or compound keys as has been already explained in the  previous section.", 
            "title": "Parameterised Queries"
        }, 
        {
            "location": "/Redis/Query Model/Query Options/#range-queries", 
            "text": "Range queries is supported in Redis using sorted sets. For example if you have a product object stored in hash data structure, then you can easily get all products within a specific price range by storing the product Ids in a sorted set data structure with product prices as the sorted set scores. Then to do the price range query, you can get elements from the sorted set using scores range as seen below:  this.jedisClient.zrangeByScore( Product.Price.Index , min, max);   In the above query, we are retrieving all product Ids within a min and a max price range. Then you can get the product details in the hash using the product Ids as seen below:  this.jedisClient.hgetAll( Products:  + productID);", 
            "title": "Range Queries"
        }, 
        {
            "location": "/Redis/Query Model/Query Options/#built-in-query-functions", 
            "text": "In this section, I will talk about how Redis supports some common query functions such as count, max, min, sum, average, and others.", 
            "title": "Built-in query functions"
        }, 
        {
            "location": "/Redis/Query Model/Query Options/#count", 
            "text": "To get the number of all elements inside a Redis data structure, different commands are supported for each data structures as shown below:  In the list data structure, the LLEN command will give you the length of the list as shown below:  (integer) 1\nredis  LPUSH mylist  Hello \n(integer) 2\nredis  LLEN mylist\n(integer) 2\nredis   In the set data structure, the SCARD will give you the number of members in the set as shown below:  redis  SADD myset  Hello \n(integer) 1\nredis  SADD myset  World \n(integer) 1\nredis  SCARD myset\n(integer) 2   And in the sorted set data structure, the ZCARD command can be used to return the number of members inside the sorted set as shown below:  redis  ZADD myzset 1  one \n(integer) 1\nredis  ZADD myzset 2  two \n(integer) 1\nredis  ZCARD myzset\n(integer) 2", 
            "title": "Count"
        }, 
        {
            "location": "/Redis/Query Model/Query Options/#min-and-max", 
            "text": "Usually min and max can be achieved using a sorted set. For instance, if you want to get the minimum and maximum price for a products, you will insert these products inside a sorted set with score as the product price. Then to get the minimum or maximum price, you will just get use the ZRANGEBYSCORE or ZREVRANGEBYSCORE commands to get the minimum and maximum prices. Example is shown below:  redis  ZADD myzset 100  productOne \n(integer) 1\nredis  ZADD myzset 90  productTwo \n(integer) 1\nredis  ZADD myzset 200  productThree \n(integer) 1\nredis  ZREVRANGEBYSCORE myzset 0 0\n1)  productThree \nredis  ZRANGEBYSCORE myzset 0 0\n1)  productTwo", 
            "title": "min and max"
        }, 
        {
            "location": "/Redis/Query Model/Query Options/#sum-average-and-similar-queries", 
            "text": "This kind of queries needs to be handled in the client side. For example if you want to get the sum price of all products in the cart, the client needs to get all products then go through all of them and sum the price.", 
            "title": "sum, average and similar queries"
        }, 
        {
            "location": "/Redis/Query Model/Regular Expressions Support/", 
            "text": "Redis supports some search commands that uses regular expressions such as KEYS which searches all keys in a Redis instance. Additionally Redis supports the SCAN command which is used to search and iterate through all keys and values (SSCAN, HSCAN, and ZSCAN are the same but specific for each data type).\n\n\nKEYS and SCAN can use global-style regular expression patterns. Example of what can be used is shown below:\n\n\n\n\nh?llo matches hello, hallo and hxllo\n\n\nh*llo matches hllo and heeeello\n\n\nh[ae]llo matches hello and hallo, but not hillo\n\n\nh[^e]llo matches hallo, hbllo, ... but not hello\n\n\nh[a-b]llo matches hallo and hbllo", 
            "title": "Regular Expressions Support"
        }, 
        {
            "location": "/Redis/Query Model/Sorting/", 
            "text": "Redis supports a sort command that can be used to sort most of Redis data structures such as lists, sets and sorted sets. This sort command can sort your data ascendingly, descendingly or alphabetically. You can also sort by a specific pattern or limit the returned data. Another useful feature of the sort command, is that you can also sort by external keys. In the below section, I will show how you can use the Sort command.\n\n\nSort ascendancy, despondingly or alphabetically\n\n\nUsing the Sort command, you can sort your list, set or sorted sets easily either ascendancy, despondingly or alphabetically. An example for each is shown below:\n\n\nSORT mylist DESC\n\nSORT mylist ASC\n\nSORT mylist ALPHA\n\n\n\n\nLimit the returned sorted data\n\n\nThe number of returned elements can be limited using the Limit modifier as shown below:\n\n\nSORT mylist LIMIT 0 5 ALPHA DESC\n\n\n\n\nSort by a certain pattern\n\n\nYou can also sort only the elements that are matching a certain pattern using GET Pattern. Example is shown below:\n\n\nSORT mylist GET object_* \n\n\n\n\nSort by an external key\n\n\nAnother useful feature of the sort command, is that you can sort the elements in a list, set or a sorted set using external key. For example if you have a list that you want to sort based on a certain weight that exists in another data structure, then you can sort the list using the weight as shown below:\n\n\nSORT mylist BY weight_*", 
            "title": "Sorting"
        }, 
        {
            "location": "/Redis/Query Model/Sorting/#sort-ascendancy-despondingly-or-alphabetically", 
            "text": "Using the Sort command, you can sort your list, set or sorted sets easily either ascendancy, despondingly or alphabetically. An example for each is shown below:  SORT mylist DESC\n\nSORT mylist ASC\n\nSORT mylist ALPHA", 
            "title": "Sort ascendancy, despondingly or alphabetically"
        }, 
        {
            "location": "/Redis/Query Model/Sorting/#limit-the-returned-sorted-data", 
            "text": "The number of returned elements can be limited using the Limit modifier as shown below:  SORT mylist LIMIT 0 5 ALPHA DESC", 
            "title": "Limit the returned sorted data"
        }, 
        {
            "location": "/Redis/Query Model/Sorting/#sort-by-a-certain-pattern", 
            "text": "You can also sort only the elements that are matching a certain pattern using GET Pattern. Example is shown below:  SORT mylist GET object_*", 
            "title": "Sort by a certain pattern"
        }, 
        {
            "location": "/Redis/Query Model/Sorting/#sort-by-an-external-key", 
            "text": "Another useful feature of the sort command, is that you can sort the elements in a list, set or a sorted set using external key. For example if you have a list that you want to sort based on a certain weight that exists in another data structure, then you can sort the list using the weight as shown below:  SORT mylist BY weight_*", 
            "title": "Sort by an external key"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/", 
            "text": "In this section, I will talk about the strengths and weaknesses of Redis and when using it makes sense.\n\n\nStrengths\n\n\nRedis has many advantages that makes it suitable for many use cases. Below I have summarised the main ones. \n\n\nVery fast in-memory database\n\n\nBecause of it is architecture and like most in-memory databases, Redis is Lightening fast. Redis is a high performance database that can be scaled easily to hundreds of gigabytes of data and millions of requests per second.\n\n\nComplex data structures\n\n\nRedis doesn't only support the String data type, rather it supports more complex data structures such as lists, hashes, sets and sorted sets. Strings can be also integers, floating point values or even a binary data such as an image or any file. This makes Redis suitable for many use cases such as to implement queues, and voting systems.\n\n\nHigh availability\n\n\nRedis provides a master-slave distributed system called Redis Sentinel to guarantee high availability. Redis Sentinel can resist certain types of failures automatically without any human intervention. Redis Sentinel will also start a failover process in case the master is not working to promote one of the slaves to replace it and reconfigure the other slaves to contact this new master.\n\n\nOn-Disk persistance\n\n\nRedis supports two ways to persist data on disk. The first option is called snapshotting or RDB which takes a snapshot of the current data periodically (based on pre-configured value) and store it on disk. The second option is called append-only file or AOF which simply works by logging the operations that modifies the data (write operations) into a log file. \n\n\nLua scripting\n\n\nRedis allows you to execute Lua scripts inside the Redis server. Scripting Redis with Lua is useful and can be used to avoid some common pitfalls that slow down development or reduce performance.\n\n\nBuilt-in replication and automatic sharding\n\n\nRedis has a built in replication that is used to scalability as well as for availability. Redis can scale reads by allowing the slave instances to share the read load. Additionally, replication is used for fault tolerance and disaster recovery. Scaling writes is supported using automatic sharding that is provided by Redis cluster.\n\n\nConfigured as a cache\n\n\nRedis can be easily configured to act automatically as a cache and use one of the popular eviction algorithms such as the LRU algorithm which makes it act like memcached database.\n\n\nPub/Sub\n\n\nThis is a special feature of Redis that uses the publish/subscribe messaging paradigm and it is suitable for implementing chatting related use cases.\n\n\nSingle threaded with transaction support\n\n\nRedis is a single threaded application only one command can be executed at a time. So the idea behind transactions support in Redis is to wrap the transaction commands and execute them all as a single command.  Redis doesn't support Rollback as in relational databases and optimistic locking can be used to avoid race conditions that will happen when another client tries to modify the same value used in a transaction.\n\n\nWeaknesses\n\n\nBelow are the main weaknesses of Redis:\n\n\nMemory limit\n\n\nRedis is an in-memory database, which means that the whole dataset should reside in the memory (RAM). This can be costly if you are planning to have large datasets.\n\n\nPersistence\n\n\nPersistence can impact performance since Redis will use memory dump to create snapshots used for persistency. Depending on your configuration of the fsync linux command, taking snapshots can slow the database down.\n\n\nQuery and Aggregation\n\n\nRedis isn't intended for rich queries since it is a key-value database. There is no internal full-text search support and it is difficult to model relationships using Redis. Additionally, aggregate functions such as sum, average aren\u2019t supported and need to be handled in the client side. Indexes aren't supported internally and \"sets\" can be used as a workaround but maintaining them manually can be very complex. Range queries are also supported by manually maintaining some \"sets\". Finally, Redis doesn't support any query language (only commands are used) and there is no support for ad-hoc queries like in relational database. \n\n\nSecurity\n\n\nRedis supports only basic security options. Redis doesn't provide any access control and this should be provided by a separate authorization layer.  Redis doesn't also support any encryption mechanism and this should be implemented also using a separate layer such as SSL proxy. \n\n\nSummary\n\n\nRedis is a very fast in-memory database that is suitable for limited use cases and can't be used as a general domain database. It is best suitable for applications that have a foreseeable dataset size since Redis's dataset can't greater than the memory. Of course, it is possible to scale out Redis easily to support very large datasets but this can be very costly. Redis is suitable for applications with reasonable dataset size that requires high speed or near real-time performance such as real-time analytics. Additionally due to its unique internal data structures, Redis can be used to implement applications such caching, session management, chat server,  and queue systems.  Redis isn't suitable for applications that are having heavily related data and that is intended to serve complex or rich queries in the future.", 
            "title": "Strengths and Weaknesses"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#strengths", 
            "text": "Redis has many advantages that makes it suitable for many use cases. Below I have summarised the main ones.", 
            "title": "Strengths"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#very-fast-in-memory-database", 
            "text": "Because of it is architecture and like most in-memory databases, Redis is Lightening fast. Redis is a high performance database that can be scaled easily to hundreds of gigabytes of data and millions of requests per second.", 
            "title": "Very fast in-memory database"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#complex-data-structures", 
            "text": "Redis doesn't only support the String data type, rather it supports more complex data structures such as lists, hashes, sets and sorted sets. Strings can be also integers, floating point values or even a binary data such as an image or any file. This makes Redis suitable for many use cases such as to implement queues, and voting systems.", 
            "title": "Complex data structures"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#high-availability", 
            "text": "Redis provides a master-slave distributed system called Redis Sentinel to guarantee high availability. Redis Sentinel can resist certain types of failures automatically without any human intervention. Redis Sentinel will also start a failover process in case the master is not working to promote one of the slaves to replace it and reconfigure the other slaves to contact this new master.", 
            "title": "High availability"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#on-disk-persistance", 
            "text": "Redis supports two ways to persist data on disk. The first option is called snapshotting or RDB which takes a snapshot of the current data periodically (based on pre-configured value) and store it on disk. The second option is called append-only file or AOF which simply works by logging the operations that modifies the data (write operations) into a log file.", 
            "title": "On-Disk persistance"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#lua-scripting", 
            "text": "Redis allows you to execute Lua scripts inside the Redis server. Scripting Redis with Lua is useful and can be used to avoid some common pitfalls that slow down development or reduce performance.", 
            "title": "Lua scripting"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#built-in-replication-and-automatic-sharding", 
            "text": "Redis has a built in replication that is used to scalability as well as for availability. Redis can scale reads by allowing the slave instances to share the read load. Additionally, replication is used for fault tolerance and disaster recovery. Scaling writes is supported using automatic sharding that is provided by Redis cluster.", 
            "title": "Built-in replication and automatic sharding"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#configured-as-a-cache", 
            "text": "Redis can be easily configured to act automatically as a cache and use one of the popular eviction algorithms such as the LRU algorithm which makes it act like memcached database.", 
            "title": "Configured as a cache"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#pubsub", 
            "text": "This is a special feature of Redis that uses the publish/subscribe messaging paradigm and it is suitable for implementing chatting related use cases.", 
            "title": "Pub/Sub"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#single-threaded-with-transaction-support", 
            "text": "Redis is a single threaded application only one command can be executed at a time. So the idea behind transactions support in Redis is to wrap the transaction commands and execute them all as a single command.  Redis doesn't support Rollback as in relational databases and optimistic locking can be used to avoid race conditions that will happen when another client tries to modify the same value used in a transaction.", 
            "title": "Single threaded with transaction support"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#weaknesses", 
            "text": "Below are the main weaknesses of Redis:", 
            "title": "Weaknesses"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#memory-limit", 
            "text": "Redis is an in-memory database, which means that the whole dataset should reside in the memory (RAM). This can be costly if you are planning to have large datasets.", 
            "title": "Memory limit"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#persistence", 
            "text": "Persistence can impact performance since Redis will use memory dump to create snapshots used for persistency. Depending on your configuration of the fsync linux command, taking snapshots can slow the database down.", 
            "title": "Persistence"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#query-and-aggregation", 
            "text": "Redis isn't intended for rich queries since it is a key-value database. There is no internal full-text search support and it is difficult to model relationships using Redis. Additionally, aggregate functions such as sum, average aren\u2019t supported and need to be handled in the client side. Indexes aren't supported internally and \"sets\" can be used as a workaround but maintaining them manually can be very complex. Range queries are also supported by manually maintaining some \"sets\". Finally, Redis doesn't support any query language (only commands are used) and there is no support for ad-hoc queries like in relational database.", 
            "title": "Query and Aggregation"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#security", 
            "text": "Redis supports only basic security options. Redis doesn't provide any access control and this should be provided by a separate authorization layer.  Redis doesn't also support any encryption mechanism and this should be implemented also using a separate layer such as SSL proxy.", 
            "title": "Security"
        }, 
        {
            "location": "/Redis/Results/Strengths and Weaknesses/#summary", 
            "text": "Redis is a very fast in-memory database that is suitable for limited use cases and can't be used as a general domain database. It is best suitable for applications that have a foreseeable dataset size since Redis's dataset can't greater than the memory. Of course, it is possible to scale out Redis easily to support very large datasets but this can be very costly. Redis is suitable for applications with reasonable dataset size that requires high speed or near real-time performance such as real-time analytics. Additionally due to its unique internal data structures, Redis can be used to implement applications such caching, session management, chat server,  and queue systems.  Redis isn't suitable for applications that are having heavily related data and that is intended to serve complex or rich queries in the future.", 
            "title": "Summary"
        }, 
        {
            "location": "/Redis/Results/Summary/", 
            "text": "Redis is an extremely fast NoSQL key-value in-memory database which stores data in different useful data structures such as strings, list, sets and sorted sets. Redis supports many powerful features like built-in persistency, pub/sub, transaction support with optimistic locking and Lua Scripting. Redis is a very good choice if you are looking for a very fast and highly scalable and available data store solution.\n\n\nUse cases\n\n\nRedis is a high performance database that can be scaled easily to hundreds of gigabytes of data and millions of requests per second. It also provides on-disk persistence and supports very unique data structures which makes it suitable for a variety of applications such as caching, as a message broker, as a chat server, in session management, as a distributed lock system, to implement queues, as a logging system, in any counting related applications,  in real time analytics, to implement leaderboards, as a voting system and many other applications.\n\n\nBasic Concepts\n\n\nRedis supports multiple useful data structures such as strings, lists, hashes, sets and sorted sets. Strings can be also integers, floating point values or even a binary data such as an image or any file. However, the size of the stored value shouldn't exceed a maximum 512MB of data. Lists stores sequence of ordered elements that follows the well-known linked list structure. The elements are added only to the head or the tail of the list which is an extremely fast operation and takes a constant time. However accessing the elements by index is not so fast and will requires an amount of work proportional to the accessed index. Hashes stores used to store complete complex objects and can be compared to the documents in a documents-based database or like a row in a relational database. Sets can be used to store unordered sequence of strings that are unique and sorted sets are a special case of the set implementation where it defines a score for each string. The score allows you to get the elements inside the sorted sets by a particular order. Also you can retrieve ordered elements in the sorted set by score range.\n\n\nInstalling\n\n\nRedis can be installed on most of the popular operating systems except windows. It can easily installed by building it from source using the distribution binaries or by using many package managers such as apt-get, yum, brew , and port.\n\n\nQuery language\n\n\nRedis supports a set of commands for each data type to run CRUD operations such as add, update, get or remove in the stored data. Those commands can be executed on bulk and a partial transaction is supported. Executing these commands can be done using the built-in client \"redis-cli\" or by using one of the supported driver clients specific for many programming languages. Redis has a relatively short list of commands that can be easily learned in few hours.\n\n\nTransaction support\n\n\nRedis is a single threaded application only one command can be executed at a time. So the idea behind transactions support in Redis is to wrap the transaction commands and execute them all as a single command. This is done by using MULTI/EXEC commands, you start the transaction by using MULTI command, then all the following commands will be queued. After all transaction commands are queued, they will be executed all as a single command using the EXEC command.  The transaction commands will be either executed together or none will be executed. However the problem occurs when any one of the transaction commands are mistakenly executed (not syntax mistakes) due to some programming bug. In this case unfortunately Redis doesn't provide any way for a Rollback as in traditional databases. Rollback isn't supported due to reasons related to performance.   To avoid race conditions that will happen when another client tries to modify the same value used in a transaction, optimistic locking is provided using WATCH/UNWATCH commands. Finally, Redis support scripting using LUA which is transactional by definition since it can be executed as a single command. Hence Lua scripting can be used to support transactions.\n\n\nData Import and Export\n\n\nRedis supports a pipe mode that was designed in order to perform mass insertion. You just put all the data you want to insert in a file and then perform mass insertion using \"redis-cli --pipe\". Pipelining is also supported in Redis which is used to batch multiple commands and send them together to the server at once without waiting for their replies. At the end you can read the replies in a single step. Pipelining can improve system performance since the round trips between client and server are reduced.\n\n\nData Layout\n\n\nIn Redis there is no schema and hence no need for data model design. We just need to think about our underline data structure that we need to store. Then we need to think about which data types we need to use in order to represent this data. An example would be to model an article voting system. We would store the article object in Hash. Then we can store the list of articles ordered by votes in a sorted sets where score is the number of votes. Finally a normal set can store the name of the users who have voted for a particular article.\n\n\nRelational data\n\n\nUsing Redis for complex relational data isn't usually recommended, however Redis supports modelling data relationships using sets and sorted sets. Generally, a one-to-one and one-to-many relationships are modelled using a single set but modeling many-to-many relationships using two sets. Join-like operations are then possible using sets intersect and union commands such as SDIFF, SINTER, SUNION, ZINTERSTORE, and ZUNIONSTORE.\n\n\nNormalisation/Denormalisation\n\n\nDenormalisation can be used in Redis wherever it is needed if it will give better performance or model data relationships.\n\n\nReferential Integrity\n\n\nSince Redis isn't a relational database, referential integrity is not maintained by Redis, and must be enforced by the client application. Also there is no concept of Foreign key for the same reasons.\n\n\nNested Data\n\n\nRedis doesn't support nested structures. For instance, Hash, lists and sets internal elements can only be of a String data type and can't embed other data types. However you can still store in the string data type any stream of bytes which can be JSON data but that wont be helpful since you can't query this nested data. \n\n\nBuilt-in query functions\n\n\nCount function is supported in Redis using different commands for each data type such as LLEN, SCARD, and ZCARD. Min and Max functions can be achieved using a sorted set using commands such as ZRANGEBYSCORE, and ZREVRANGEBYSCORE. Finally, there is no support for aggregate functions such as sum and average and these functions need to be handled manually in the client application.\n\n\nQuery\n\n\nRedis is a key-value database which means you can query your data easily using the keys. A secondary or compound index-like can be created and maintained manually to support querying using fields other than the key. Finally, Range queries are supported using sorted sets. \n\n\nFull Text Search\n\n\nRedis doesn't support full-text search internally, but some search engines can be used with Redis such as elastic search and Apache Solr.\n\n\nRegular Expressions\n\n\nRedis supports some search commands that uses regular expressions such as KEYS which searches all keys in a Redis instance. Additionally Redis supports the SCAN command which is used to search and iterate through all keys and values (SSCAN, HSCAN, and ZSCAN are the same but specific for each data type). KEYS and SCAN can use global-style regular expression patterns.\n\n\nIndexing\n\n\nRedis is a key-value store, the key acts as the primary index in all data structures. Redis doesn't support internally other type of indexes such secondary and compound indexes. However, secondary and compound indexes can be implemented and maintained manually by the clients using sets and sorted sets.  Although creating secondary indexes and compound indexes is still possible somehow using sets and sorted sets, maintaining theses indexes manually at each write operation can be difficult and is prone to human error. \n\n\nGrouping and Filtering\n\n\nIn general, grouping and filtering data in Redis is done. To group your data in Redis, the set data structure is the best candidate since you will be able to easily insert elements from the same group type as members of the set using sets and sorted sets. To filter data in Redis, you can use the intersection, union, and difference functionalities provided by the set and sorted sets. \n\n\nSorting\n\n\nRedis supports a sort command that can be used to sort most of Redis data structures such as lists, sets and sorted sets. This sort command can sort your data ascendingly, descendingly or alphabetically. You can also sort by a specific pattern or limit the returned data. Another useful feature of the sort command, is that you can also sort by external keys.\n\n\nLua scripting\n\n\nRedis allows you to execute Lua scripts inside the Redis server. Scripting Redis with Lua is useful and can be used to avoid some common pitfalls that slow down development or reduce performance. To load the script to Redis server you can use either EVAL or EVALSHA commands which will evaluate the scripts using the Lua interpreter and load it. By using EVAL you can simply load a Lua program without defining a Lue function and pass arguements to the Lua program as redis keys.  The script is executed in Atomicity way which makes it useful for transaction related tasks.\n\n\nPub/Sub\n\n\nThis is a special feature of Redis that can be used to implement applications such as a chat server. The idea is mainly characterised by listeners who can subscribe to certain channels to receive real time messages. The publishers can send binary messages of type string to channels without having any knowledge of whether there are subscribers or not. This feature is following the publish/subscribe messaging paradigm. \n\n\nExpire\n\n\nIn Redis you can set a timeout on a key. It means that the key will be automatically deleted after the timeout period expires. This feature makes Redis a good database to implement cashing and similar applications.\n\n\nConfigure Redis as a cache\n\n\nIf you plan to use Redis as a cache system, then instead of expiring keys manually you can just let Redis take care of this automatically. Redis provides a way to easily configure your Redis server to act automatically as a cache and use one of the popular eviction algorithms such as the LRU algorithm which makes it act like memcached database.\n\n\nConfiguration\n\n\nRedis can start without a configuration file and then it will take its built-in default configurations. Redis stores its configurations inside a file called redis.config and you can also pass configuration parameters via the command line. Configuring Redis while the server is running is also possible without the need to stop or restart the server using CONFIG GET , and CONFIG SET.\n\n\nScalability\n\n\nScaling reads in Redis is done using replications where slave instances can share the read load. Scaling writes is supported using sharding. The important question about sharding is who will shard the data. In Redis, the partitioning can be done in the client side where the clients directly select the right instance to be used to write or read a certain key. Most of the clients or drivers that are provided by Redis have implementations for partitioning. Another implementation for partitioning is by using a proxy which means that clients send request to a proxy that is able to interact with Redis and will forward the clients' requests to the right Redis instance then send back the reply to the clients. A famous proxy used by Redis and Memcached is \nTwemproxy\n. Finally it is also possible to use an implementation for partitioning called query routing where you just send your requests to a random instance that will forward your request to the right Redis instance. Starting from Redis version 3.0, Redis cluster is the preferred way to get automatic partitioning and high availability. Redis cluster uses a hybrid implementation of query routing and some clients side implementation. \n\n\nPersistency\n\n\nRedis supports two ways to persist data on disk. The first option is called snapshotting or RDB which takes a snapshot of the current data periodically (based on pre-configured value) and store it on disk. The second option is called append-only file or AOF which simply works by logging the operations that modifies the data (write operations) into a log file. Processing this log file later if needed will produce the same dataset that was in memory. Additionally depending on your durability requirements, you need to set the fsync options as explained below:\n\n\n\n\n\"fsync every\" when new command is logged to the AOF. This option is used if you want to have a full durability support but it is also a very slow option. \n\n\n\"fsync every second\". This is the default configuration which will provide a good durability option but with a 1 second data lose risk in case of a disaster.This option is fast enough.\n\n\n\"Never fsync\", using this option will let the operating system handle the fsync which can be very unsafe in term of durability. On the other hand, this option is very fast.\n\n\n\n\nAtomicity can be guaranteed by executing a group of commands either using MULTI/EXEC blocks or by using a Lua script.\nConsistency is guaranteed by using the WATCH/UNWATCH blocks.\nIsolation is always guaranteed at command level, and for group of commands it can be also guaranteed by MULTI/EXEC block or a Lua script.\nFull Durability can be also guaranteed when using AOF with executing fsync with each new command as explained before.\n\n\nBackup\n\n\nBackup in Redis is basically the snapshots that were taken of your dataset if the snapshot option is enabled. Your data will be stored in an RDB file called dump.rdb file in your redis directory. To restore your data, you just need to move this dump file to your Redis directory and then start up your server. To have a better backup, you can use snapshotting with replication so that you will have the same copy of your data in all your instances including masters and slaves. This will ensure that your data is save in case the master server crashed completely\n\n\nSecurity\n\n\nRedis doesn't provide any access control and this should be provided by a separate authorization layer.  However Redis provides an authentication mechanism that is optional and can be turned on from redis.conf. Redis supports the BIND command to allow only specific IP addresses to access the Redis server for better security. Redis doesn't also support any encryption mechanism and this should also be implemented using a separate layer like using encryption mechanisms such as SSL proxy. The \"rename-command\u201d used to rename the original commands which reduces the risk that might happen if unauthorised clients access your server. Finally,  NoSQL injection attacks are impossible since Redis protocol has no concept of string escaping. NoSQL injection attacks are impossible since Redis protocol has no concept of string escaping.\n\n\nUpgrading\n\n\nRedis doesn't support online upgrades (server must restart and the clients can't connect to it during the upgrade window). A workaround for online upgrade is to start a slave server and direct all the clients to it while you do an upgrade to the master. After the upgrade is finished in the master, we can stop the slave server and then redirect the clients requests again to the master server.\n\n\nAvailability\n\n\nRedis provides a distributed system called Redis Sentinel to guarantee high availability. Redis Sentinel can resist certain types of failures automatically without any human intervention. Redis Sentinel can be used also for other tasks such as monitoring, as a configuration provider for clients and as a notification system. By Monitoring we mean that Redis Sentinel will monitor your master and slave instances and check if they are working as expected. It will also send notifications to the admin in case anything went wrong. Clients can also contact the Redis Sentinel to get any configurations parameters for a certain Redis instance such as address and port. Finally Redis Sentinel will start a failover process in case the master is not working to promote one of the slaves to replace it and reconfigure the other slaves to contact this new master.\n\n\nAn important note is that in order to get the best robustness out of Redis Sentinel, you should configure it in such a way that it will not be a single point of failure. This means that you use multiple instances in separate machines or virtual machines that fail independently which can cooperate together to handle Redis failure detection and recovery.  You need at least three Redis Sentinel instance to guarantee its Robustness.", 
            "title": "Summary"
        }, 
        {
            "location": "/Redis/Results/Summary/#use-cases", 
            "text": "Redis is a high performance database that can be scaled easily to hundreds of gigabytes of data and millions of requests per second. It also provides on-disk persistence and supports very unique data structures which makes it suitable for a variety of applications such as caching, as a message broker, as a chat server, in session management, as a distributed lock system, to implement queues, as a logging system, in any counting related applications,  in real time analytics, to implement leaderboards, as a voting system and many other applications.", 
            "title": "Use cases"
        }, 
        {
            "location": "/Redis/Results/Summary/#basic-concepts", 
            "text": "Redis supports multiple useful data structures such as strings, lists, hashes, sets and sorted sets. Strings can be also integers, floating point values or even a binary data such as an image or any file. However, the size of the stored value shouldn't exceed a maximum 512MB of data. Lists stores sequence of ordered elements that follows the well-known linked list structure. The elements are added only to the head or the tail of the list which is an extremely fast operation and takes a constant time. However accessing the elements by index is not so fast and will requires an amount of work proportional to the accessed index. Hashes stores used to store complete complex objects and can be compared to the documents in a documents-based database or like a row in a relational database. Sets can be used to store unordered sequence of strings that are unique and sorted sets are a special case of the set implementation where it defines a score for each string. The score allows you to get the elements inside the sorted sets by a particular order. Also you can retrieve ordered elements in the sorted set by score range.", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/Redis/Results/Summary/#installing", 
            "text": "Redis can be installed on most of the popular operating systems except windows. It can easily installed by building it from source using the distribution binaries or by using many package managers such as apt-get, yum, brew , and port.", 
            "title": "Installing"
        }, 
        {
            "location": "/Redis/Results/Summary/#query-language", 
            "text": "Redis supports a set of commands for each data type to run CRUD operations such as add, update, get or remove in the stored data. Those commands can be executed on bulk and a partial transaction is supported. Executing these commands can be done using the built-in client \"redis-cli\" or by using one of the supported driver clients specific for many programming languages. Redis has a relatively short list of commands that can be easily learned in few hours.", 
            "title": "Query language"
        }, 
        {
            "location": "/Redis/Results/Summary/#transaction-support", 
            "text": "Redis is a single threaded application only one command can be executed at a time. So the idea behind transactions support in Redis is to wrap the transaction commands and execute them all as a single command. This is done by using MULTI/EXEC commands, you start the transaction by using MULTI command, then all the following commands will be queued. After all transaction commands are queued, they will be executed all as a single command using the EXEC command.  The transaction commands will be either executed together or none will be executed. However the problem occurs when any one of the transaction commands are mistakenly executed (not syntax mistakes) due to some programming bug. In this case unfortunately Redis doesn't provide any way for a Rollback as in traditional databases. Rollback isn't supported due to reasons related to performance.   To avoid race conditions that will happen when another client tries to modify the same value used in a transaction, optimistic locking is provided using WATCH/UNWATCH commands. Finally, Redis support scripting using LUA which is transactional by definition since it can be executed as a single command. Hence Lua scripting can be used to support transactions.", 
            "title": "Transaction support"
        }, 
        {
            "location": "/Redis/Results/Summary/#data-import-and-export", 
            "text": "Redis supports a pipe mode that was designed in order to perform mass insertion. You just put all the data you want to insert in a file and then perform mass insertion using \"redis-cli --pipe\". Pipelining is also supported in Redis which is used to batch multiple commands and send them together to the server at once without waiting for their replies. At the end you can read the replies in a single step. Pipelining can improve system performance since the round trips between client and server are reduced.", 
            "title": "Data Import and Export"
        }, 
        {
            "location": "/Redis/Results/Summary/#data-layout", 
            "text": "In Redis there is no schema and hence no need for data model design. We just need to think about our underline data structure that we need to store. Then we need to think about which data types we need to use in order to represent this data. An example would be to model an article voting system. We would store the article object in Hash. Then we can store the list of articles ordered by votes in a sorted sets where score is the number of votes. Finally a normal set can store the name of the users who have voted for a particular article.", 
            "title": "Data Layout"
        }, 
        {
            "location": "/Redis/Results/Summary/#relational-data", 
            "text": "Using Redis for complex relational data isn't usually recommended, however Redis supports modelling data relationships using sets and sorted sets. Generally, a one-to-one and one-to-many relationships are modelled using a single set but modeling many-to-many relationships using two sets. Join-like operations are then possible using sets intersect and union commands such as SDIFF, SINTER, SUNION, ZINTERSTORE, and ZUNIONSTORE.", 
            "title": "Relational data"
        }, 
        {
            "location": "/Redis/Results/Summary/#normalisationdenormalisation", 
            "text": "Denormalisation can be used in Redis wherever it is needed if it will give better performance or model data relationships.", 
            "title": "Normalisation/Denormalisation"
        }, 
        {
            "location": "/Redis/Results/Summary/#referential-integrity", 
            "text": "Since Redis isn't a relational database, referential integrity is not maintained by Redis, and must be enforced by the client application. Also there is no concept of Foreign key for the same reasons.", 
            "title": "Referential Integrity"
        }, 
        {
            "location": "/Redis/Results/Summary/#nested-data", 
            "text": "Redis doesn't support nested structures. For instance, Hash, lists and sets internal elements can only be of a String data type and can't embed other data types. However you can still store in the string data type any stream of bytes which can be JSON data but that wont be helpful since you can't query this nested data.", 
            "title": "Nested Data"
        }, 
        {
            "location": "/Redis/Results/Summary/#built-in-query-functions", 
            "text": "Count function is supported in Redis using different commands for each data type such as LLEN, SCARD, and ZCARD. Min and Max functions can be achieved using a sorted set using commands such as ZRANGEBYSCORE, and ZREVRANGEBYSCORE. Finally, there is no support for aggregate functions such as sum and average and these functions need to be handled manually in the client application.", 
            "title": "Built-in query functions"
        }, 
        {
            "location": "/Redis/Results/Summary/#query", 
            "text": "Redis is a key-value database which means you can query your data easily using the keys. A secondary or compound index-like can be created and maintained manually to support querying using fields other than the key. Finally, Range queries are supported using sorted sets.", 
            "title": "Query"
        }, 
        {
            "location": "/Redis/Results/Summary/#full-text-search", 
            "text": "Redis doesn't support full-text search internally, but some search engines can be used with Redis such as elastic search and Apache Solr.", 
            "title": "Full Text Search"
        }, 
        {
            "location": "/Redis/Results/Summary/#regular-expressions", 
            "text": "Redis supports some search commands that uses regular expressions such as KEYS which searches all keys in a Redis instance. Additionally Redis supports the SCAN command which is used to search and iterate through all keys and values (SSCAN, HSCAN, and ZSCAN are the same but specific for each data type). KEYS and SCAN can use global-style regular expression patterns.", 
            "title": "Regular Expressions"
        }, 
        {
            "location": "/Redis/Results/Summary/#indexing", 
            "text": "Redis is a key-value store, the key acts as the primary index in all data structures. Redis doesn't support internally other type of indexes such secondary and compound indexes. However, secondary and compound indexes can be implemented and maintained manually by the clients using sets and sorted sets.  Although creating secondary indexes and compound indexes is still possible somehow using sets and sorted sets, maintaining theses indexes manually at each write operation can be difficult and is prone to human error.", 
            "title": "Indexing"
        }, 
        {
            "location": "/Redis/Results/Summary/#grouping-and-filtering", 
            "text": "In general, grouping and filtering data in Redis is done. To group your data in Redis, the set data structure is the best candidate since you will be able to easily insert elements from the same group type as members of the set using sets and sorted sets. To filter data in Redis, you can use the intersection, union, and difference functionalities provided by the set and sorted sets.", 
            "title": "Grouping and Filtering"
        }, 
        {
            "location": "/Redis/Results/Summary/#sorting", 
            "text": "Redis supports a sort command that can be used to sort most of Redis data structures such as lists, sets and sorted sets. This sort command can sort your data ascendingly, descendingly or alphabetically. You can also sort by a specific pattern or limit the returned data. Another useful feature of the sort command, is that you can also sort by external keys.", 
            "title": "Sorting"
        }, 
        {
            "location": "/Redis/Results/Summary/#lua-scripting", 
            "text": "Redis allows you to execute Lua scripts inside the Redis server. Scripting Redis with Lua is useful and can be used to avoid some common pitfalls that slow down development or reduce performance. To load the script to Redis server you can use either EVAL or EVALSHA commands which will evaluate the scripts using the Lua interpreter and load it. By using EVAL you can simply load a Lua program without defining a Lue function and pass arguements to the Lua program as redis keys.  The script is executed in Atomicity way which makes it useful for transaction related tasks.", 
            "title": "Lua scripting"
        }, 
        {
            "location": "/Redis/Results/Summary/#pubsub", 
            "text": "This is a special feature of Redis that can be used to implement applications such as a chat server. The idea is mainly characterised by listeners who can subscribe to certain channels to receive real time messages. The publishers can send binary messages of type string to channels without having any knowledge of whether there are subscribers or not. This feature is following the publish/subscribe messaging paradigm.", 
            "title": "Pub/Sub"
        }, 
        {
            "location": "/Redis/Results/Summary/#expire", 
            "text": "In Redis you can set a timeout on a key. It means that the key will be automatically deleted after the timeout period expires. This feature makes Redis a good database to implement cashing and similar applications.", 
            "title": "Expire"
        }, 
        {
            "location": "/Redis/Results/Summary/#configure-redis-as-a-cache", 
            "text": "If you plan to use Redis as a cache system, then instead of expiring keys manually you can just let Redis take care of this automatically. Redis provides a way to easily configure your Redis server to act automatically as a cache and use one of the popular eviction algorithms such as the LRU algorithm which makes it act like memcached database.", 
            "title": "Configure Redis as a cache"
        }, 
        {
            "location": "/Redis/Results/Summary/#configuration", 
            "text": "Redis can start without a configuration file and then it will take its built-in default configurations. Redis stores its configurations inside a file called redis.config and you can also pass configuration parameters via the command line. Configuring Redis while the server is running is also possible without the need to stop or restart the server using CONFIG GET , and CONFIG SET.", 
            "title": "Configuration"
        }, 
        {
            "location": "/Redis/Results/Summary/#scalability", 
            "text": "Scaling reads in Redis is done using replications where slave instances can share the read load. Scaling writes is supported using sharding. The important question about sharding is who will shard the data. In Redis, the partitioning can be done in the client side where the clients directly select the right instance to be used to write or read a certain key. Most of the clients or drivers that are provided by Redis have implementations for partitioning. Another implementation for partitioning is by using a proxy which means that clients send request to a proxy that is able to interact with Redis and will forward the clients' requests to the right Redis instance then send back the reply to the clients. A famous proxy used by Redis and Memcached is  Twemproxy . Finally it is also possible to use an implementation for partitioning called query routing where you just send your requests to a random instance that will forward your request to the right Redis instance. Starting from Redis version 3.0, Redis cluster is the preferred way to get automatic partitioning and high availability. Redis cluster uses a hybrid implementation of query routing and some clients side implementation.", 
            "title": "Scalability"
        }, 
        {
            "location": "/Redis/Results/Summary/#persistency", 
            "text": "Redis supports two ways to persist data on disk. The first option is called snapshotting or RDB which takes a snapshot of the current data periodically (based on pre-configured value) and store it on disk. The second option is called append-only file or AOF which simply works by logging the operations that modifies the data (write operations) into a log file. Processing this log file later if needed will produce the same dataset that was in memory. Additionally depending on your durability requirements, you need to set the fsync options as explained below:   \"fsync every\" when new command is logged to the AOF. This option is used if you want to have a full durability support but it is also a very slow option.   \"fsync every second\". This is the default configuration which will provide a good durability option but with a 1 second data lose risk in case of a disaster.This option is fast enough.  \"Never fsync\", using this option will let the operating system handle the fsync which can be very unsafe in term of durability. On the other hand, this option is very fast.   Atomicity can be guaranteed by executing a group of commands either using MULTI/EXEC blocks or by using a Lua script.\nConsistency is guaranteed by using the WATCH/UNWATCH blocks.\nIsolation is always guaranteed at command level, and for group of commands it can be also guaranteed by MULTI/EXEC block or a Lua script.\nFull Durability can be also guaranteed when using AOF with executing fsync with each new command as explained before.", 
            "title": "Persistency"
        }, 
        {
            "location": "/Redis/Results/Summary/#backup", 
            "text": "Backup in Redis is basically the snapshots that were taken of your dataset if the snapshot option is enabled. Your data will be stored in an RDB file called dump.rdb file in your redis directory. To restore your data, you just need to move this dump file to your Redis directory and then start up your server. To have a better backup, you can use snapshotting with replication so that you will have the same copy of your data in all your instances including masters and slaves. This will ensure that your data is save in case the master server crashed completely", 
            "title": "Backup"
        }, 
        {
            "location": "/Redis/Results/Summary/#security", 
            "text": "Redis doesn't provide any access control and this should be provided by a separate authorization layer.  However Redis provides an authentication mechanism that is optional and can be turned on from redis.conf. Redis supports the BIND command to allow only specific IP addresses to access the Redis server for better security. Redis doesn't also support any encryption mechanism and this should also be implemented using a separate layer like using encryption mechanisms such as SSL proxy. The \"rename-command\u201d used to rename the original commands which reduces the risk that might happen if unauthorised clients access your server. Finally,  NoSQL injection attacks are impossible since Redis protocol has no concept of string escaping. NoSQL injection attacks are impossible since Redis protocol has no concept of string escaping.", 
            "title": "Security"
        }, 
        {
            "location": "/Redis/Results/Summary/#upgrading", 
            "text": "Redis doesn't support online upgrades (server must restart and the clients can't connect to it during the upgrade window). A workaround for online upgrade is to start a slave server and direct all the clients to it while you do an upgrade to the master. After the upgrade is finished in the master, we can stop the slave server and then redirect the clients requests again to the master server.", 
            "title": "Upgrading"
        }, 
        {
            "location": "/Redis/Results/Summary/#availability", 
            "text": "Redis provides a distributed system called Redis Sentinel to guarantee high availability. Redis Sentinel can resist certain types of failures automatically without any human intervention. Redis Sentinel can be used also for other tasks such as monitoring, as a configuration provider for clients and as a notification system. By Monitoring we mean that Redis Sentinel will monitor your master and slave instances and check if they are working as expected. It will also send notifications to the admin in case anything went wrong. Clients can also contact the Redis Sentinel to get any configurations parameters for a certain Redis instance such as address and port. Finally Redis Sentinel will start a failover process in case the master is not working to promote one of the slaves to replace it and reconfigure the other slaves to contact this new master.  An important note is that in order to get the best robustness out of Redis Sentinel, you should configure it in such a way that it will not be a single point of failure. This means that you use multiple instances in separate machines or virtual machines that fail independently which can cooperate together to handle Redis failure detection and recovery.  You need at least three Redis Sentinel instance to guarantee its Robustness.", 
            "title": "Availability"
        }, 
        {
            "location": "/Redis/Special Features/Expire Option/", 
            "text": "In Redis you can set a timeout on a key. It means that the key will be automatically deleted after the timeout period expires. This feature makes Redis a good database to implement cashing and similar applications. Using this feature is so easy and can be done using the command EXPIRE and providing the timeout as shown in the example below:\n\n\nredis\n SET mykey \nHello\n\nOK\nredis\n EXPIRE mykey 10\n(integer) 1\n\n\n\n\nTo know how much time is left for the key to be deleted, you can use the command TTL which will give you the remaining time to live for a certain key. The timeout can be also removed later if needed to persist the key again using the command PERSIST as shown below:\n\n\nredis\n SET mykey \nHello\n\nOK\nredis\n EXPIRE mykey 10\n(integer) 1\nredis\n TTL mykey\n(integer) 10\nredis\n PERSIST mykey\n(integer) 1\nredis\n TTL mykey\n(integer) -1\n\n\n\n\nIf you want to expire the key at a certain time instead of providing a timeout, you can use EXPIREAT which expires a certain key at a specific timestamp as seen below:\n\n\nredis\n SET mykey \nHello\n\nOK\nredis\n EXISTS mykey\n(integer) 1\nredis\n EXPIREAT mykey 1293840000\n(integer) 1", 
            "title": "Expire Option"
        }, 
        {
            "location": "/Redis/Special Features/Pub:Sub Support/", 
            "text": "This is a special feature of Redis that can be used to implement many applications. The idea is mainly characterised by listeners who can subscribe to certain channels to receive real time messages. The publishers can send binary messages of type string to channels without having any knowledge of whether there are subscribers or not. This feature is following the publish/subscribe messaging paradigm. For example if a subscriber issue the below command:\n\n\nSUBSCRIBE channel1 channel2\n\n\n\n\nThen Redis will push any message that was published by any client to these channels to all the subscribed clients.  The subscriber can unsubscribe from the channels whenever he wants by issuing below command:\n\n\nUNSUBSCRIBE channel1 channel2\n\n\n\n\nPublishing the messages can be done by issuing the below command:\n\n\nPUBLISH channel1 Hello\n\n\n\n\nAnother interesting command is the pattern-matching subscribing or publishing. An example is given below:\n\n\nPSUBSCRIBE news.* \n\n\n\n\nThis command will publish messages to all the channels that start with the word \"news.\" such as news.weather or news.sport. The same can be used for subscription as seen below:\n\n\nPSUBSCRIBE news.*\n# or\nPUNSUBSCRIBE news.*\n\n\n\n\nThis feature is very useful and can be used to implement many applications such as a chat server.", 
            "title": "Pub:Sub Support"
        }, 
        {
            "location": "/Redis/Special Features/Redis as a Cache/", 
            "text": "If you plan to use Redis as a cache system, then instead of expiring keys manually you can just let Redis take care of this automatically. Redis provides a way to easily configure your Redis server to act as a cache.  For instance you can use the below configurations to allow for a maximum of 2mb memory limit and make all keys expire automatically:\n\n\nmaxmemory 2mb\nmaxmemory-policy allkeys-lru\n\n\n\n\nThe keys will be expired based on the LRU (least recently used) algorithm as long as we hit the 2mb memory limit. This feature makes Redis act like memcached database. For more details about the possible options, please have a look to Redis \ndocumentations\n.", 
            "title": "Redis as a Cache"
        }, 
        {
            "location": "/Redis/Special Features/Scripting Support/", 
            "text": "Redis allows you to execute Lua scripts inside the Redis server. Scripting Redis with Lua is useful and can be used to avoid some common pitfalls that slow down development or reduce performance. To load the script to Redis server you can use either EVAL or EVALSHA commands which will evaluate the scripts using the Lua interpreter and load it. By using EVAL you can simply load a Lua program without defining a Lue function and pass arguements to the Lua program as redis keys. Example is given below :\n\n\n eval \nreturn {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}\n 2 key1 key2 first second\n1) \nkey1\n\n2) \nkey2\n\n3) \nfirst\n\n4) \nsecond\n\n\n\n\n\nIn order to call Redis commands from the Lua program you can use redis.call() as seen below :\n\n\n eval \nreturn redis.call('set',KEYS[1],'bar')\n 1 foo\nOK\n\n\n\n\nThe good thing is the fact that there is a one-to-one conversion between Lua program data types and Redis data types. You can check Redis \ndocumentation\n to see the conversion table.\n\n\nAn important note is that the script is executed in Atomicity way which makes it useful for transaction related tasks.\n\n\nEVALSHA is also similar to the way EVAL works, the only differnce is that it takes as a first argument the SHA1 digest of the script and not the complete script. The idea behind this is to reduce the bandwidth. So if the Redis server still remembers a script with the matching SHA1 digest, it will retrieve and that script from cash and execute it. Else it will throw an error. \nExample is given below:\n\n\n evalsha 6b1bf486c81ceb7edf3c093f4c48582e38c0e791 0\n\nresults", 
            "title": "Scripting Support"
        }
    ]
}